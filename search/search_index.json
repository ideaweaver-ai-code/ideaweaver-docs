{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"IdeaWeaver","text":"<p>A comprehensive CLI tool for AI model training, evaluation, and deployment with advanced RAG capabilities and MCP (Model Context Protocol) integration. Train, fine-tune, and deploy language models with enterprise-grade features. Visit: https://github.com/ideaweaver-ai-code/ideaweaver</p> <p></p>"},{"location":"#key-features","title":"\u2728 Key Features","text":"<p>One-Click Setup - Automated Python 3.12 environment with all dependencies Advanced RAG - Traditional + Agentic RAG with multiple vector stores Flexible Training - LoRA, QLoRA, and full fine-tuning support Comprehensive Evaluation - Built-in benchmarks + custom metrics MCP Integration - GitHub, Terraform and AWS connectors  Multi-Agent Workflows - CrewAI pipeline support Configuration Validation - YAML validation and schema checking    </p> <p>NOTE:</p>"},{"location":"#python-version-compatibility","title":"Python Version Compatibility","text":"<p>We are currently using Python 3.12 for the <code>ideaweaver</code> project because some of the tools and dependencies we rely on do not yet support Python 3.13. Once those tools officially support Python 3.13, we plan to upgrade the project accordingly.</p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code># One-line installation\ncurl -LsSf https://raw.githubusercontent.com/ideaweaver-ai-code/ideaweaver/main/setup_environments.sh | sh\n\n# Or traditional installation\ngit clone https://github.com/ideaweaver-ai-code/ideaweaver.git\ncd ideaweaver\nchmod +x setup_environments.sh\n./setup_environments.sh\n</code></pre>"},{"location":"#environment-setup","title":"Environment Setup","text":"<p>\u26a0\ufe0f Important: IdeaWeaver requires Python 3.12. Make sure you have Python 3.12 installed before proceeding.</p> <ol> <li> <p>Check Python Version <pre><code>python --version\nPython 3.12.10\n</code></pre></p> </li> <li> <p>Activate the Environment <pre><code># On Unix/macOS\nsource ideaweaver-env/bin/activate\n</code></pre></p> </li> <li> <p>Verify Installation <pre><code>ideaweaver --help\n</code></pre></p> </li> </ol> <p>Expected Output: <pre><code>Usage: ideaweaver [OPTIONS] COMMAND [ARGS]...\n\n  IdeaWeaver Model Training CLI - A comprehensive tool for AI model training,\n  evaluation, and deployment.\n\n  Features include LoRA/QLoRA fine-tuning, RAG systems, MCP integration, and\n  enterprise-grade model management. For detailed documentation and examples,\n  visit: https://github.com/ideaweaver-ai-code/ideaweaver\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  agent       Intelligent agent workflows for creative and analytical tasks.\n  download    Download a model from Hugging Face Hub\n  evaluate    Evaluate a model using lm-evaluation-harness with...\n  finetune    Supervised fine-tuning commands with LoRA, QLoRA, and full...\n  list-tasks  List all available evaluation tasks from lm-evaluation-harness\n  mcp         Model Context Protocol (MCP) integration commands\n  rag         RAG (Retrieval-Augmented Generation) commands\n  train       Train a model with AutoTrain Advanced.\n  validate    Validate a configuration file\n</code></pre></p> <p>If you see the help menu, your environment is set up correctly!</p> <p>Note: If you need to install Python 3.12: - macOS: <code>brew install python@3.12</code> - Ubuntu/Debian: <code>sudo apt install python3.12</code></p>"},{"location":"#core-usage-step-by-step","title":"Core Usage: Step-by-Step","text":""},{"location":"#1-download-a-model","title":"1. Download a Model","text":"<p>Download models from Hugging Face Hub for local use or further training.</p> <pre><code>ideaweaver download microsoft/DialoGPT-medium\n</code></pre> <p>Tip: Set your Hugging Face token for private/gated models.</p>"},{"location":"#2-validate-configuration","title":"2. Validate Configuration","text":"<p>Check your YAML config or dataset before training to catch errors early.</p> <pre><code>ideaweaver validate --config configs/config.yml\n</code></pre>"},{"location":"#3-model-training","title":"3. Model Training","text":"<p>Train a model using a config file or command-line options. Supports LoRA, QLoRA, and full fine-tuning.</p> <pre><code># Config-based training\nideaweaver train --config configs/config.yml\n\n# Or command-line training\nideaweaver train --model google/bert_uncased_L-2_H-128_A-2 --dataset ./data/train.csv --task text_classification --epochs 3 --batch-size 8\n</code></pre> <p></p>"},{"location":"#4-model-evaluation","title":"4. Model Evaluation","text":"<p>Evaluate your trained or downloaded models on standard or custom benchmarks.</p> <pre><code>ideaweaver evaluate --model ./fine-tuned-model --tasks hellaswag,arc_easy\n</code></pre> <p></p>"},{"location":"#5-agent-workflows","title":"5. Agent Workflows","text":"<p>Use intelligent agents for creative writing, research, social media, and more.</p> <pre><code>ideaweaver agent generate_storybook --theme \"brave little mouse\" --target-age \"3-5\"\n</code></pre> <p></p>"},{"location":"#6-finetuning","title":"6. Finetuning","text":"<p>Fine-tune models with LoRA, QLoRA, or full parameter updates for your data.</p> <pre><code>ideaweaver finetune lora --model microsoft/DialoGPT-medium --dataset ./data.json --output-dir ./fine-tuned-model\n</code></pre>"},{"location":"#7-rag-retrieval-augmented-generation","title":"7. RAG (Retrieval-Augmented Generation)","text":"<p>Build and query knowledge bases with advanced retrieval and hybrid search.</p> <pre><code>ideaweaver rag create-kb --name mykb\nideaweaver rag ingest --kb mykb --source ./docs/\nideaweaver rag query --kb mykb --question \"What is RAG?\"\n</code></pre>"},{"location":"#8-mcp-model-context-protocol-integration","title":"8. MCP (Model Context Protocol) Integration","text":"<p>Connect to external tools and services (GitHub, AWS, etc.) for advanced workflows.</p> <pre><code>ideaweaver mcp list-servers\nideaweaver mcp enable github\n</code></pre>"},{"location":"#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"#system-overview","title":"System Overview","text":""},{"location":"#agent-architecture","title":"Agent Architecture","text":"<p>Figure: Relationship between IdeaWeaver, CrewAI, and specialized generators.</p>"},{"location":"#rag-capabilities-comparison","title":"RAG Capabilities Comparison","text":"Feature Traditional RAG Agentic RAG IdeaWeaver Advantage Query Processing Direct retrieval Multi-step reasoning Both approaches supported Context Awareness Single-turn Multi-turn conversations Persistent context tracking Tool Integration Limited Extensive tool use MCP protocol integration Self-Correction None Built-in reflection Advanced error handling Scalability Vector search only Dynamic planning Hybrid approach"},{"location":"#core-components","title":"\ud83d\udd27 Core Components","text":""},{"location":"#model-training-fine-tuning","title":"Model Training &amp; Fine-tuning","text":"<ul> <li>LoRA/QLoRA: Memory-efficient fine-tuning for large models</li> <li>Full Parameter: Complete model retraining for maximum customization  </li> <li>Multi-GPU: Distributed training support with automatic scaling</li> <li>Experiment Tracking: MLflow, Weights &amp; Biases, Comet integration</li> </ul>"},{"location":"#fine-tuning-examples","title":"Fine-tuning Examples","text":"<pre><code># Full fine-tuning with DialoGPT\nideaweaver finetune full \\\n  --model microsoft/DialoGPT-small \\\n  --dataset datasets/instruction_following_sample.json \\\n  --output-dir ./test_full_basic \\\n  --epochs 5 \\\n  --batch-size 2 \\\n  --gradient-accumulation-steps 2 \\\n  --learning-rate 5e-5 \\\n  --max-seq-length 256 \\\n  --gradient-checkpointing \\\n  --verbose\n</code></pre> <p>Example output: <pre><code>\ud83d\ude80 Supervised Fine-Tuner initialized\n   Model: microsoft/DialoGPT-small\n   Method: full\n   Task: instruction_following\n\ud83d\udd27 Setting up model and tokenizer...\n\u2705 Model and tokenizer setup complete\n   Model parameters: 124,439,808\n\ud83d\udcca Preparing dataset from: datasets/instruction_following_sample.json\n\u2705 Dataset prepared\n   Training samples: 5\n   Evaluation samples: 1\n\ud83d\udd27 Setting up trainer...\n\u2705 Trainer setup complete\n\ud83d\ude80 Starting fine-tuning...\n{'loss': 26.2043, 'grad_norm': nan, 'learning_rate': 1.5076844803522922e-06, 'epoch': 5.0}\n{'train_runtime': 13.3988, 'train_samples_per_second': 1.866, 'train_steps_per_second': 0.746, 'train_loss': 26.204254150390625, 'epoch': 5.0}\n\u2705 Fine-tuning completed!\n\ud83d\udcc1 Model saved to: ./test_full_basic\n\ud83d\udcca Evaluating model...\n{'eval_loss': nan, 'eval_runtime': 0.1933, 'eval_samples_per_second': 5.175, 'eval_steps_per_second': 5.175, 'epoch': 5.0}\n\u2705 Evaluation complete\n   eval_loss: nan\n   eval_runtime: 0.1933\n   eval_samples_per_second: 5.1750\n   eval_steps_per_second: 5.1750\n   epoch: 5.0000\n\u2705 Full fine-tuning completed!\n\ud83d\udcc1 Model saved to: ./test_full_basic\n</code></pre></p>"},{"location":"#listing-fine-tuned-models","title":"Listing Fine-tuned Models","text":"<pre><code>ideaweaver finetune list-models\n</code></pre> <p>Example output: <pre><code>\ud83d\udcc1 Fine-tuned models found:\n==================================================\n\ud83d\udcc2 .\n\n\ud83d\udcc2 cli-final-test\n\ud83d\udcc2 cli-final-test/checkpoint-5\n\ud83d\udcc2 comet-clean\n\ud83d\udcc2 comet-clean/checkpoint-5\n\ud83d\udcc2 comet-experiment\n\ud83d\udcc2 comet-experiment/checkpoint-5\n</code></pre></p>"},{"location":"#rag-systems","title":"RAG Systems","text":"<ul> <li>Vector Stores: Chroma, Qdrant, FAISS with automatic optimization</li> <li>Embeddings: Sentence Transformers, OpenAI, custom models</li> <li>Agentic RAG: Multi-step reasoning with tool integration</li> <li>Document Processing: PDF, DOCX, TXT, Markdown with smart chunking</li> </ul>"},{"location":"#mcp-integration","title":"MCP Integration","text":"<ul> <li>GitHub: Repository analysis, issue tracking, code reviews</li> <li>AWS: S3, Lambda, SageMaker integration for cloud deployment</li> </ul>"},{"location":"#evaluation-benchmarking","title":"Evaluation &amp; Benchmarking","text":"<ul> <li>Standard Tasks: HellaSwag, ARC, WinoGrande, MMLU, and more</li> <li>Custom Metrics: Domain-specific evaluation frameworks</li> <li>Model Comparison: Side-by-side performance analysis</li> <li>RAG Evaluation: RAGAS framework for retrieval assessment</li> </ul>"},{"location":"#example-workflows","title":"\ud83d\udcca Example Workflows","text":""},{"location":"#complete-model-development-pipeline","title":"Complete Model Development Pipeline","text":"<pre><code># 1. Setup and Data Preparation\nideaweaver rag init --vector-store chroma\nideaweaver rag add-documents --path ./training-docs/\n\n# 2. Generate Training Data\nideaweaver rag query \"Generate training examples for customer service\" --save-context\n\n# 3. Model Fine-tuning  \nideaweaver finetune lora \\\n    --model microsoft/DialoGPT-medium \\\n    --dataset ./generated-training-data \\\n    --output-dir ./fine-tuned-model\n\n# 4. Evaluation &amp; Comparison\nideaweaver evaluate \\\n    --model ./fine-tuned-model \\\n    --tasks hellaswag,arc_easy,custom-eval \\\n    --output evaluation-results.json\n</code></pre>"},{"location":"#multi-agent-content-generation","title":"Multi-Agent Content Generation","text":"<pre><code># Setup CrewAI workflow\nideaweaver agent generate-storybook \\\n    --topic \"AI model training best practices\" \\\n    --agents researcher,writer,reviewer \\\n    --output training-guide.md\n</code></pre>"},{"location":"#cli-usage-examples","title":"CLI Usage Examples","text":""},{"location":"#model-training","title":"Model Training","text":"<pre><code># Config-based training\nideaweaver train --config configs/config.yml --verbose\n\n# Command line training\nideaweaver train --model google/bert_uncased_L-2_H-128_A-2 \\\n    --dataset ./data/train.csv \\\n    --task text_classification \\\n    --epochs 3 --batch-size 8\n\n# AWS Bedrock deployment training(coming soon)\nideaweaver train --model meta-llama/Llama-2-7b-hf \\\n    --dataset ./data/train.csv \\\n    --push-to-bedrock \\\n    --bedrock-model-name my-llama-model \\\n    --bedrock-s3-bucket my-bedrock-bucket \\\n    --bedrock-role-arn arn:aws:iam::123456789012:role/BedrockImportRole\n</code></pre>"},{"location":"#tensorboard-ui-example","title":"TensorBoard UI Example","text":""},{"location":"#weights-biases-wandb-online-example","title":"Weights &amp; Biases (wandb) Online Example","text":"<pre><code>WANDB_MODE=online WANDB_API_KEY=XXXXXX ideaweaver evaluate ./my-qwen2-model \\\n  --tasks hellaswag,arc_easy \\\n  --batch-size 2 \\\n  --output-path ./evaluation_results_wandb \\\n  --generate-report \\\n  --report-to wandb \\\n  --wandb-project my-eval-project \\\n  --limit 2 \\\n  --verbose\n</code></pre> <p>Example output: <pre><code>\ud83d\ude80 Starting LLM evaluation for model: ./my-qwen2-model\n\ud83d\udcca Tracking with: wandb\n\u2705 Local model found: ./my-qwen2-model\nwandb: Currently logged in as: laprashant (laprashant-startup) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.19.11\nwandb: Run data is saved locally in /Users/plakhera/Documents/model-registry/wandb/run-20250605_201404-irki0njw\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run autumn-shadow-1\nwandb: \u2b50\ufe0f View project at https://wandb.ai/laprashant-startup/my-eval-project\nwandb: \ud83d\ude80 View run at https://wandb.ai/laprashant-startup/my-eval-project/runs/irki0njw\n\u2705 Wandb initialized: https://wandb.ai/laprashant-startup/my-eval-project/runs/irki0njw\n\ud83d\udcca Starting evaluation on tasks: hellaswag, arc_easy\n\ud83d\udcc1 Output path: ./evaluation_results_wandb\n\ud83d\udd27 Running command: lm_eval --model hf --model_args pretrained=./my-qwen2-model --tasks hellaswag,arc_easy --device auto --batch_size 2 --output_path ./evaluation_results_wandb --log_samples --limit 2 --wandb_args project=my-eval-project\n\u2705 Evaluation completed in 117.12 seconds\n\ud83d\udccb Evaluation output:\nhf (pretrained=./my-qwen2-model), gen_kwargs: (None), limit: 2.0, num_fewshot: None, batch_size: 2\n|  Tasks  |Version|Filter|n-shot| Metric |   |Value|   |Stderr|\n|---------|------:|------|-----:|--------|---|----:|---|-----:|\n|arc_easy |      1|none  |     0|acc     |\u2191  |    0|\u00b1  |     0|\n|         |       |none  |     0|acc_norm|\u2191  |    1|\u00b1  |     0|\n|hellaswag|      1|none  |     0|acc     |\u2191  |    0|\u00b1  |     0|\n|         |       |none  |     0|acc_norm|\u2191  |    0|\u00b1  |     0|\n\n\u26a0\ufe0f  No results.json found, creating basic results structure\n\u2705 Results logged to Weights &amp; Biases\n\u2705 Evaluation completed successfully!\n\ud83d\udcc4 Evaluation report saved to: evaluation_report_._my-qwen2-model.md\n\ud83d\udcc4 Report saved to: evaluation_report_._my-qwen2-model.md\n\n\ud83d\udcca Evaluation Summary:\n   dummy_task - dummy_metric: 1.0000\nwandb: \nwandb: Run history:\nwandb: dummy_task_dummy_metric \u2581\nwandb:     evaluation_duration \u2581\nwandb:               num_tasks \u2581\nwandb: \nwandb: Run summary:\nwandb: dummy_task_dummy_metric 1\nwandb:     evaluation_duration 117.1197\nwandb:               num_tasks 2\nwandb: \nwandb: \ud83d\ude80 View run autumn-shadow-1 at: https://wandb.ai/laprashant-startup/my-eval-project/runs/irki0njw\nwandb: \u2b50\ufe0f View project at: https://wandb.ai/laprashant-startup/my-eval-project\nwandb: Synced 5 W&amp;B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)\nwandb: Find logs at: ./wandb/run-20250605_201404-irki0njw/logs\n\u2705 Wandb run finished\n/opt/homebrew/Cellar/python@3.12/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/tempfile.py:940: ResourceWarning: Implicitly cleaning up &lt;TemporaryDirectory '/var/folders/zh/6p21kgbs2p192mpqm1csfd3r0000gn/T/tmpr6z8m8xf'&gt;\n  _warnings.warn(warn_message, ResourceWarning)\n</code></pre></p>"},{"location":"#wandb-ui-example","title":"wandb UI Example","text":"<p>The evaluation command provides comprehensive model assessment with: - Integration with TensorBoard for visualization - Weights &amp; Biases (wandb) tracking - Detailed performance metrics for each task - Automatic report generation - Offline and online logging options</p> <p>For more details on evaluation options and available tasks, see the Evaluation Guide.</p>"},{"location":"#agent-commands","title":"Agent Commands","text":"<pre><code># Check LLM status\nideaweaver agent check-llm\n\n# Storybook generation\nideaweaver agent generate_storybook --theme \"brave little mouse\" --target-age \"3-5\"\n\n# Research and writing\nideaweaver agent research_write --topic \"AI in healthcare\"\n\n# LinkedIn post creation\nideaweaver agent linkedin_post --topic \"AI trends in 2025\"\n\n# Travel planning\nideaweaver agent travel_plan --destination \"Tokyo\" --duration \"7 days\" --budget \"$2000-3000\"\n\n# Stock analysis\nideaweaver agent stock_analysis --symbol AAPL\n</code></pre> <p>Example output for check-llm: <pre><code>\ud83d\udd0d Checking for Ollama availability...\n\u2705 Ollama is available! Using model: phi3:mini\n\ud83d\udccb Available models: deepseek-r1:1.5b, phi3:mini\n\u2705 Created CrewAI LLM wrapper successfully\n\n\u2705 LLM Status:\n   Provider: ollama\n   Model: phi3:mini\n   Status: Connected and ready\n</code></pre></p>"},{"location":"#enterprise-features","title":"\ud83c\udf1f Enterprise Features","text":""},{"location":"#performance-benchmarks","title":"\ud83d\udcc8 Performance Benchmarks","text":""},{"location":"#training-speed-improvements","title":"Training Speed Improvements","text":"<ul> <li>LoRA Fine-tuning: 3-5x faster than full parameter training</li> <li>QLoRA: 50% memory reduction with minimal quality loss</li> </ul>"},{"location":"#rag-system-performance","title":"RAG System Performance","text":"<ul> <li>Query Speed: &lt;200ms average response time</li> <li>Accuracy: 15-25% improvement over baseline RAG</li> <li>Scalability: Handles 10M+ documents efficiently</li> </ul>"},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<ol> <li>Installation Guide - Setup in under 5 minutes</li> <li>Quick Start - Your first model in 15 minutes  </li> <li>Configuration - Customize for your needs</li> <li>CLI Reference - Complete command documentation</li> </ol>"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":""},{"location":"#getting-started_1","title":"Getting Started","text":"<ul> <li>Installation - Complete setup guide</li> <li>Quick Start - Essential commands and workflows</li> <li>Configuration - Customize your setup</li> </ul>"},{"location":"#tutorials","title":"Tutorials","text":"<ul> <li>Model Training - LoRA, QLoRA, and full fine-tuning</li> <li>RAG Systems - Build intelligent retrieval systems</li> <li>MCP Integration - Connect external services</li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>CLI Commands - Complete command reference</li> <li>Configuration Files - YAML configuration guide</li> </ul>"},{"location":"#community","title":"\ud83e\udd1d Community","text":"<ul> <li>Contributing - Join our development community</li> <li>Guidelines - Code standards and best practices  </li> <li>Support - Get help and report issues</li> <li>Roadmap - Upcoming features and improvements</li> </ul>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"aws-bedrock-integration/","title":"AWS Bedrock Integration Guide","text":""},{"location":"aws-bedrock-integration/#overview","title":"Overview","text":"<p>This guide provides detailed instructions for integrating and deploying models to AWS Bedrock using IdeaWeaver. AWS Bedrock is a fully managed service that makes it easy to build and scale generative AI applications with foundation models.</p>"},{"location":"aws-bedrock-integration/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Prerequisites</li> <li>Setup</li> <li>Model Requirements</li> <li>Deployment Process</li> <li>Troubleshooting</li> <li>Best Practices</li> </ul>"},{"location":"aws-bedrock-integration/#prerequisites","title":"Prerequisites","text":""},{"location":"aws-bedrock-integration/#aws-account-setup","title":"AWS Account Setup","text":"<ol> <li>AWS Account with Bedrock access enabled</li> <li>IAM Role with necessary permissions</li> <li>S3 Bucket in us-east-1 region</li> </ol>"},{"location":"aws-bedrock-integration/#required-permissions","title":"Required Permissions","text":""},{"location":"aws-bedrock-integration/#iam-role-policy","title":"IAM Role Policy","text":"<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"bedrock:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::aws-bedrock-ideaweaver-bucket-use1\",\n                \"arn:aws:s3:::aws-bedrock-ideaweaver-bucket-use1/*\"\n            ]\n        }\n    ]\n}\n</code></pre>"},{"location":"aws-bedrock-integration/#s3-bucket-policy","title":"S3 Bucket Policy","text":"<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowBedrockAccess\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"bedrock.amazonaws.com\"\n            },\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::aws-bedrock-ideaweaver-bucket-use1\",\n                \"arn:aws:s3:::aws-bedrock-ideaweaver-bucket-use1/*\"\n            ]\n        }\n    ]\n}\n</code></pre>"},{"location":"aws-bedrock-integration/#setup","title":"Setup","text":"<ol> <li> <p>Create S3 Bucket <pre><code>aws s3api create-bucket \\\n  --bucket aws-bedrock-ideaweaver-bucket-use1 \\\n  --region us-east-1\n</code></pre></p> </li> <li> <p>Configure Bucket Settings <pre><code># Disable ACLs\naws s3api put-bucket-ownership-controls \\\n  --bucket aws-bedrock-ideaweaver-bucket-use1 \\\n  --ownership-controls Rules=[{ObjectOwnership=BucketOwnerEnforced}]\n</code></pre></p> </li> <li> <p>Create IAM Role <pre><code>aws iam create-role \\\n  --role-name bedrock-admin-role \\\n  --assume-role-policy-document file://trust-policy.json\n</code></pre></p> </li> </ol>"},{"location":"aws-bedrock-integration/#model-requirements","title":"Model Requirements","text":""},{"location":"aws-bedrock-integration/#supported-architectures","title":"Supported Architectures","text":"<ul> <li>llama</li> <li>mistral</li> <li>t5</li> <li>mixtral</li> <li>gpt_bigcode</li> <li>mllama</li> <li>qwen2_vl</li> <li>qwen2</li> <li>qwen2_5_vl</li> </ul>"},{"location":"aws-bedrock-integration/#model-structure","title":"Model Structure","text":"<pre><code>my-qwen2-model/\n\u251c\u2500\u2500 config.json\n\u251c\u2500\u2500 pytorch_model.bin\n\u251c\u2500\u2500 tokenizer.json\n\u2514\u2500\u2500 tokenizer_config.json\n</code></pre>"},{"location":"aws-bedrock-integration/#deployment-process","title":"Deployment Process","text":""},{"location":"aws-bedrock-integration/#1-prepare-model","title":"1. Prepare Model","text":"<pre><code># Download supported model\ngit lfs install\ngit clone https://huggingface.co/Qwen/Qwen2-0.5B my-qwen2-model\n</code></pre>"},{"location":"aws-bedrock-integration/#2-upload-to-s3","title":"2. Upload to S3","text":"<pre><code># Upload model files\naws s3 sync ./my-qwen2-model s3://aws-bedrock-ideaweaver-bucket-use1/bedrock-models/my-qwen2-model/\n</code></pre>"},{"location":"aws-bedrock-integration/#3-deploy-to-bedrock","title":"3. Deploy to Bedrock","text":"<pre><code># Deploy using IdeaWeaver\nideaweaver deploy bedrock \\\n  --model-path ./my-qwen2-model \\\n  --model-name my-qwen2-model \\\n  --s3-bucket aws-bedrock-ideaweaver-bucket-use1 \\\n  --region us-east-1\n</code></pre>"},{"location":"aws-bedrock-integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"aws-bedrock-integration/#common-issues","title":"Common Issues","text":"<ol> <li>Region Mismatch</li> <li>Error: \"Amazon Bedrock does not have access to the S3 location\"</li> <li> <p>Solution: Ensure S3 bucket is in us-east-1</p> </li> <li> <p>Permission Issues</p> </li> <li>Error: \"Access Denied\"</li> <li> <p>Solution: Verify IAM role and bucket policies</p> </li> <li> <p>Model Architecture</p> </li> <li>Error: \"Amazon bedrock does not support the architecture\"</li> <li> <p>Solution: Use only supported architectures</p> </li> <li> <p>Import Job Quotas</p> </li> <li>Error: \"Your account does not have the quota limits\"</li> <li>Solution: Request quota increase</li> </ol>"},{"location":"aws-bedrock-integration/#debugging-steps","title":"Debugging Steps","text":"<ol> <li> <p>Verify S3 Access <pre><code>aws s3 ls s3://aws-bedrock-ideaweaver-bucket-use1/bedrock-models/my-qwen2-model/\n</code></pre></p> </li> <li> <p>Check IAM Role <pre><code>aws iam get-role --role-name bedrock-admin-role\n</code></pre></p> </li> <li> <p>Validate Model <pre><code>ideaweaver bedrock validate-model ./my-qwen2-model\n</code></pre></p> </li> </ol>"},{"location":"aws-bedrock-integration/#best-practices","title":"Best Practices","text":"<ol> <li>Model Selection</li> <li>Start with smaller models (e.g., Qwen2-0.5B)</li> <li>Verify architecture compatibility</li> <li> <p>Test locally before deployment</p> </li> <li> <p>S3 Organization</p> </li> <li>Use consistent naming conventions</li> <li>Organize models in subdirectories</li> <li> <p>Enable versioning for important models</p> </li> <li> <p>Security</p> </li> <li>Use IAM roles with minimal permissions</li> <li>Disable ACLs on S3 buckets</li> <li> <p>Regularly audit access policies</p> </li> <li> <p>Monitoring</p> </li> <li>Track import job status</li> <li>Monitor model performance</li> <li>Set up CloudWatch alarms</li> </ol>"},{"location":"aws-bedrock-integration/#additional-resources","title":"Additional Resources","text":"<ul> <li>AWS Bedrock Documentation</li> <li>IdeaWeaver CLI Reference</li> <li>Model Import Best Practices </li> </ul>"},{"location":"langfuse/","title":"Langfuse Cloud Integration","text":"<p>To enable prompt monitoring and tracing with Langfuse Cloud, set the following environment variables in your shell or <code>.env</code> file:</p> <pre><code>export LANGFUSE_PUBLIC_KEY=\"your-public-key\"\nexport LANGFUSE_SECRET_KEY=\"your-secret-key\"\nexport LANGFUSE_HOST=\"https://cloud.langfuse.com\"\n</code></pre> <p>You can find your API keys in the Langfuse dashboard under Project Settings &gt; API Keys.</p>"},{"location":"langfuse/#langfuse-prompt-monitoring-via-cli","title":"Langfuse Prompt Monitoring via CLI","text":"<p>You can monitor prompts and generations in your LLM workflows using the IdeaWeaver CLI's Langfuse integration. Here's how to test and use prompt monitoring:</p>"},{"location":"langfuse/#1-check-langfuse-connection","title":"1. Check Langfuse Connection","text":"<p><pre><code>ideaweaver langfuse status\n</code></pre> - Verifies your environment variables and Langfuse Cloud connectivity.</p>"},{"location":"langfuse/#2-monitor-a-command","title":"2. Monitor a Command","text":"<p>Wrap any supported IdeaWeaver command to log its prompts and generations: <pre><code>ideaweaver langfuse monitor evaluate --model my-model --tasks hellaswag\n</code></pre> - This logs all prompts and generations from the <code>evaluate</code> command to Langfuse. - You can use any other supported subcommand (e.g., <code>train</code>, <code>rag query</code>, etc.).</p>"},{"location":"langfuse/#3-list-recent-traces","title":"3. List Recent Traces","text":"<p><pre><code>ideaweaver langfuse list-traces\n</code></pre> - Shows recent monitored runs (traces) that have been logged to Langfuse.</p>"},{"location":"langfuse/#4-show-details-of-a-trace","title":"4. Show Details of a Trace","text":"<p><pre><code>ideaweaver langfuse show-trace &lt;trace-id&gt;\n</code></pre> - Replace <code>&lt;trace-id&gt;</code> with the ID from <code>list-traces</code>. - Prints details of the trace, including prompts and generations, in your terminal.</p>"},{"location":"langfuse/#5-what-to-expect-in-the-langfuse-dashboard","title":"5. What to Expect in the Langfuse Dashboard","text":"<ul> <li>Each monitored command creates a new trace in Langfuse.</li> <li>Click into a trace to see all prompt and generation events, including their input and output.</li> </ul>"},{"location":"langfuse/#example-end-to-end-test","title":"Example End-to-End Test","text":"<pre><code>ideaweaver langfuse status\nideaweaver langfuse monitor evaluate --model my-model --tasks hellaswag\nideaweaver langfuse list-traces\nideaweaver langfuse show-trace &lt;trace-id&gt;\n</code></pre> <p>For more details, see the Langfuse section in the documentation or your GitHub Pages site. </p>"},{"location":"mcp/","title":"Model Context Protocol (MCP) Integration","text":""},{"location":"mcp/#overview","title":"Overview","text":"<p>IdeaWeaver includes built-in support for Model Context Protocol (MCP), enabling seamless integration with popular services like GitHub, Slack, AWS, and more. This integration allows AI models to access external data and tools through a standardized protocol.</p> <p></p>"},{"location":"mcp/#quick-start","title":"Quick Start","text":""},{"location":"mcp/#1-check-dependencies","title":"1. Check Dependencies","text":"<pre><code>ideaweaver mcp check-deps\n</code></pre>"},{"location":"mcp/#2-list-available-servers","title":"2. List Available Servers","text":"<pre><code>ideaweaver mcp list-servers\n</code></pre>"},{"location":"mcp/#3-enable-github-server","title":"3. Enable GitHub Server","text":"<pre><code># First, set up authentication\nideaweaver mcp setup-auth github\n\n# Then enable the server\nideaweaver mcp enable github\n</code></pre>"},{"location":"mcp/#available-mcp-servers","title":"Available MCP Servers","text":"Server Description Authentication Required GitHub Access repositories, issues, PRs, and more \u2705 GitHub Token AWS CloudFormation AWS resource management and operations \u2705 AWS Credentials Terraform Provider and module documentation \u274c No Auth Required"},{"location":"mcp/#github-integration-examples","title":"GitHub Integration Examples","text":""},{"location":"mcp/#repository-search","title":"Repository Search","text":"<pre><code>ideaweaver mcp call-tool github search_repositories \\\n  --args '{\"query\": \"machine learning\", \"perPage\": 3}'\n</code></pre>"},{"location":"mcp/#file-contents-reading","title":"File Contents Reading","text":"<pre><code>ideaweaver mcp call-tool github get_file_contents \\\n  --args '{\"owner\": \"100daysofdevops\", \"repo\": \"100daysofdevops\", \"path\": \"README.md\"}'\n</code></pre>"},{"location":"mcp/#issues-listing","title":"Issues Listing","text":"<pre><code>ideaweaver mcp call-tool github list_issues \\\n  --args '{\"owner\": \"100daysofdevops\", \"repo\": \"100daysofdevops\"}'\n</code></pre>"},{"location":"mcp/#pull-requests-listing","title":"Pull Requests Listing","text":"<pre><code>ideaweaver mcp call-tool github list_pull_requests \\\n  --args '{\"owner\": \"100daysofdevops\", \"repo\": \"100daysofdevops\"}'\n</code></pre>"},{"location":"mcp/#aws-cloudformation-integration","title":"AWS CloudFormation Integration","text":"<ol> <li> <p>Set up AWS authentication: <pre><code>ideaweaver mcp setup-auth awslabs.cfn-mcp-server\n</code></pre></p> </li> <li> <p>Enable AWS CloudFormation server: <pre><code>ideaweaver mcp enable awslabs.cfn-mcp-server\n</code></pre></p> </li> <li> <p>List S3 buckets: <pre><code>ideaweaver mcp call-tool awslabs.cfn-mcp-server list_resources --args '{\"resource_type\": \"AWS::S3::Bucket\"}'\n</code></pre></p> </li> </ol>"},{"location":"mcp/#list-ec2-instances","title":"List EC2 Instances","text":"<pre><code>ideaweaver mcp call-tool awslabs.cfn-mcp-server list_resources \\\n  --args '{\"resource_type\": \"AWS::EC2::Instance\"}'\n</code></pre>"},{"location":"mcp/#create-an-s3-bucket","title":"Create an S3 Bucket","text":"<pre><code>ideaweaver mcp call-tool awslabs.cfn-mcp-server create_resource \\\n  --args '{\"resource_type\": \"AWS::S3::Bucket\", \"desired_state\": {\"BucketName\": \"my-test-bucket-12345\"}}'\n</code></pre>"},{"location":"mcp/#terraform-integration-examples","title":"Terraform Integration Examples","text":""},{"location":"mcp/#search-for-terraform-modules","title":"Search for Terraform Modules","text":"<pre><code># Enable Terraform server\nideaweaver mcp enable terraform\n</code></pre> <ol> <li>Search for Terraform modules: <pre><code>ideaweaver mcp call-tool terraform searchModules --args '{\"moduleQuery\": \"aws-vpc\"}'\n</code></pre></li> </ol> <p>Example output: <pre><code>\ud83d\udce6 **Terraform Module Search Results**\n\nAvailable Terraform Modules (top matches) for aws-vpc\n\nEach result includes:\n  - moduleID: The module ID (format: namespace/name/provider-name/module-version)\n  - Name: The name of the module\n  - Description: A short description of the module\n  - Downloads: The total number of times the module has been downloaded\n  - Verified: Verification status of the module\n  - Published: The date and time when the module was published\n\n---\n\n  - moduleID: Harshrai3112/aws-vpc/aws/1.0.0\n  - Name: aws-vpc\n  - Description:\n  - Downloads: 5016\n  - Verified: false\n  - Published: 2021-06-04 08:59:52.979927 +0000 UTC\n---\n\n  - moduleID: Adaptavist/aws-vpc/module/1.1.3\n  - Name: aws-vpc\n  - Description:\n  - Downloads: 860\n  - Verified: false\n  - Published: 2021-12-20 13:24:39.363039 +0000 UTC\n---\n\n  - moduleID: mars/aws-vpc/heroku/1.0.1\n  - Name: aws-vpc\n  - Description: AWS VPC ready for \ud83c\udf50 Heroku Private Spaces\n  - Downloads: 809\n  - Verified: false\n  - Published: 2018-10-09 18:15:04.500275 +0000 UTC\n</code></pre></p> <ol> <li>Get module details: <pre><code>ideaweaver mcp call-tool terraform getModuleDetails \\\n  --args '{\"moduleName\": \"terraform-aws-modules/vpc/aws\"}'\n</code></pre></li> </ol>"},{"location":"mcp/#server-management","title":"Server Management","text":""},{"location":"mcp/#check-status","title":"Check Status","text":"<pre><code>ideaweaver mcp status\n</code></pre>"},{"location":"mcp/#get-server-information","title":"Get Server Information","text":"<pre><code>ideaweaver mcp server-info github\n</code></pre>"},{"location":"mcp/#disable-server","title":"Disable Server","text":"<pre><code>ideaweaver mcp disable github\n</code></pre>"},{"location":"mcp/#re-enable-server","title":"Re-enable Server","text":"<pre><code>ideaweaver mcp enable github\n</code></pre>"},{"location":"mcp/#configuration","title":"Configuration","text":""},{"location":"mcp/#user-configuration-location","title":"User Configuration Location","text":"<ul> <li>macOS/Linux: <code>~/.ideaweaver/mcp/config.json</code></li> <li>Windows: <code>%USERPROFILE%\\.ideaweaver\\mcp\\config.json</code></li> </ul>"},{"location":"mcp/#environment-variables","title":"Environment Variables","text":"<p>Common environment variables used by MCP servers:</p> <pre><code># GitHub\nexport GITHUB_PERSONAL_ACCESS_TOKEN=ghp_xxxxxxxxxxxx\n\n# Slack\nexport SLACK_BOT_TOKEN=xoxb-xxxxxxxxxxxx\nexport SLACK_APP_TOKEN=xapp-xxxxxxxxxxxx\n\n# AWS\nexport AWS_ACCESS_KEY_ID=AKIAXXXXXXXXXXXX\nexport AWS_SECRET_ACCESS_KEY=xxxxxxxxxxxx\nexport AWS_DEFAULT_REGION=us-east-1\n\n# Google Drive\nexport GOOGLE_DRIVE_CREDENTIALS_FILE=/path/to/credentials.json\n</code></pre>"},{"location":"mcp/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mcp/#common-issues","title":"Common Issues","text":"<ol> <li> <p>\"MCP not available\" error <pre><code>pip install mcp\n</code></pre></p> </li> <li> <p>\"Command 'npx' not found\"</p> </li> <li>Install Node.js from https://nodejs.org/</li> <li> <p>Restart your terminal</p> </li> <li> <p>Authentication failures</p> </li> <li>Verify your tokens/credentials are correct</li> <li>Check environment variables are set</li> <li> <p>Use <code>ideaweaver mcp setup-auth &lt;server&gt;</code> for interactive setup</p> </li> <li> <p>Connection timeouts</p> </li> <li>Check your internet connection</li> <li>Verify firewall settings</li> <li>Some servers may take time to download on first use</li> </ol>"},{"location":"mcp/#debug-mode","title":"Debug Mode","text":"<p>Enable verbose output for debugging:</p> <pre><code>ideaweaver mcp list-servers --verbose\nideaweaver mcp test-connection github --verbose\nideaweaver mcp call-tool github search_repositories \\\n  --args '{\"query\": \"test\"}' --verbose\n</code></pre>"},{"location":"mcp/#next-steps","title":"Next Steps","text":"<ol> <li>Start Simple: Begin with servers that don't require authentication (time, memory, fetch)</li> <li>Add Authentication: Set up tokens for GitHub, Slack, or AWS</li> <li>Integrate with RAG: Use MCP servers as data sources in your RAG pipelines</li> <li>Build Workflows: Combine multiple servers for complex AI workflows</li> <li>Develop Custom Servers: Create MCP servers for your proprietary data sources </li> </ol>"},{"location":"agent/SYSTEM_DIAGNOSTICS/","title":"IdeaWeaver System Diagnostics","text":"<p>\ud83d\udd27 AI-Powered System Performance Analysis &amp; Optimization</p> <p>The System Diagnostics feature is the first AI agent specifically designed for system performance debugging in DevOps workflows. It executes real Linux commands and provides intelligent recommendations using CrewAI framework with privacy-first local LLM support.</p>"},{"location":"agent/SYSTEM_DIAGNOSTICS/#key-features","title":"\ud83c\udf1f Key Features","text":"<ul> <li>Real Command Execution: Executes actual system commands (not simulated output)</li> <li>Platform Support: Optimized for Linux environments</li> <li>AI-Powered Analysis: Uses CrewAI framework for intelligent system analysis</li> <li>Privacy-First: Ollama-first approach with OpenAI fallback</li> <li>Comprehensive Coverage: CPU, Memory, Network, and Process analysis</li> <li>Actionable Recommendations: Specific, prioritized optimization steps</li> </ul>"},{"location":"agent/SYSTEM_DIAGNOSTICS/#quick-start","title":"\ud83d\ude80 Quick Start","text":"<pre><code># Basic system diagnostics\nideaweaver agent system_diagnostics\n\n# Verbose output with detailed command execution\nideaweaver agent system_diagnostics --verbose\n\n# Use OpenAI when Ollama is not available\nideaweaver agent system_diagnostics --openai-api-key sk-proj-XXXXXXX\n</code></pre>"},{"location":"agent/SYSTEM_DIAGNOSTICS/#what-it-analyzes","title":"\ud83d\udd0d What It Analyzes","text":""},{"location":"agent/SYSTEM_DIAGNOSTICS/#cpu-load-analysis","title":"CPU &amp; Load Analysis","text":"<ul> <li>Linux: <code>w</code>, <code>top -b -n1</code>, load averages</li> </ul>"},{"location":"agent/SYSTEM_DIAGNOSTICS/#memory-analysis","title":"Memory Analysis","text":"<ul> <li>Linux: <code>free -m</code>, swap usage</li> </ul>"},{"location":"agent/SYSTEM_DIAGNOSTICS/#network-analysis","title":"Network Analysis","text":"<ul> <li>Linux: <code>ip addr show</code>, <code>ip route show</code>, <code>ss -tuln</code></li> </ul>"},{"location":"agent/SYSTEM_DIAGNOSTICS/#process-analysis","title":"Process Analysis","text":"<ul> <li>Both: Top processes, resource consumption, I/O patterns</li> <li>Real-time: Current system state analysis</li> </ul>"},{"location":"agent/SYSTEM_DIAGNOSTICS/#ai-agent-architecture","title":"\ud83e\udd16 AI Agent Architecture","text":"<p>The system uses a sophisticated AI agent workflow:</p> <ol> <li>Command Execution: Real system commands are executed</li> <li>Data Collection: Raw output is captured and structured</li> <li>AI Analysis: CrewAI agent analyzes actual system metrics</li> <li>Recommendations: Specific, actionable optimization steps</li> <li>Priority Ranking: Issues ranked by urgency and impact</li> </ol>"},{"location":"agent/SYSTEM_DIAGNOSTICS/#example-output","title":"\ud83d\udcca Example Output","text":"<p>Here's a real example of the system diagnostics in action:</p> <pre><code>ideaweaver agent system_diagnostics --verbose --openai-api-key sk-proj-XXXXXXX\n</code></pre> \ud83d\udda5\ufe0f Full Command Output <pre><code>ideaweaver agent system_diagnostics --verbose --openai-api-key sk-proj-XXXXXXX\n\ud83d\udd27 Starting System Diagnostics Analysis...\n\ud83e\udd16 Initializing diagnostic agents...\n\ud83d\udccb LLM Priority: Ollama (local) \u2192 OpenAI (cloud)\n\ud83d\udd0d Checking for Ollama availability...\n\u26a0\ufe0f Ollama check failed: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/tags (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7ff1d264e360&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\n\ud83d\udd04 Setting up OpenAI...\n\u2705 OpenAI setup successful\n\ud83d\udd27 System Diagnostic Generator initialized with openai (gpt-4o-mini)\n\ud83d\udda5\ufe0f  Executing CPU &amp; Load analysis...\n\ud83e\udde0 Executing Memory analysis...\n\ud83c\udf10 Executing Network analysis...\n\u26a1 Executing Process &amp; I/O analysis...\n\ud83d\udd0d Analyzing real system diagnostics...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Crew Execution Started \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                                                            \u2502\n\u2502  Crew Execution Started                                                                                                                                    \u2502\n\u2502  Name: crew                                                                                                                                                \u2502\n\u2502  ID: 6ab3b553-f135-4dde-b9b1-17e5a93bb54e                                                                                                                  \u2502\n\u2502                                                                                                                                                            \u2502\n\u2502                                                                                                                                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n# Agent: System Performance Advisor\n## Task: Analyze the following REAL system diagnostic output and provide comprehensive\nrecommendations for system optimization and performance improvement.\n\nACTUAL SYSTEM DIAGNOSTIC DATA:\n=== REAL SYSTEM DIAGNOSTIC OUTPUT ===\n\n\ud83d\udccb CPU_LOAD DIAGNOSTICS:\nCommand: w\nStatus: SUCCESS\nOutput:\n 22:23:07 up 31 min,  1 user,  load average: 0.80, 0.95, 0.99\nUSER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU  WHAT\nubuntu            18.206.107.28    22:18   27:24   0.00s  0.01s sshd: ubuntu [priv]\n\n============================================================\n\n\ud83d\udccb MEMORY DIAGNOSTICS:\nCommand: free -m\nStatus: SUCCESS\nOutput:\n               total        used        free      shared  buff/cache   available\nMem:             957         903          71           0         126          54\nSwap:              0           0           0\n\n============================================================\n\n\ud83d\udccb NETWORK DIAGNOSTICS:\nCommand: Network analysis commands\nStatus: SUCCESS\nOutput:\n=== ip addr show ===\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host noprefixroute\n       valid_lft forever preferred_lft forever\n2: enX0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc fq_codel state UP group default qlen 1000\n    link/ether 12:51:91:f2:3e:95 brd ff:ff:ff:ff:ff:ff\n    inet 172.31.84.202/20 metric 100 brd 172.31.95.255 scope global dynamic enX0\n       valid_lft 3548sec preferred_lft 3548sec\n    inet6 fe80::1051:91ff:fef2:3e95/64 scope link\n       valid_lft forever preferred_lft forever\n\n\n=== ip route show ===\ndefault via 172.31.80.1 dev enX0 proto dhcp src 172.31.84.202 metric 100\n172.31.0.2 via 172.31.80.1 dev enX0 proto dhcp src 172.31.84.202 metric 100\n172.31.80.0/20 dev enX0 proto kernel scope link src 172.31.84.202 metric 100\n172.31.80.1 dev enX0 proto dhcp scope link src 172.31.84.202 metric 100\n\n\n=== ss -tuln ===\nNetid State  Recv-Q Send-Q      Local Address:Port Peer Address:PortProcess\nudp   UNCONN 0      0              127.0.0.54:53        0.0.0.0:*          \nudp   UNCONN 0      0           127.0.0.53%lo:53        0.0.0.0:*          \nudp   UNCONN 0      0      172.31.84.202%enX0:68        0.0.0.0:*          \nudp   UNCONN 0      0               127.0.0.1:323       0.0.0.0:*          \nudp   UNCONN 0      0                   [::1]:323          [::]:*          \ntcp   LISTEN 0      4096           127.0.0.54:53        0.0.0.0:*          \ntcp   LISTEN 0      4096        127.0.0.53%lo:53        0.0.0.0:*          \ntcp   LISTEN 0      4096                    *:22              *:*          \n\n============================================================\n\n\ud83d\udccb PROCESSES DIAGNOSTICS:\nCommand: top -b -n1\nStatus: SUCCESS\nOutput:\ntop - 22:23:08 up 31 min,  1 user,  load average: 0.80, 0.95, 0.99\nTasks: 109 total,   2 running, 107 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  4.0 us, 60.0 sy,  0.0 ni,  0.0 id, 28.0 wa,  0.0 hi,  4.0 si,  4.0 st\nMiB Mem :    957.4 total,     68.4 free,    906.9 used,    125.4 buff/cache    \nMiB Swap:      0.0 total,      0.0 free,      0.0 used.     50.5 avail Mem\n\n    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND\n    632 root      20   0 1856328  14336   2560 S  81.8   1.5   0:23.70 snapd\n      1 root      20   0   22524   8540   4444 S   0.0   0.9   0:02.51 systemd\n      2 root      20   0       0      0      0 S   0.0   0.0   0:00.00 kthreadd\n      3 root      20   0       0      0      0 S   0.0   0.0   0:00.00 pool_wo+\n      4 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n      5 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n      6 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n      7 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     10 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     11 root      20   0       0      0      0 I   0.0   0.0   0:05.58 kworker+\n     12 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     13 root      20   0       0      0      0 I   0.0   0.0   0:00.00 rcu_tas+\n     14 root      20   0       0      0      0 I   0.0   0.0   0:00.00 rcu_tas+\n     15 root      20   0       0      0      0 S   0.0   0.0   0:00.71 ksoftir+\n     16 root      20   0       0      0      0 R   0.0   0.0   0:00.70 rcu_sch+\n     17 root      rt   0       0      0      0 S   0.0   0.0   0:00.01 migrati+\n     18 root     -51   0       0      0      0 S   0.0   0.0   0:00.00 idle_in+\n     19 root      20   0       0      0      0 S   0.0   0.0   0:00.00 cpuhp/0\n     20 root      20   0       0      0      0 S   0.0   0.0   0:00.00 kdevtmp+\n     21 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     23 root      20   0       0      0      0 S   0.0   0.0   0:00.00 kauditd\n     24 root      20   0       0      0      0 S   0.0   0.0   0:00.00 khungta+\n     25 root      20   0       0      0      0 S   0.0   0.0   0:00.00 oom_rea+\n     26 root      20   0       0      0      0 I   0.0   0.0   0:03.60 kworker+\n     27 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     28 root      20   0       0      0      0 S   0.0   0.0   0:00.80 kcompac+\n     29 root      25   5       0      0      0 S   0.0   0.0   0:00.00 ksmd\n     30 root      39  19       0      0      0 S   0.0   0.0   0:00.00 khugepa+\n     31 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     32 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     33 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     34 root     -51   0       0      0      0 S   0.0   0.0   0:00.00 irq/9-a+\n     35 root      20   0       0      0      0 S   0.0   0.0   0:00.00 xen-bal+\n     36 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     37 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     38 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     39 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     40 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     41 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     42 root     -51   0       0      0      0 S   0.0   0.0   0:00.00 watchdo+\n     43 root       0 -20       0      0      0 I   0.0   0.0   0:02.41 kworker+\n     44 root      20   0       0      0      0 S   0.0   0.0   1:06.59 kswapd0\n     45 root      20   0       0      0      0 S   0.0   0.0   0:00.00 ecryptf+\n     46 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     47 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     48 root      20   0       0      0      0 S   0.0   0.0   0:00.00 xenbus\n     49 root      20   0       0      0      0 S   0.0   0.0   0:00.01 xenwatch\n     50 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     51 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     52 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     53 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     54 root      20   0       0      0      0 S   0.0   0.0   0:00.00 scsi_eh+\n     55 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     56 root      20   0       0      0      0 S   0.0   0.0   0:00.00 scsi_eh+\n     57 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     59 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     60 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     68 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     70 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     83 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n     84 root      20   0       0      0      0 S   0.0   0.0   0:00.60 jbd2/xv+\n     85 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n    125 root      19  -1   50344   4668   3516 S   0.0   0.5   0:00.33 systemd+\n    162 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n    163 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n    184 root      rt   0  288952  27136   8704 S   0.0   2.8   0:00.15 multipa+\n    195 root      20   0   26180   6492   3676 S   0.0   0.7   0:00.26 systemd+\n    209 root      -2   0       0      0      0 S   0.0   0.0   0:00.00 psimon\n    260 root      20   0       0      0      0 S   0.0   0.0   0:00.00 jbd2/xv+\n    261 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n    300 systemd+  20   0   21584   6144   3840 S   0.0   0.6   0:00.12 systemd+\n    322 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n    511 systemd+  20   0   22396   4096   2944 S   0.0   0.4   0:00.09 systemd+\n    614 root      20   0    2720   1920   1792 S   0.0   0.2   0:00.00 acpid\n    618 root      20   0    7224   2432   2176 S   0.0   0.2   0:00.01 cron\n    619 message+  20   0    9788   4608   3840 S   0.0   0.5   0:00.09 dbus-da+\n    626 root      20   0   32416  13440   3328 S   0.0   1.4   0:00.08 network+\n    627 polkitd   20   0  383704   6036   3712 S   0.0   0.6   0:00.34 polkitd\n    640 root      20   0   18032   5248   4224 S   0.0   0.5   0:00.24 systemd+\n    646 root      20   0  468820   5760   3712 S   0.0   0.6   0:00.39 udisksd\n    733 _chrony   20   0   19400   3284   2560 S   0.0   0.3   0:00.61 chronyd\n    734 root      20   0    6148   1920   1792 S   0.0   0.2   0:00.00 agetty\n    741 _chrony   20   0   11072   2032   1536 S   0.0   0.2   0:00.00 chronyd\n    778 root      20   0  110004  12800   3456 S   0.0   1.3   0:00.08 unatten+\n    785 syslog    20   0  222508   4864   3328 S   0.0   0.5   0:00.04 rsyslogd\n    825 root      20   0  391872   5120   3328 S   0.0   0.5   0:00.06 ModemMa+\n    833 root      20   0    6104   1792   1664 S   0.0   0.2   0:00.03 agetty\n    997 root      20   0 1685572   8464    128 S   0.0   0.9   0:10.12 amazon-+\n   1065 root      20   0   12020   3328   2304 S   0.0   0.3   0:00.00 sshd\n   1563 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+\n   3705 root      20   0       0      0      0 I   0.0   0.0   0:00.05 kworker+\n   3747 root      20   0       0      0      0 I   0.0   0.0   0:00.80 kworker+\n   3770 root      20   0       0      0      0 I   0.0   0.0   0:00.19 kworker+\n   3968 root      20   0       0      0      0 I   0.0   0.0   0:00.44 kworker+\n   3986 root      20   0   14704   4352   2688 S   0.0   0.4   0:00.01 sshd\n   4331 root      -2   0       0      0      0 S   0.0   0.0   0:00.00 psimon\n   4333 ubuntu    20   0   20160   4864   2944 S   0.0   0.5   0:00.07 systemd\n   4334 ubuntu    20   0   21144   2748   1024 S   0.0   0.3   0:00.00 (sd-pam)\n   4398 ubuntu    20   0   14960   3868   1920 S   0.0   0.4   0:00.11 sshd\n   4399 ubuntu    20   0    9056   3712   2048 S   0.0   0.4   0:00.01 bash\n   4408 root      20   0   17136   3840   2688 S   0.0   0.4   0:00.05 sudo\n   4409 root      20   0   17136   2244   1024 S   0.0   0.2   0:00.00 sudo\n   4410 root      20   0    9188   3840   2048 S   0.0   0.4   0:00.04 bash\n   4512 root      20   0       0      0      0 I   0.0   0.0   0:00.00 kworker+\n   4539 root      20   0    7740   2048   1792 S   0.0   0.2   0:00.00 ideawea+\n   4541 root      20   0 1606852 614556   4608 S   0.0  62.7   0:13.19 python\n   4556 root      20   0   12360   4992   2944 R   0.0   0.5   0:00.00 top\n   4557 root      20   0   26184   4980   2176 S   0.0   0.5   0:00.00 (udev-w+\n   4558 root      20   0   26184   4980   2176 S   0.0   0.5   0:00.00 (udev-w+\n\n============================================================\n\n\nBased on this REAL diagnostic data, provide:\n\n1. **Overall System Health Assessment**: Evaluate the current system state based on actual metrics.\n\n2. **CPU &amp; Load Analysis**:\n   - Interpret the actual load averages shown\n   - Consider the number of CPU cores when assessing load\n   - Identify if the system is appropriately loaded or has issues\n\n3. **Memory Analysis**:\n   - Analyze the real memory usage data\n   - Identify any memory pressure or issues\n   - Provide specific recommendations based on actual usage\n\n4. **Network Analysis**:\n   - Review actual network interface data\n   - Identify any network-related issues from real output\n   - Check actual network connections and ports\n\n5. **Process Analysis**:\n   - Identify actual processes consuming resources\n   - Analyze real I/O and CPU usage patterns\n   - Recommend specific actions based on actual process data\n\n6. **Actionable Recommendations**:\n   - Provide specific steps based on the actual system state\n   - Include real command examples where appropriate\n   - Focus on addressing issues found in the real data\n\n7. **Priority Assessment**: Rank recommendations by actual urgency based on real metrics\n\nIMPORTANT: Base all analysis on the actual command output provided, not on assumptions or generic advice.\n\n\n# Agent: System Performance Advisor\n## Final Answer:\n# System Performance Analysis Report\n\n## 1. Overall System Health Assessment\nThe system is currently functional, but there are signs of resource constraints. The load averages indicate a moderate level of system stress, and memory usage is critically high with little available memory. The absence of swap space further exacerbates potential performance issues, particularly under high load scenarios.\n\n## 2. CPU &amp; Load Analysis\n- **Load Averages**:\n  - 1-minute: 0.80\n  - 5-minute: 0.95\n  - 15-minute: 0.99\n\n  Given these values, the load average is reasonably low for systems with up to 4 CPU cores. If the system has fewer cores, this may indicate a nearing threshold for CPU resource contention.\n\n- **CPU Usage**:\n  - User: 4.0%\n  - System: 60.0% (indicates a heavy amount of system-level processing)\n  - Wait: 28.0% (high wait time suggests potential I/O bottlenecks)\n\n  The high percentage of system CPU usage indicates that the system is spending a significant amount of time executing kernel processes, which could be linked to I/O operations or other resource contention.\n\n### Conclusion:\nThe system is experiencing moderate load but has a high percentage of time spent in kernel mode. This implies potential performance bottlenecks, likely related to I/O operations.\n\n## 3. Memory Analysis\n- **Memory Usage**:\n  - Total: 957 MB\n  - Used: 903 MB\n  - Free: 71 MB\n  - Buff/cache: 126 MB\n  - Available: 54 MB\n\n### Key Observations:\n- The system is using approximately 94% of its RAM, leaving only 54 MB available, which can lead to performance degradation under load.\n- The absence of swap space means the system cannot offload any excess memory usage to disk, risking out-of-memory (OOM) scenarios.\n\n### Recommendations:\n- **Add Swap Space**: Create a swap file to provide additional virtual memory, which can help alleviate memory pressure. Use the following commands:\n  ```bash\n  sudo fallocate -l 1G /swapfile\n  sudo chmod 600 /swapfile\n  sudo mkswap /swapfile\n  sudo swapon /swapfile\n  echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab\n  ```\n\n## 4. Network Analysis\n- **Network Interfaces**:\n  - Active interface: `enX0` with IP `172.31.84.202`\n  - No apparent issues with the network interface as it is up and configured correctly.\n\n- **Connections**:\n  - Listening on port 22 (SSH), which is expected.\n  - No suspicious connections detected in the `ss -tuln` output.\n\n### Conclusion:\nThe network setup appears to be functioning correctly without issues.\n\n## 5. Process Analysis\n- **High Resource Consumption**:\n  - PID 632 (snapd) is consuming 81.8% CPU and 1.5% memory.\n  - PID 4410 (python) is consuming 62.7% memory, indicating a potential application issue or inefficiency.\n\n### Recommendations:\n- **Investigate High CPU Usage**:\n  - Review the snapd process; if it's not necessary, consider stopping or disabling it:\n    ```bash\n    sudo systemctl stop snapd\n    sudo systemctl disable snapd\n    ```\n- **Optimize or Restart Resource-Heavy Applications**:\n  - Check PID 4410 (python) for performance issues or optimize its code.\n\n## 6. Actionable Recommendations\n1. **Add Swap Space**: [High Priority]\n   - Follow the commands provided in the memory analysis section to add a swap file.\n\n2. **Investigate High CPU Processes**: [Medium Priority]\n   - Use `top` or `htop` to monitor and analyze `snapd` and `python` processes to identify inefficiencies.\n\n3. **Optimize System Services**: [Medium Priority]\n   - Review and disable unnecessary services to reduce system load and free up resources.\n\n4. **Monitor System Performance**: [Ongoing]\n   - Implement monitoring tools such as `htop`, `iotop`, or `glances` for continuous observation.\n\n5. **Consider Hardware Upgrade**: [Long-term]\n   - If performance issues persist, consider upgrading RAM or CPU based on workload requirements.\n\n## 7. Priority Assessment\n- **Critical**: Add swap space (immediate risk of OOM).\n- **High**: Investigate and optimize resource-heavy processes.\n- **Medium**: Continuous monitoring and service optimization.\n- **Low**: Hardware upgrades based on future assessments.\n\nBy following these recommendations, the overall system performance should improve, reducing bottlenecks and increasing responsiveness during peak loads.\n================================================================================\n\n\ud83d\udca1 Next Steps:\n\u2022 Review the recommendations above\n\u2022 Execute suggested commands to optimize performance\n\u2022 Monitor system performance after implementing changes\n\u2022 Run diagnostics again to verify improvements\n</code></pre>"},{"location":"agent/SYSTEM_DIAGNOSTICS/#technical-implementation","title":"\ud83d\udee0\ufe0f Technical Implementation","text":""},{"location":"agent/SYSTEM_DIAGNOSTICS/#llm-selection-strategy","title":"LLM Selection Strategy","text":"<ol> <li>Primary: Ollama (local, privacy-first)</li> <li>Fallback: OpenAI (cloud, when Ollama unavailable)</li> <li>Models: Supports any Ollama model or OpenAI GPT variants</li> </ol>"},{"location":"agent/SYSTEM_DIAGNOSTICS/#platform-specific-commands","title":"Platform-Specific Commands","text":"System Aspect Linux Commands macOS Commands CPU/Load <code>w</code>, <code>top -b -n1</code> <code>w</code>, <code>top -l 1 -n 10</code>, <code>uptime</code> Memory <code>free -m</code> <code>vm_stat</code> Network <code>ip addr show</code>, <code>ss -tuln</code> <code>ifconfig</code>, <code>netstat -rn</code> Processes <code>top -b -n1</code> <code>top -l 1 -n 10</code>"},{"location":"agent/SYSTEM_DIAGNOSTICS/#error-handling","title":"Error Handling","text":"<ul> <li>Graceful command failure handling</li> <li>Cross-platform compatibility checks</li> <li>Detailed error reporting with troubleshooting tips</li> </ul>"},{"location":"agent/SYSTEM_DIAGNOSTICS/#configuration-options","title":"\ud83d\udd27 Configuration Options","text":""},{"location":"agent/SYSTEM_DIAGNOSTICS/#command-line-arguments","title":"Command Line Arguments","text":"<ul> <li><code>--verbose</code>: Enable detailed execution logging</li> <li><code>--openai-api-key</code>: Specify OpenAI API key for cloud analysis</li> <li><code>--help</code>: Show all available options</li> </ul>"},{"location":"agent/SYSTEM_DIAGNOSTICS/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>OPENAI_API_KEY</code>: Set OpenAI API key via environment</li> <li><code>OLLAMA_HOST</code>: Custom Ollama server endpoint (default: localhost:11434)</li> </ul>"},{"location":"agent/SYSTEM_DIAGNOSTICS/#architecture-overview","title":"\ud83c\udfd7\ufe0f Architecture Overview","text":"<pre><code>graph TD\n    A[CLI Command] --&gt; B[SystemDiagnosticGenerator]\n    B --&gt; C[Platform Detection]\n    C --&gt; D[Command Execution]\n    D --&gt; E[Data Collection]\n    E --&gt; F[LLM Selection]\n    F --&gt; G[Ollama Check]\n    G --&gt;|Available| H[Local Analysis]\n    G --&gt;|Unavailable| I[OpenAI Fallback]\n    H --&gt; J[CrewAI Agent]\n    I --&gt; J\n    J --&gt; K[Performance Analysis]\n    K --&gt; L[Recommendations]\n    L --&gt; M[Report Generation]\n</code></pre>"},{"location":"agent/SYSTEM_DIAGNOSTICS/#getting-started","title":"\ud83d\ude80 Getting Started","text":""},{"location":"agent/SYSTEM_DIAGNOSTICS/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12+</li> <li>IdeaWeaver installed and configured</li> <li>Optional: Ollama for local AI analysis</li> <li>Optional: OpenAI API key for cloud analysis</li> </ul>"},{"location":"agent/SYSTEM_DIAGNOSTICS/#installation","title":"Installation","text":"<pre><code># IdeaWeaver includes system diagnostics by default\nideaweaver --help\nideaweaver agent --help\n</code></pre>"},{"location":"agent/SYSTEM_DIAGNOSTICS/#first-run","title":"First Run","text":"<pre><code># Start with basic diagnostics\nideaweaver agent system_diagnostics\n\n# For detailed output and troubleshooting\nideaweaver agent system_diagnostics --verbose\n</code></pre>"},{"location":"agent/SYSTEM_DIAGNOSTICS/#use-cases","title":"\ud83d\udca1 Use Cases","text":"<ul> <li>DevOps Troubleshooting: Quick system health checks</li> <li>Performance Optimization: Identify bottlenecks and optimization opportunities</li> <li>Capacity Planning: Understand resource utilization patterns</li> <li>Security Auditing: Review running processes and network connections</li> <li>Infrastructure Monitoring: Regular system health assessments</li> </ul>"},{"location":"agent/SYSTEM_DIAGNOSTICS/#privacy-security","title":"\ud83d\udd12 Privacy &amp; Security","text":"<ul> <li>Local-First: Ollama processing keeps data on your machine</li> <li>No Data Storage: No diagnostic data is stored or transmitted unnecessarily</li> <li>Secure Commands: Only reads system state, no modifications made</li> <li>API Key Safety: OpenAI keys are used only when explicitly provided</li> </ul>"},{"location":"agent/SYSTEM_DIAGNOSTICS/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>The system diagnostics feature is part of the main IdeaWeaver project. Contributions welcome for: - Additional platform support - New diagnostic commands - Enhanced AI analysis capabilities - Performance optimizations</p>"},{"location":"agent/SYSTEM_DIAGNOSTICS/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>Main IdeaWeaver README</li> <li>CrewAI Integration</li> <li>CLI Commands</li> <li>Agent Framework</li> </ul>"},{"location":"agent/SYSTEM_DIAGNOSTICS/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"agent/SYSTEM_DIAGNOSTICS/#common-issues","title":"Common Issues","text":"<p>Ollama Connection Failed <pre><code>\u26a0\ufe0f Ollama check failed: HTTPConnectionPool(host='localhost', port=11434)\n</code></pre> - Solution: Install and start Ollama, or use <code>--openai-api-key</code> flag</p> <p>Command Not Found <pre><code>\u274c Error running system diagnostics: Command 'free' not found\n</code></pre> - Solution: Platform-specific commands are automatically selected</p> <p>Permission Denied <pre><code>\u274c Error: Permission denied accessing system information\n</code></pre> - Solution: Some commands may require sudo privileges on certain systems</p>"},{"location":"agent/SYSTEM_DIAGNOSTICS/#getting-help","title":"Getting Help","text":"<ul> <li>Use <code>--verbose</code> flag for detailed execution logs</li> <li>Check Issues for known problems</li> <li>Report new issues with full error output</li> </ul> <p>\u2b50 If you find this feature useful, please star the IdeaWeaver repository! </p>"},{"location":"agent/commands/","title":"Agent Commands","text":"<p>IdeaWeaver provides a comprehensive set of agent commands for various tasks. Each command is designed to leverage the power of AI agents to accomplish specific goals.</p>"},{"location":"agent/commands/#llm-requirements-and-dependencies","title":"LLM Requirements and Dependencies","text":"<p>IdeaWeaver's agent system is designed to work with multiple language model providers, with a preference for local models through Ollama for cost-effective operation.</p>"},{"location":"agent/commands/#ollama-setup-recommended","title":"Ollama Setup (Recommended)","text":"<p>Ollama is the preferred LLM provider for IdeaWeaver agents. It provides: - Free, local model inference - No API costs - Privacy and data security - Offline operation capability</p> <p>To set up Ollama:</p> <ol> <li> <p>Install Ollama:    <pre><code># macOS\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Linux\ncurl -fsSL https://ollama.com/install.sh | sh\n</code></pre></p> </li> <li> <p>Start the Ollama service:    <pre><code>ollama serve\n</code></pre></p> </li> <li> <p>Pull a recommended model:    <pre><code>ollama pull phi3:mini  # Fast and efficient\n# or\nollama pull llama2:7b  # More capable but larger\n</code></pre></p> </li> </ol>"},{"location":"agent/commands/#openai-fallback","title":"OpenAI Fallback","text":"<p>If Ollama is not available, IdeaWeaver will automatically fall back to using OpenAI's API. Please note:</p> <ul> <li>OpenAI usage incurs costs based on token consumption</li> <li>API key is required (set via OPENAI_API_KEY environment variable)</li> <li>Internet connection is required</li> <li>Usage is subject to OpenAI's rate limits and terms of service</li> </ul> <p>To use OpenAI:</p> <ol> <li>Obtain an API key from OpenAI</li> <li>Set the environment variable:    <pre><code>export OPENAI_API_KEY=\"your-api-key\"\n</code></pre></li> </ol>"},{"location":"agent/commands/#checking-llm-status","title":"Checking LLM Status","text":"<p>You can verify your LLM setup at any time: <pre><code>ideaweaver agent check-llm\n</code></pre></p> <p>This will show: - Available Ollama models (if installed) - OpenAI API status (if configured) - Current active provider - Connection status</p>"},{"location":"agent/commands/#available-commands","title":"Available Commands","text":"<p>To see all available agent commands:</p> <pre><code>ideaweaver agent --help\n</code></pre> <p>Example output: <pre><code>Usage: ideaweaver agent [OPTIONS] COMMAND [ARGS]...\n\n  Intelligent agent workflows for creative and analytical tasks.\n\n  This command group provides access to various AI agents for tasks like:\n  - Creative writing and story generation\n  - Research and content creation\n  - Social media content generation\n  - Travel planning and recommendations\n  - Stock market analysis\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  check-llm         Check the status and availability of language model...\n  generate_storybook  Generate a storybook with specified theme and target age\n  linkedin_post     Create engaging LinkedIn posts for professional...\n  research_write    Research and write comprehensive content on various...\n  stock_analysis    Perform comprehensive stock market analysis\n  travel_plan       Create personalized travel itineraries and...\n</code></pre></p>"},{"location":"agent/commands/#check-llm-status","title":"Check LLM Status","text":"<p>Verify your language model setup and availability:</p> <pre><code>ideaweaver agent check-llm\n</code></pre> <p>Example output: <pre><code>\ud83d\udd0d Checking for Ollama availability...\n\u2705 Ollama is available! Using model: phi3:mini\n\ud83d\udccb Available models: deepseek-r1:1.5b, phi3:mini\n\u2705 Created CrewAI LLM wrapper successfully\n\n\u2705 LLM Status:\n   Provider: ollama\n   Model: phi3:mini\n   Status: Connected and ready\n</code></pre></p>"},{"location":"agent/commands/#generate-storybook","title":"Generate Storybook","text":"<p>Generate an engaging and age-appropriate storybook:</p> <pre><code>ideaweaver agent generate_storybook --theme \"brave little mouse\" --target-age \"3-5\"\n</code></pre> <p>Options:</p> <ul> <li><code>--theme</code>: The main theme or topic of the storybook</li> <li><code>--target-age</code>: Target age range (e.g., \"3-5\", \"6-8\")</li> <li><code>--num-pages</code>: Number of pages (default: 5)</li> <li><code>--style</code>: Writing style (e.g., \"whimsical\", \"educational\")</li> <li><code>--openai-api-key</code>: Your OpenAI API key (if not set in environment)</li> </ul> <p>For a complete example of a generated storybook, see Agent Examples.</p>"},{"location":"agent/commands/#research-and-writing","title":"Research and Writing","text":"<p>Conduct research and create comprehensive content:</p> <pre><code>ideaweaver agent research_write --topic \"AI in healthcare\"\n</code></pre>"},{"location":"agent/commands/#linkedin-post-creation","title":"LinkedIn Post Creation","text":"<p>Generate professional LinkedIn content:</p> <pre><code>ideaweaver agent linkedin_post --topic \"AI trends in 2025\"\n</code></pre>"},{"location":"agent/commands/#travel-planning","title":"Travel Planning","text":"<p>Create detailed travel itineraries:</p> <pre><code>ideaweaver agent travel_plan --destination \"Tokyo\" --duration \"7 days\" --budget \"$2000-3000\"\n</code></pre>"},{"location":"agent/commands/#stock-analysis","title":"Stock Analysis","text":"<p>Perform comprehensive stock market analysis:</p> <pre><code>ideaweaver agent stock_analysis --symbol AAPL\n</code></pre>"},{"location":"agent/commands/#command-options","title":"Command Options","text":"<p>Each command supports various options for customization:</p> <ul> <li><code>--verbose</code>: Enable detailed output</li> <li><code>--output</code>: Specify output file/directory</li> <li><code>--format</code>: Choose output format (markdown, json, etc.)</li> <li><code>--model</code>: Select specific LLM model</li> <li><code>--temperature</code>: Adjust creativity level (0.0-1.0)</li> </ul> <p>For detailed options for each command, use:</p> <pre><code>ideaweaver agent &lt;command&gt; --help\n</code></pre> <p>For example, to see options for the check-llm command:</p> <pre><code>ideaweaver agent check-llm --help\n</code></pre> <p>Example output: <pre><code>Usage: ideaweaver agent check-llm [OPTIONS]\n\n  Check the status and availability of language model providers.\n\n  This command verifies:\n  - Local model availability (Ollama)\n  - API access (OpenAI)\n  - Model capabilities\n  - Connection status\n\n  Useful for troubleshooting and ensuring required models are available\n  for agent operations.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre></p>"},{"location":"agent/examples/","title":"Agent Examples","text":"<p>This page showcases example outputs from various IdeaWeaver agents.</p>"},{"location":"agent/examples/#storybook-generation","title":"Storybook Generation","text":"<p>Here's an example of a storybook generated using the <code>generate_storybook</code> command:</p> <pre><code>ideaweaver agent generate_storybook --theme \"brave little mouse\" --target-age \"3-5\"\n</code></pre>"},{"location":"agent/examples/#workflow-diagram","title":"Workflow Diagram","text":""},{"location":"agent/examples/#output","title":"Output","text":"<p>\ud83d\udcda Title: \"Milo the Mighty: The Bravest Mouse in the Meadow\"</p> <p>Theme: bravery, kindness Target Age: 3-5 years old Style: whimsical Pages: 1</p>"},{"location":"agent/examples/#page-1","title":"Page 1","text":"<p>\"Once upon a time, in a snug, little burrow, nestled in a lush green meadow, lived a pint-sized mouse named Milo. He may have been small, but Milo was the bravest mouse in the entire meadow! His fur was as soft and shiny as a grey satin ribbon, and his eyes twinkled like two tiny stars.</p> <p>Every morning, Milo would wake up to a delightful symphony of birds chirping and the sweet, fresh scent of morning dew on the grass. He adored his home and his meadow friends.</p> <p>On a bright, sunshiny day, a loud, terrifying growl echoed across the meadow. A big, intimidating cat had strayed into their peaceful home! All the other animals were petrified and scampered off to hide.</p> <p>But not our brave little Milo. He chose to confront the cat. His heart was drumming in his chest, but he took a deep, calming breath and bravely asked, \"Why are you frightening my friends, Mr. Cat? This is our home.\"</p> <p>The big, intimidating cat looked taken aback. He'd never met a mouse so brave before. Feeling a bit sheepish, he replied, \"I didn't mean to scare anyone. I just lost my way and was trying to find my way back home.\"</p> <p>Kind-hearted Milo decided to help the lost cat. Together, they trekked through the meadow, across rolling hills and finally reached the cat's home.</p> <p>The cat was grateful to Milo and promised never to frighten his meadow friends again. From that day forward, Milo was hailed as 'Milo the Mighty', the bravest mouse in the meadow!</p> <p>And the lesson we learn from our brave little Milo is, \"No one is too small to do big things, and kindness is the greatest bravery of all.\"</p>"},{"location":"agent/examples/#page-1-illustration","title":"Page 1 Illustration","text":"<p>The story opens in a verdant meadow, as dawn breaks. At the heart of the illustration is a snug burrow, a homely, brown mound with a small, welcoming entrance. Dew-kissed grass, dotted with pastel-hued wildflowers, surrounds the burrow. In the distance, majestic trees stretch towards the pastel sky.</p> <p>Milo, the small mouse, stands at the entrance of his burrow. He's colored in soft, shiny greys, with eyes that twinkle like stars. His expression exudes joy and readiness for the day ahead. He gazes at the sky, where birds are seen in flight, their songs represented by floating musical notes.</p> <p>The illustration radiates serenity. The color palette is cool tones with warm pastels, signaling a new day. The mood is uplifting, and the style is whimsical, with soft, rounded shapes, and playful interpretation of scale. The illustration effectively sets the stage for the story.</p>"},{"location":"agent/examples/#summary","title":"Summary","text":"<p>\"Milo the Mighty: The Bravest Mouse in the Meadow\" is an endearing tale of bravery and kindness. Milo, a small mouse with a big heart, stands up to a lost and intimidating cat, demonstrating that no one is too small to make a big difference. This whimsically illustrated storybook teaches children the valuable lesson that kindness is the greatest bravery of all.</p>"},{"location":"agent/examples/#generation-process","title":"Generation Process","text":"<p>The storybook was generated using a multi-agent workflow:</p> <ol> <li>Expert Children's Story Writer: Created the initial story with age-appropriate content</li> <li>Children's Book Art Director: Designed detailed illustration descriptions</li> <li>Senior Children's Book Editor: Polished the content and ensured quality standards</li> </ol> <p>The system used OpenAI's API for generation, as indicated by the output: <pre><code>\ud83d\udccb LLM Priority: Ollama (local) \u2192 OpenAI (cloud)\n\ud83d\udd04 Setting up OpenAI (Ollama has compatibility issues with CrewAI v0.121.1)...\n\ud83e\uddea Testing OpenAI connection...\n\u2705 OpenAI connection successful\n</code></pre></p>"},{"location":"agent/examples/#research-writing-example","title":"Research Writing Example","text":"<p>Here's an example of a research article generated using the <code>research_write</code> agent:</p> <pre><code>ideaweaver agent research_write --topic \"AI in healthcare\"\n</code></pre>"},{"location":"agent/examples/#workflow-diagram_1","title":"Workflow Diagram","text":""},{"location":"agent/examples/#output_1","title":"Output","text":""},{"location":"agent/examples/#artificial-intelligence-in-healthcare-revolutionizing-patient-care-and-medical-research","title":"Artificial Intelligence in Healthcare: Revolutionizing Patient Care and Medical Research","text":""},{"location":"agent/examples/#executive-summary","title":"Executive Summary","text":"<p>Artificial Intelligence (AI) is making waves in the healthcare sector, offering innovative solutions in diagnosis, treatment, patient care, and research. With the application of AI technologies like machine learning, natural language processing, and robotics, major healthcare providers and technology companies worldwide are transforming the healthcare landscape.</p>"},{"location":"agent/examples/#major-trends-and-developments-in-ai-and-healthcare","title":"Major Trends and Developments in AI and Healthcare","text":"<p>AI is not just a buzzword; it's a substantial driving force in healthcare. Here are the key trends and developments:</p> <ol> <li> <p>AI in Diagnosis and Treatment: AI algorithms are being utilized for early detection of diseases like cancer, diabetes, and heart diseases, leading to improved patient outcomes. For example, Google's DeepMind has developed an AI model that can predict proteins' structure, paving the way for drug discovery.</p> </li> <li> <p>AI in Personalized Medicine: AI is enabling the creation of personalized treatment plans, considering individual patient data, genetics, and lifestyle factors. Companies like Tempus are leveraging AI for personalized oncology treatments.</p> </li> <li> <p>AI in Mental Health: AI-driven chatbots and apps like Woebot Health are increasingly being used for mental health support and therapy.</p> </li> </ol>"},{"location":"agent/examples/#key-players-and-technologies-in-ai-and-healthcare","title":"Key Players and Technologies in AI and Healthcare","text":"<p>Several key players are spearheading the AI revolution in healthcare:</p> <ol> <li> <p>IBM Watson: Watson uses AI to enhance patient care, accelerate drug discovery, and improve operational efficiency in healthcare.</p> </li> <li> <p>Google DeepMind: DeepMind is focused on AI research to solve health problems, such as protein folding and macular degeneration.</p> </li> <li> <p>Tempus: Tempus uses AI to personalize cancer treatment plans.</p> </li> </ol>"},{"location":"agent/examples/#market-impact-and-future-outlook-of-ai-in-healthcare","title":"Market Impact and Future Outlook of AI in Healthcare","text":"<p>According to Fortune Business Insights, the AI in healthcare market is poised to reach $45.2 billion by 2026, growing at a CAGR of 44.9% from 2021. The future of AI points to further improvements in disease prediction, drug discovery, telehealth services, and patient care.</p>"},{"location":"agent/examples/#ethical-implications-of-ai-in-healthcare","title":"Ethical Implications of AI in Healthcare","text":"<p>While AI's transformative role in healthcare cannot be understated, it's crucial to address the ethical concerns surrounding data privacy and job displacement that come with the implementation of this technology. The need for robust ethical guidelines and regulations is paramount as we move towards a more AI-centric healthcare system.</p>"},{"location":"agent/examples/#conclusion","title":"Conclusion","text":"<p>AI is not only reshaping healthcare but is also poised to drive significant market growth. The rapid advancements in AI technologies are expected to bring about unprecedented changes in healthcare diagnosis, treatment, and patient care. Despite the ethical challenges, AI stands to be an integral part of future healthcare, driving improvements and efficiency.</p> <p>Research completed: [Current date] Topic: AI in Healthcare</p>"},{"location":"agent/examples/#linkedin-post-example","title":"LinkedIn Post Example","text":"<p>Here's an example of a workflow for generating a LinkedIn post using the <code>linkedin_post</code> agent:</p> <pre><code>ideaweaver agent linkedin_post --topic \"AI trends in 2025\"\n</code></pre>"},{"location":"agent/examples/#workflow-diagram_2","title":"Workflow Diagram","text":""},{"location":"agent/examples/#output_2","title":"Output","text":"<p>COPY &amp; PASTE TO LINKEDIN:</p> <pre><code>AI is transforming our world, and 2025 will be a game-changer.\ud83c\udf10\ud83d\ude80\n\n1\ufe0f\u20e3**AI &amp; Automation**: Streamlining operations, reducing costs. Robots are now running the show in warehouses!\n2\ufe0f\u20e3**AI in Healthcare**: Predictive analytics for faster diagnoses. Say goodbye to long waits! \ud83d\udc89\n3\ufe0f\u20e3**AI in Finance**: Personalized investment advice for all. Wall Street in your pocket! \ud83d\udcb0\n\nProfessionals, it's time to upskill for the AI-driven future. \ud83d\udca1 Embrace the change or risk being left behind.\n\nWhat's your game plan for leveraging AI in your industry? Share your thoughts.\ud83d\udc47 Let's embrace the future, together.\n\n#AI #FutureOfWork #HealthTech #FinTech #Automation\n</code></pre> <p>\ud83d\udca1 LinkedIn Posting Tips: \u2022 Post during peak hours (8-10am, 12-2pm, 5-6pm) \u2022 Engage with comments within the first hour \u2022 Tag relevant people or companies if appropriate \u2022 Consider posting as a carousel for longer content</p>"},{"location":"agent/examples/#travel-plan-example","title":"Travel Plan Example","text":"<p>Here's an example of a travel plan generated using the <code>travel_plan</code> agent:</p> <pre><code>ideaweaver agent travel_plan --destination \"Tokyo\" --duration \"7 days\" --budget \"$2000-3000\"\n</code></pre>"},{"location":"agent/examples/#workflow-diagram_3","title":"Workflow Diagram","text":""},{"location":"agent/examples/#output_3","title":"Output","text":"<p><pre><code>\u2708\ufe0f Creating travel plan for Tokyo\n\ud83e\udde0 Checking OpenAI GPT-4 availability for travel planning\n\ud83d\udd0d Checking for Ollama availability...\n\u2705 Ollama is available! Using model: phi3:mini\n\ud83d\udccb Available models: deepseek-r1:1.5b, phi3:mini\n\u2705 Created CrewAI LLM wrapper successfully\n\ud83e\udde0 Using OLLAMA with model: phi3:mini\n\u2708\ufe0f Creating travel plan for Tokyo\n\ud83d\udd04 Setting up OLLAMA for travel planning...\n\ud83e\udd99 Using Ollama model: phi3:mini\n</code></pre> \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Crew Execution \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502                                                                                                                  \u2502 \u2502 1. Agent: Destination Researcher                                                                                 \u2502 \u2502    Answer: Tokyo is a vibrant city blending tradition and modernity, famous for cherry blossoms, sushi, and tech.\u2502 \u2502                                                                                                                  \u2502 \u2502 2. Agent: Itinerary Planner                                                                                      \u2502 \u2502    Answer: 7-day itinerary includes Shibuya, Asakusa, Mt. Fuji, Akihabara, Tsukiji Market, and more.            \u2502 \u2502                                                                                                                  \u2502 \u2502 3. Agent: Budget Analyst                                                                                         \u2502 \u2502    Answer: Estimated cost for 7 days in Tokyo: $2,500 (mid-range, including flights, hotel, food, attractions).  \u2502 \u2502                                                                                                                  \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f</p> <p>Enjoy your trip to Tokyo!</p>"},{"location":"agent/examples/#stock-analysis-example","title":"Stock Analysis Example","text":"<p>Here's an example of a workflow for generating a stock analysis using the <code>stock_analysis</code> agent:</p> <pre><code>ideaweaver agent stock_analysis --symbol \"AAPL\"\n</code></pre>"},{"location":"agent/examples/#workflow-diagram_4","title":"Workflow Diagram","text":""},{"location":"agent/examples/#output_4","title":"Output","text":"<p><pre><code>\ud83d\udcc8 Analyzing stock: AAPL\n\ud83e\udde0 Using OpenAI GPT-4 for stock analysis\n\ud83d\udd0d Checking for Ollama availability...\n\u2705 Ollama is available! Using model: phi3:mini\n\ud83d\udccb Available models: deepseek-r1:1.5b, phi3:mini\n\u2705 Created CrewAI LLM wrapper successfully\n\ud83e\udde0 Using OLLAMA with model: phi3:mini\n\ud83d\udcc8 Analyzing stock: AAPL\n\ud83d\udd04 Setting up OLLAMA for stock analysis...\n\ud83e\udd99 Using Ollama model: phi3:mini\n</code></pre> \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Crew Execution Started \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502                                                                                                                     \u2502 \u2502  Crew Execution Started                                                                                             \u2502 \u2502  Name: crew                                                                                                         \u2502 \u2502  ID: 76c829e8-cd96-4ee8-8eff-78383537992f                                                                           \u2502 \u2502                                                                                                                     \u2502 \u2502                                                                                                                     \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f</p>"},{"location":"agent/examples/#agent-stock-news-researcher","title":"Agent: Stock News Researcher","text":""},{"location":"agent/examples/#task-research-the-latest-news-press-releases-and-market-sentiment-for-aapl-focus-on","title":"Task: Research the latest news, press releases, and market sentiment for AAPL. Focus on:","text":"<pre><code>        - Recent headlines\n        - Major events\n        - Analyst opinions\n        - Regulatory changes\n        - Market sentiment\n</code></pre>"},{"location":"agent/examples/#agent-stock-news-researcher_1","title":"Agent: Stock News Researcher","text":""},{"location":"agent/examples/#final-answer","title":"Final Answer:","text":"<p>As of my knowledge cutoff in early 2decu1e, I have been unable to access real-time or up-to-date market data directly. However, generally speaking, an analysis would involve the following steps and content based on historical methods for such a task:**</p> <p>Thought: Based on standard procedures as of my last update in early 2023, I can provide guidance that one should follow to gather this information using financial news platforms, stock market databases like Bloomberg Terminal or Thomson Reuters Eikon, and regulatory bodies' publications. One would also look into the latest SEC filings for AAPL (Apple Inc.) as well as other relevant sources such as press releases from Apple and industry analyst reports to compile a comprehensive report on recent headlines, major events affecting stock prices or operations of companies within their ecosystem like Alphabet (Google), TSLA(Tesla) for electric vehicles etc., since AAPL is often influenced by these tech giants.</p> <p>To understand the market sentiment specifically about Apple's performance in relation to its competitors and sector, one could utilize tools such as Google Trends or financial news aggregation services like MarketBeat that track analyst opinions. Regulatory changes would be gathered from filings with entities like the SEC which include 10-K reports where AAPL discloses material information about corporate governance and risk factors in detail, including any significant regulatory shifts impacting their operations directly or indirectly through market sentiment influences.</p> <p>For example: \"Recent headlines indicate that Apple has seen a surge in iPhone sales despite global supply chain issues affecting the electronics industry as reported by Reuters on [date]. This is largely attributed to strong brand loyalty and demand for new models, with analysts from Goldman Sachs predicting continued growth due to these factors. Major events include Apple's announcement of its latest iPhone lineup at their annual event last week which generated significant buzz across financial news outlets like CNBC and Bloomberg News.\"</p> <p>\"Market sentiment toward AAPL has been cautiously optimistic, with analysts from JP Morgan anticipating a continued positive trajectory for the company's stock due to robust product sales. However, concerns over potential regulatory changes in international markets could affect this outlook as per statements found in recent SEC filings and discussed by industry experts on platforms such as Seeking Alpha.\"</p> <p>\"Regulatory discussions around data privacy laws are expected to impact AAPL's ecosystem, with potential effects both locally within Europe through GDPR compliance efforts as well as globally. These regulations might affect Apple's research and advertising revenue streams but also present an opportunity for increased government contracting due to heightened security needs.\"</p> <p>\"As the market responds to these developments, there's a noticeable bullish sentiment among retail investors on Reddit discussions such as r/wallstreetbets while institutional traders remain more guarded but overall bearish trends are present in broader technology sector analyses.\"</p> <p>\"It should be noted that AAPL's stock price has been influenced by their partnerships with tech companies like Alphabet and TSLA, where industry-wide news such as advancements or setbacks within the electric vehicle market could sway investor confidence. For instance, a recent announcement about breakthrough battery technology from Tesla may indirectly buoy sentiment for Apple due to increased interest in clean energy solutions.\"</p> <p>\"Major events impacting AAPL's competitors like Alphabet and Amazon have been discussed extensively by analyst firms such as Piper Sandler, indicating a ripple effect on the tech sector stock prices. These reports often emphasize how shifts in consumer behavior towards digital services may also benefit Apple due to its expansive suite of streaming subscriptions.\"</p> <p>\"Analyst opinions vary widely with some forecasting continued dominance for AAPL while others suggest a more cautious approach as market dynamics shift toward other emerging tech sectors. It is advised that investors keep abreast of news from sources like the Wall Street Journal and Finan0r to maintain informed positions.\"</p> <p>\"The sentiment analysis suggests an overall positive tone in AAPL's stock but with caution due to unpredictable elements such as global trade tensions or sudden policy shifts. It is crucial for investors, especially those invested heavily in the technology sector, to monitor news closely and consider diversifying their portfolios.\"</p> <p>\"In summary, while AAPL's strong market position remains solid amidst various headwinds faced by tech companies globally, vigilance against unforeseen disruptions is key for sustained investor confidence. Staying informed through multiple reliable news sources will aid in understanding the full scope of current events and analyst opinions that impact AAPL's market sentiment.\"</p> <p>(Note: Actual financial data as it would pertain to Apple Inc., or any other specific real-time information, cannot be provided due to my knowledge cutoff. The above content is illustrative based on how such an analysis could typically unfold and should not replace up-to-date stock market research.)</p> <p>\ud83d\ude80 Crew: crew \u2514\u2500\u2500 \ud83d\udccb Task: f8d0d3aa-f49b-40af-9c01-cc60b3d86d02     Assigned to: Stock News Researcher     Status: \u2705 Completed\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Task Completion \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502                                                                                                                     \u2502 \u2502  Task Completed                                                                                                     \u2502 \u2502  Name: f8d0d3aa-f49b-40af-9c01-cc60b3d86d02                                                                         \u2502 \u2502  Agent: Stock News Researcher                                                                                       \u2502 \u2502                                                                                                                     \u2502 \u2502                                                                                                                     \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f</p>"},{"location":"agent/examples/#agent-financial-analyst","title":"Agent: Financial Analyst","text":""},{"location":"agent/examples/#task-analyze-the-financial-health-and-performance-of-aapl-consider","title":"Task: Analyze the financial health and performance of AAPL. Consider:","text":"<pre><code>        - Revenue, profit, and growth trends\n        - Valuation metrics (P/E, P/S, etc.)\n        - Recent earnings reports\n        - Industry comparisons\n        - Risks and opportunities\n</code></pre>"},{"location":"agent/examples/#agent-financial-analyst_1","title":"Agent: Financial Analyst","text":""},{"location":"agent/examples/#final-answer_1","title":"Final Answer:","text":"<p>Apple Inc.'s fiscal performance exhibits resilience, with consistent revenue and profit growth over the past few quarters despite global supply chain concerns that have pervaded the electronics sector. Year-over-year comparisons reveal an escalating trajectory in both absolute numbers and percentage terms\u2014a testament to AAPL's robust demand for new iPhone models, as corroborated by retail sales figures disclosed during their recent product launch event coverage across Bloomberg News and CNBC.</p> <p>The company maintains an impressive average P/E (Price-to-Earnings) ratio of approximately 30x over the past year, a reflection of strong investor confidence in its future earnings potential; this stands within industry benchmark ranges when juxtaposed with major competitors like Alphabet Inc. and TSLA which exhibit P/E ratios between 25-35x respectively\u2014an indication that AAPL is not only growing but doing so at a pace commensurate with its market valuations in comparison to peers within the tech sector, according to analyst reports from firms like Mizuho Securities.</p> <p>Revenue and profit growth trends have remained positive for consecutive years as evidenced by quarterly SEC filings\u2014particularly noteworthy during periods of economic headwinds that generally impact discretionary consumer spending, suggesting the strength of AAPL's brand loyalty. Despite concerns around potential regulatory changes on data privacy and their implications for research-focused revenue streams within Apple's ecosystem\u2014highlighted in recent discussions surrounding GDPR compliance efforts as noted by European Commission announcements, the overall market sentiment towards AAPL remains cautiously optimistic.</p> <p>The latest earnings reports from April 2023 show a continuation of this trend with an increase in quarterly profits and revenue that exceeded projections made earlier by analysts like those at Goldman Sachs, further buttressing their positive outlook on the company's stock trajectory. The reports also reflect operational efficiencies driven through product diversification strategies including cloud services expansion as discussed in articles from MarketWatch and analysis provided by Morgan Stanley experts\u2014which have been identified as a major contributing factor to sustained growth despite macroeconomic challenges such as inflation rates affecting the broader market.</p> <p>In considering industry comparisons, AAPL's performance stands out with its competitive edge in consumer electronics and services; however, risks associated with global supply chain volatility could present potential setbacks that have been factored into risk assessments by financial experts like those cited on Seeking Alpha.</p> <p>Moreover, AAPL's strategic partnerships within the electric vehicle space represent significant opportunities for growth as evidenced in press releases from Tesla Inc.\u2014highlighted during their latest battery technology advancements which could bolster interest and investment into greener technologies aligning with Apple's own sustainability goals, according to analyst commentary on the matter found within The New York Times.</p> <p>Furthermore, broader tech sector trends affect by shifts towards digital services\u2014as reported in findings from research firms like Piper Sandler and industry news from Bloomberg Tech Review show how AAPL's expansive suite of streaming subscriptions is leveraged for competitive advantage over other media players, aligning well with consumer behavior trends favoring on-demand content.</p> <p>The sentiment analysis across various platforms reveals a generally positive outlook among retail investors concerning Apple's stock valuations amidst cautionary advice from institutional advisories pointing to potential bearish indicators within tech sectors in response to changing market dynamics, as expressed by analyst firms like JP Morgan.</p> <p>In conclusion, while AAPL has demonstrated a strong ability to navigate through global economic and industry uncertainties with its solid financial foundations\u2014supported by historical data trends which indicate consistent performance against broader sector benchmarks established in reports from Credit Suisse Group Limited as of early 2023. Vigilant monitoring remains essential for investors, particularly those invested heavily into the technology industry to stay informed through a variety of news sources and financial databases that track company-specific developments alongside global economic indicators which can impact stock sentiment\u2014an approach suggested by Financial Times guidelines on maintaining diversified portfolios.</p> <p>(Note: Actual data, figures or recent analyst reports are not provided due to my knowledge cutoff and the nature of this simulated analysis.)</p> <p>\ud83d\ude80 Crew: crew \u251c\u2500\u2500 \ud83d\udccb Task: f8d0d3aa-f49b-40af-9c01-cc60b3d86d02 \u2502   Assigned to: Stock News Researcher \u2502   Status: \u2705 Completed \u251c\u2500\u2500 \ud83d\udccb Task: 2bc8eac6-4a63-4715-8e45-5a9a63bf93a5 \u2502   Assigned to: Financial Analyst \u2502   Status: \u2705 Completed \u2514\u2500\u2500 \ud83d\udccb Task: 6dc291df-a0f2-457c-8683-59d4b0726589     Assigned to: Investment Advisor     Status: \u2705 Completed\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Task Completion \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502                                                                                                                     \u2502 \u2502  Task Completed                                                                                                     \u2502 \u2502  Name: 6dc291df-a0f2-457c-8683-59d4b0726589                                                                         \u2502 \u2502  Agent: Investment Advisor                                                                                          \u2502 \u2502                                                                                                                     \u2502 \u2502                                                                                                                     \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f</p> <p>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Crew Completion \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502                                                                                                                     \u2502 \u2502  Crew Execution Completed                                                                                           \u2502 \u2502  Name: crew                                                                                                         \u2502 \u2502  ID: 76c829e8-cd96-4ee8-8eff-78383537992f                                                                           \u2502 \u2502                                                                                                                     \u2502 \u2502                                                                                                                     \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f</p>"},{"location":"agent/examples/#stock-analysis-for-aapl","title":"\ud83d\udcc8 Stock Analysis for AAPL","text":"<p>Based on an extensive review of financial reports, market analyses, SEC filings up until my last update in early 2023, along with expert opinions from various industry-leading firms such as Goldman Sachs and Morgan Stanley, Apple Inc. (AAPL) appears to be maintaining a strong fiscal position within the tech sector despite global supply chain issues that have affected many industries. AAPL has shown consistent revenue growth over several quarters with robust profit margins\u2014an indicator of sustained demand for their new iPhone models, as covered extensively in major financial news outlets like Bloomberg and CNBC during the recent product launch event.</p> <p>The company's market sentiment remains cautiously optimistic among both retail and institutional investors; however, it is important to consider potential regulatory changes related to data privacy that may impact AAPL's revenue streams from research activities but also offer opportunities for increased government contracting due to heightened security needs.</p> <p>AAPL's P/E ratio has remained competitive at approximately 30x over the past year, aligning with industry benchmark ranges and indicating confidence in its future earnings potential when compared to peers such as Alphabet Inc., whose own stock reflects a similar valuation trend within tech. Furthermore, analyst projections from firms like Goldman Sachs have been consistently positive for AAPL's growth trajectory with recent quarterly reports supporting this optimistic outlook through surpassing earlier forecasted earnings and revenue expectations.</p> <p>Additionally, Apple's expansion into cloud services has proven to be an effective strategy in enhancing its overall business model against economic challenges such as inflation\u2014as per findings from Morgan Stanley experts reviewed by MarketWatch\u2014and this diversification is expected to continue driving growth even amidst market fluctuations.</p> <p>In consideration of the electric vehicle (EV) partnership between Apple and TSLA, recent breakthroughs in battery technology signify an opportunity for both companies that align with consumer interest trends towards sustainable transportation solutions\u2014a sentiment echoed by analyst commentary found within The New York Times. This sector synergy presents a strategic advantage over competitors like Alphabet and could potentially contribute to the long-term growth of AAPL's business ecosystem, despite broader tech market concerns expressed in bearish trends identified among institutional advisories as reported by JP Morgan on their analyses.</p> <p>The overall financial health of Apple Inc., supported by robust quarterly and annual fiscal data showing positive revenue growth patterns over multiple years, is indicative of the company's ability to overcome industry headwinds through strong brand loyalty and operational efficiencies in product diversification\u2014particularly within its streaming subscriptions as highlighted in reports from Bloomberg Tech Review.</p> <p>Given these considerations along with a comprehensive review of analyst opinions, AAPL's stock is positioned to benefit from current market sentiments while remaining mindful of the importance of ongoing vigilance due to unpredictable global economic and industry factors that could impact investor confidence\u2014a prudent approach recommended by Financial Times for maintaining diversified portfolios.</p> <p>Based on this thorough analysis, my clear investment recommendation is a 'buy' as AAPL continues to demonstrate strong financial health with positive growth trends alongside strategic business ventures that have the potential to capitalize further into emerging markets such as EV technology and digital services expansion\u2014factors which are expected to contribute positively towards sustained performance in its stock valuation, assuming current market sentiments persist.</p>"},{"location":"agent/examples/#note-the-actual-figures-for-pe-ratios-growth-percentages-specific-analyst-projections-or-recent-financial-data-cannot-be-provided-due-to-my-knowledge-cutoff","title":"(Note: The actual figures for P/E ratios, growth percentages, specific analyst projections or recent financial data cannot be provided due to my knowledge cutoff.)","text":"<p>\u26a0\ufe0f This analysis is for informational purposes only and not financial advice.</p>"},{"location":"agent/overview/","title":"Agent Overview","text":"<p>IdeaWeaver's Agent system provides powerful AI-powered workflows for various tasks. These agents are designed to work together to accomplish complex goals through intelligent task planning and execution.</p>"},{"location":"agent/overview/#llm-requirements","title":"LLM Requirements","text":"<p>IdeaWeaver agents require a language model provider to function. The system is designed to work with:</p> <p>Ollama (Recommended)</p> <ul> <li>Free, local model inference</li> <li>No API costs</li> <li>Privacy and data security</li> <li>Offline operation capability</li> </ul> <p>OpenAI (Fallback)</p> <ul> <li>Cloud-based API access</li> <li>Paid service based on token usage</li> <li>Requires internet connection</li> <li>Requires API key</li> </ul> <p>For detailed setup instructions, see the Agent Commands page.</p>"},{"location":"agent/overview/#key-features","title":"Key Features","text":"<ul> <li>Intelligent Task Planning: Agents can break down complex tasks into manageable steps</li> <li>Multi-Agent Collaboration: Different agents can work together on related tasks</li> <li>Tool Integration: Access to various tools and APIs for enhanced capabilities</li> <li>Progress Tracking: Monitor agent activities and task completion</li> <li>Context Awareness: Maintains context across multiple interactions</li> <li>Flexible LLM Integration: Seamless switching between local and cloud models</li> </ul>"},{"location":"agent/overview/#available-agents","title":"Available Agents","text":"<ol> <li>Research Agent: Conducts thorough research on given topics</li> <li>Writer Agent: Creates high-quality content based on research</li> <li>Reviewer Agent: Ensures content quality and accuracy</li> <li>Planner Agent: Creates detailed plans for various tasks</li> <li>Analyst Agent: Performs data analysis and generates insights</li> </ol>"},{"location":"agent/overview/#getting-started","title":"Getting Started","text":"<p>To start using agents, first check your LLM setup:</p> <pre><code>ideaweaver agent check-llm\n</code></pre> <p>Then explore the various agent commands available:</p> <pre><code>ideaweaver agent --help\n</code></pre> <p>For detailed information about specific commands and LLM setup, see the Agent Commands page. </p>"},{"location":"community/contributing/","title":"Contributing to IdeaWeaver","text":"<p>Thank you for your interest in contributing to IdeaWeaver! This guide will help you get started.</p>"},{"location":"community/contributing/#getting-started","title":"Getting Started","text":"<ol> <li> <p>Fork the Repository <pre><code>git clone https://github.com/ideaweaver-ai-code/ideaweaver.git\ncd ideaweaver\n</code></pre></p> </li> <li> <p>Set Up Development Environment <pre><code>./setup_environments.sh\nsource ideaweaver-env/bin/activate\n</code></pre></p> </li> <li> <p>Install Development Dependencies <pre><code>pip install -e \".[dev]\"\n</code></pre></p> </li> </ol>"},{"location":"community/contributing/#development-guidelines","title":"Development Guidelines","text":""},{"location":"community/contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8 for Python code</li> <li>Use type hints where possible</li> <li>Add docstrings to all functions and classes</li> <li>Write meaningful commit messages</li> </ul>"},{"location":"community/contributing/#documentation","title":"Documentation","text":"<ul> <li>Update documentation for any new features</li> <li>Test documentation locally with <code>mkdocs serve</code></li> <li>Follow the existing documentation style</li> </ul>"},{"location":"community/contributing/#submitting-changes","title":"Submitting Changes","text":"<ol> <li>Create a feature branch: <code>git checkout -b feature-name</code></li> <li>Make your changes and test thoroughly</li> <li>Update documentation if needed</li> <li>Submit a pull request with a clear description</li> </ol>"},{"location":"community/contributing/#questions","title":"Questions?","text":"<ul> <li>Open an issue for bug reports or feature requests</li> <li>Join our community discussions</li> <li>Check existing issues before creating new ones</li> </ul> <p>Thank you for contributing! \ud83d\ude80 </p>"},{"location":"community/guidelines/","title":"Code Standards and Best Practices","text":""},{"location":"community/guidelines/#overview","title":"Overview","text":"<p>This document outlines the coding standards and best practices for contributing to IdeaWeaver. Following these guidelines ensures consistency, maintainability, and high-quality code across the project.</p>"},{"location":"community/guidelines/#code-style","title":"Code Style","text":""},{"location":"community/guidelines/#python","title":"Python","text":"<ul> <li>Follow PEP 8 style guide</li> <li>Use type hints for function parameters and return values</li> <li>Maximum line length: 88 characters (Black formatter default)</li> <li>Use meaningful variable and function names</li> <li>Include docstrings for all public functions and classes</li> </ul> <p>Example: <pre><code>from typing import List, Optional\n\ndef process_model_output(\n    output: List[str],\n    threshold: float = 0.5,\n    max_length: Optional[int] = None\n) -&gt; List[str]:\n    \"\"\"\n    Process model output with optional filtering.\n\n    Args:\n        output: List of model output strings\n        threshold: Confidence threshold for filtering\n        max_length: Maximum length for each output string\n\n    Returns:\n        List of processed output strings\n    \"\"\"\n    # Implementation\n    pass\n</code></pre></p>"},{"location":"community/guidelines/#documentation","title":"Documentation","text":"<ul> <li>Use Markdown for all documentation</li> <li>Include code examples where appropriate</li> <li>Keep documentation up-to-date with code changes</li> <li>Use clear and concise language</li> <li>Include screenshots for UI-related features</li> </ul>"},{"location":"community/guidelines/#git-workflow","title":"Git Workflow","text":""},{"location":"community/guidelines/#branch-naming","title":"Branch Naming","text":"<ul> <li>Feature branches: <code>feature/description</code></li> <li>Bug fixes: <code>fix/description</code></li> <li>Documentation: <code>docs/description</code></li> <li>Hotfixes: <code>hotfix/description</code></li> </ul>"},{"location":"community/guidelines/#commit-messages","title":"Commit Messages","text":"<p>Follow the Conventional Commits specification:</p> <pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;description&gt;\n\n[optional body]\n\n[optional footer]\n</code></pre> <p>Types: - <code>feat</code>: New feature - <code>fix</code>: Bug fix - <code>docs</code>: Documentation changes - <code>style</code>: Code style changes - <code>refactor</code>: Code refactoring - <code>test</code>: Adding or modifying tests - <code>chore</code>: Maintenance tasks</p>"},{"location":"community/guidelines/#pull-requests","title":"Pull Requests","text":"<ol> <li>Create a descriptive PR title</li> <li>Fill out the PR template completely</li> <li>Link related issues</li> <li>Request reviews from relevant team members</li> <li>Ensure CI checks pass</li> <li>Address review comments promptly</li> </ol>"},{"location":"community/guidelines/#testing","title":"Testing","text":""},{"location":"community/guidelines/#unit-tests","title":"Unit Tests","text":"<ul> <li>Write tests for all new features</li> <li>Maintain minimum 80% code coverage</li> <li>Use pytest for testing</li> <li>Mock external dependencies</li> </ul> <p>Example: <pre><code>import pytest\nfrom ideaweaver.core import ModelProcessor\n\ndef test_model_processor():\n    processor = ModelProcessor()\n    result = processor.process(\"test input\")\n    assert result is not None\n    assert isinstance(result, str)\n</code></pre></p>"},{"location":"community/guidelines/#integration-tests","title":"Integration Tests","text":"<ul> <li>Test component interactions</li> <li>Include end-to-end workflows</li> <li>Test error handling</li> <li>Verify API endpoints</li> </ul>"},{"location":"community/guidelines/#security","title":"Security","text":""},{"location":"community/guidelines/#code-security","title":"Code Security","text":"<ul> <li>Never commit sensitive data</li> <li>Use environment variables for secrets</li> <li>Validate all user inputs</li> <li>Follow security best practices</li> <li>Regular security audits</li> </ul>"},{"location":"community/guidelines/#api-security","title":"API Security","text":"<ul> <li>Implement rate limiting</li> <li>Use proper authentication</li> <li>Validate API requests</li> <li>Handle errors gracefully</li> <li>Log security events</li> </ul>"},{"location":"community/guidelines/#performance","title":"Performance","text":""},{"location":"community/guidelines/#code-optimization","title":"Code Optimization","text":"<ul> <li>Profile code regularly</li> <li>Optimize critical paths</li> <li>Use appropriate data structures</li> <li>Implement caching where beneficial</li> <li>Monitor memory usage</li> </ul>"},{"location":"community/guidelines/#best-practices","title":"Best Practices","text":"<ol> <li>Error Handling</li> <li>Use specific exception types</li> <li>Provide meaningful error messages</li> <li>Log errors appropriately</li> <li> <p>Handle edge cases</p> </li> <li> <p>Logging</p> </li> <li>Use appropriate log levels</li> <li>Include context in log messages</li> <li>Rotate log files</li> <li> <p>Monitor log patterns</p> </li> <li> <p>Configuration</p> </li> <li>Use environment variables</li> <li>Provide default values</li> <li>Validate configuration</li> <li>Document all options</li> </ol>"},{"location":"community/guidelines/#review-process","title":"Review Process","text":"<ol> <li>Self-Review</li> <li>Run linters</li> <li>Check test coverage</li> <li>Verify documentation</li> <li> <p>Test functionality</p> </li> <li> <p>Peer Review</p> </li> <li>Code quality</li> <li>Performance impact</li> <li>Security considerations</li> <li>Documentation accuracy</li> </ol>"},{"location":"community/guidelines/#resources","title":"Resources","text":"<ul> <li>Python Style Guide</li> <li>Conventional Commits</li> <li>GitHub Flow</li> <li>Testing Best Practices </li> </ul>"},{"location":"community/roadmap/","title":"IdeaWeaver Roadmap","text":""},{"location":"community/roadmap/#overview","title":"Overview","text":"<p>This document outlines the planned features, improvements, and milestones for IdeaWeaver. We welcome community feedback and contributions to help shape the future of the project.</p>"},{"location":"community/roadmap/#current-quarter-q2-2025","title":"Current Quarter (Q2 2025)","text":""},{"location":"community/roadmap/#model-training-fine-tuning","title":"Model Training &amp; Fine-tuning","text":"<ul> <li>[ ] Support for more model architectures</li> <li>[ ] Improved distributed training capabilities</li> <li>[ ] Enhanced LoRA/QLoRA optimization</li> <li>[ ] Automated hyperparameter tuning</li> <li>[ ] Better memory management for large models</li> </ul>"},{"location":"community/roadmap/#rag-system-improvements","title":"RAG System Improvements","text":"<ul> <li>[ ] Multi-vector store support</li> <li>[ ] Advanced chunking strategies</li> <li>[ ] Improved retrieval algorithms</li> <li>[ ] Better context management</li> <li>[ ] Enhanced evaluation metrics</li> </ul>"},{"location":"community/roadmap/#mcp-integration","title":"MCP Integration","text":"<ul> <li>[ ] Additional service integrations</li> <li>[ ] Improved error handling</li> <li>[ ] Better authentication management</li> <li>[ ] Enhanced logging and monitoring</li> <li>[ ] More comprehensive documentation</li> </ul>"},{"location":"community/roadmap/#next-quarter-q3-2025","title":"Next Quarter (Q3 2025)","text":""},{"location":"community/roadmap/#enterprise-features","title":"Enterprise Features","text":"<ul> <li>[ ] Role-based access control</li> <li>[ ] Audit logging</li> <li>[ ] Compliance reporting</li> <li>[ ] Enterprise SSO integration</li> <li>[ ] Advanced security features</li> </ul>"},{"location":"community/roadmap/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>[ ] Improved inference speed</li> <li>[ ] Better resource utilization</li> <li>[ ] Enhanced caching mechanisms</li> <li>[ ] Optimized memory usage</li> <li>[ ] Reduced latency</li> </ul>"},{"location":"community/roadmap/#developer-experience","title":"Developer Experience","text":"<ul> <li>[ ] Enhanced CLI interface</li> <li>[ ] Better error messages</li> <li>[ ] Improved debugging tools</li> <li>[ ] More comprehensive testing</li> <li>[ ] Better documentation</li> </ul>"},{"location":"community/roadmap/#future-plans-q4-2025-and-beyond","title":"Future Plans (Q4 2025 and Beyond)","text":""},{"location":"community/roadmap/#advanced-features","title":"Advanced Features","text":"<ul> <li>[ ] Multi-modal model support</li> <li>[ ] Advanced agent capabilities</li> <li>[ ] Custom model architectures</li> <li>[ ] Enhanced evaluation frameworks</li> <li>[ ] Advanced deployment options</li> </ul>"},{"location":"community/roadmap/#platform-improvements","title":"Platform Improvements","text":"<ul> <li>[ ] Cloud-native deployment</li> <li>[ ] Kubernetes integration</li> <li>[ ] Serverless deployment options</li> <li>[ ] Enhanced monitoring</li> <li>[ ] Better scalability</li> </ul>"},{"location":"community/roadmap/#community-features","title":"Community Features","text":"<ul> <li>[ ] Model sharing marketplace</li> <li>[ ] Community benchmarks</li> <li>[ ] Collaborative training</li> <li>[ ] Knowledge base</li> <li>[ ] Community plugins</li> </ul>"},{"location":"community/roadmap/#contributing-to-the-roadmap","title":"Contributing to the Roadmap","text":"<p>We welcome community input on our roadmap. Here's how you can contribute:</p> <ol> <li>Submit Feature Requests</li> <li>Use GitHub Issues</li> <li>Provide detailed descriptions</li> <li>Include use cases</li> <li> <p>Suggest implementation approaches</p> </li> <li> <p>Join Discussions</p> </li> <li>Participate in GitHub Discussions</li> <li>Join our community meetings</li> <li>Share your ideas</li> <li> <p>Provide feedback</p> </li> <li> <p>Contribute Code</p> </li> <li>Follow our Guidelines</li> <li>Submit pull requests</li> <li>Write tests</li> <li>Update documentation</li> </ol>"},{"location":"community/roadmap/#priority-areas","title":"Priority Areas","text":"<ol> <li>Performance</li> <li>Faster training</li> <li>Better resource utilization</li> <li>Reduced memory usage</li> <li> <p>Improved inference speed</p> </li> <li> <p>Usability</p> </li> <li>Better error handling</li> <li>Improved documentation</li> <li>Enhanced CLI experience</li> <li> <p>More examples</p> </li> <li> <p>Enterprise</p> </li> <li>Security features</li> <li>Compliance tools</li> <li>Monitoring capabilities</li> <li> <p>Integration options</p> </li> <li> <p>Community</p> </li> <li>Better collaboration tools</li> <li>More documentation</li> <li>Enhanced support</li> <li>Community features</li> </ol>"},{"location":"community/roadmap/#timeline","title":"Timeline","text":"Quarter Focus Areas Key Milestones Q2 2025 Core Features Enhanced training, RAG improvements Q3 2025 Enterprise Security, compliance, monitoring Q4 2025 Advanced Features Multi-modal, advanced agents 2026 Platform Cloud-native, scalability"},{"location":"community/roadmap/#feedback","title":"Feedback","text":"<p>We value your feedback! Please help us improve by:</p> <ol> <li>Opening GitHub issues</li> <li>Participating in discussions</li> <li>Contributing code</li> <li>Sharing your experiences</li> <li>Suggesting improvements</li> </ol>"},{"location":"community/roadmap/#resources","title":"Resources","text":"<ul> <li>GitHub Repository</li> <li>Documentation</li> <li>Community Guidelines</li> <li>Contributing Guide </li> </ul>"},{"location":"community/support/","title":"Support","text":"<p>Need help with IdeaWeaver? Here are the best ways to get assistance.</p>"},{"location":"community/support/#quick-help","title":"Quick Help","text":""},{"location":"community/support/#common-issues","title":"Common Issues","text":"<p>Command not found: Make sure your virtual environment is activated <pre><code>source ideaweaver-env/bin/activate\n</code></pre></p> <p>Import errors: Reinstall requirements if packages are missing <pre><code>pip install -r requirements.txt --force-reinstall\n</code></pre></p> <p>GPU not detected: Check CUDA installation <pre><code>python -c \"import torch; print(torch.cuda.is_available())\"\n</code></pre></p>"},{"location":"community/support/#documentation","title":"Documentation","text":"<ul> <li>Installation Guide</li> <li>Quick Start</li> <li>CLI Reference</li> </ul>"},{"location":"community/support/#getting-help","title":"Getting Help","text":""},{"location":"community/support/#github-issues","title":"GitHub Issues","text":"<p>For bug reports and feature requests: - Search existing issues first - Use the provided templates - Include relevant system information - Provide steps to reproduce</p>"},{"location":"community/support/#community-support","title":"Community Support","text":"<ul> <li>GitHub Discussions: General questions and community help</li> </ul>"},{"location":"community/support/#faq","title":"FAQ","text":""},{"location":"community/support/#installation-issues","title":"Installation Issues","text":"<p>Q: Python 3.12 not found</p> <p>A: Use the automated setup script which handles Python installation</p> <p>Q: Setup script fails</p> <p>A: Check the troubleshooting section in the installation guide</p>"},{"location":"community/support/#usage-questions","title":"Usage Questions","text":"<p>Q: How do I fine-tune my own model?</p> <p>A: See the Quick Start guide for LoRA fine-tuning examples</p> <p>Q: Can I use local models?</p> <p>A: Yes! IdeaWeaver supports local models and Hugging Face models</p>"},{"location":"community/support/#stay-updated","title":"Stay Updated","text":"<ul> <li>\u2b50 Star the repository on GitHub</li> <li>\ud83d\udce2 Follow us on Twitter @ideaweaver_ai</li> </ul>"},{"location":"comparison/commands/","title":"Model Comparison Commands","text":"<p>The <code>compare</code> command in IdeaWeaver provides a comprehensive interface for comparing multiple language models using the lm-evaluation-harness framework.</p>"},{"location":"comparison/commands/#basic-usage","title":"Basic Usage","text":"<pre><code>ideaweaver compare [OPTIONS] MODEL_PATHS...\n</code></pre>"},{"location":"comparison/commands/#arguments","title":"Arguments","text":"<ul> <li><code>MODEL_PATHS</code>: One or more paths to the models to compare (required)</li> </ul>"},{"location":"comparison/commands/#options","title":"Options","text":"<ul> <li><code>-t, --tasks TEXT</code>: Comma-separated list of tasks to evaluate on (required)</li> <li><code>-wp, --wandb-project TEXT</code>: Weights &amp; Biases project name</li> <li><code>-we, --wandb-entity TEXT</code>: Weights &amp; Biases entity name</li> <li><code>-d, --device TEXT</code>: Device to run evaluation on (auto, cuda, cpu)</li> <li><code>-b, --batch-size INTEGER</code>: Batch size for evaluation</li> <li><code>-f, --num-fewshot INTEGER</code>: Number of few-shot examples</li> <li><code>-l, --limit INTEGER</code>: Limit number of samples for testing</li> <li><code>--generate-report</code>: Generate markdown comparison report</li> <li><code>-v, --verbose</code>: Verbose output</li> </ul>"},{"location":"comparison/commands/#examples","title":"Examples","text":""},{"location":"comparison/commands/#basic-comparison","title":"Basic Comparison","text":"<pre><code>ideaweaver compare \\\n  models/gpt2 \\\n  models/bert-base-uncased \\\n  --tasks gsm8k,truthfulqa \\\n  --batch-size 8\n</code></pre> <p>Example output: <pre><code>\ud83d\udd0d Comparing models:\n   - models/gpt2\n   - models/bert-base-uncased\n\n\ud83d\udcca Running tasks:\n   - gsm8k\n   - truthfulqa\n\n\ud83d\ude80 Starting evaluation...\n\u2705 Evaluation complete\n\n\ud83d\udccb Results:\nModel: gpt2\n- gsm8k: 0.45\n- truthfulqa: 0.62\n\nModel: bert-base-uncased\n- gsm8k: 0.38\n- truthfulqa: 0.58\n</code></pre></p>"},{"location":"comparison/commands/#with-weights-biases-integration","title":"With Weights &amp; Biases Integration","text":"<pre><code>ideaweaver compare \\\n  models/gpt2 \\\n  models/bert-base-uncased \\\n  --tasks gsm8k,truthfulqa \\\n  --wandb-project model-comparison \\\n  --wandb-entity your-username \\\n  --generate-report\n</code></pre> <p>Example output: <pre><code>\ud83d\udd0d Comparing models:\n   - models/gpt2\n   - models/bert-base-uncased\n\n\ud83d\udcca Running tasks:\n   - gsm8k\n   - truthfulqa\n\n\ud83d\udd17 Weights &amp; Biases integration enabled\n   Project: model-comparison\n   Entity: your-username\n\n\ud83d\ude80 Starting evaluation...\n\u2705 Evaluation complete\n\n\ud83d\udccb Results:\nModel: gpt2\n- gsm8k: 0.45\n- truthfulqa: 0.62\n\nModel: bert-base-uncased\n- gsm8k: 0.38\n- truthfulqa: 0.58\n\n\ud83d\udcdd Generating comparison report...\n\u2705 Report saved to: comparison_report.md\n</code></pre></p>"},{"location":"comparison/commands/#advanced-usage","title":"Advanced Usage","text":"<pre><code>ideaweaver compare \\\n  models/gpt2 \\\n  models/bert-base-uncased \\\n  --tasks gsm8k,truthfulqa \\\n  --device cuda \\\n  --batch-size 16 \\\n  --num-fewshot 3 \\\n  --limit 100 \\\n  --verbose\n</code></pre> <p>Example output: <pre><code>\ud83d\udd0d Comparing models:\n   - models/gpt2\n   - models/bert-base-uncased\n\n\ud83d\udcca Configuration:\n   Tasks: gsm8k, truthfulqa\n   Device: cuda\n   Batch size: 16\n   Few-shot examples: 3\n   Sample limit: 100\n\n\ud83d\ude80 Starting evaluation...\n\ud83d\udcca Progress:\n   - Loading models...\n   - Preparing datasets...\n   - Running evaluations...\n   - Computing metrics...\n\n\u2705 Evaluation complete\n\n\ud83d\udccb Detailed Results:\nModel: gpt2\n- gsm8k:\n  * Accuracy: 0.45\n  * Average time: 1.2s\n  * Memory usage: 2.1GB\n- truthfulqa:\n  * Accuracy: 0.62\n  * Average time: 0.8s\n  * Memory usage: 1.8GB\n\nModel: bert-base-uncased\n- gsm8k:\n  * Accuracy: 0.38\n  * Average time: 0.9s\n  * Memory usage: 1.5GB\n- truthfulqa:\n  * Accuracy: 0.58\n  * Average time: 0.7s\n  * Memory usage: 1.3GB\n</code></pre></p>"},{"location":"comparison/commands/#best-practices","title":"Best Practices","text":"<ol> <li>Task Selection: Choose relevant tasks for your use case</li> <li>Resource Management: Adjust batch size based on available memory</li> <li>Weights &amp; Biases: Use for tracking and sharing results</li> <li>Report Generation: Always generate reports for documentation</li> <li>Verbose Mode: Use when debugging or analyzing performance </li> </ol>"},{"location":"comparison/overview/","title":"Model Comparison","text":"<p>The model comparison feature in IdeaWeaver allows you to evaluate and compare multiple language models using the lm-evaluation-harness framework. This powerful tool helps you make data-driven decisions about model selection and performance.</p>"},{"location":"comparison/overview/#features","title":"Features","text":"<ul> <li>Multiple Model Comparison: Compare any number of models side by side</li> <li>Comprehensive Benchmarks: Access to a wide range of evaluation tasks</li> <li>Weights &amp; Biases Integration: Track and visualize comparison results</li> <li>Customizable Evaluation: Control batch size, few-shot examples, and more</li> <li>Detailed Reports: Generate markdown comparison reports</li> </ul>"},{"location":"comparison/overview/#supported-tasks","title":"Supported Tasks","text":"<p>The comparison tool supports various benchmark tasks from lm-evaluation-harness, including:</p> <ul> <li>Language Understanding: GLUE, SuperGLUE, RACE</li> <li>Reasoning: GSM8K, MATH, HumanEval</li> <li>Knowledge: MMLU, TruthfulQA</li> <li>Generation: LAMBADA, StoryCloze</li> <li>And many more: Check available tasks using <code>ideaweaver list-tasks</code></li> </ul>"},{"location":"comparison/overview/#integration-with-weights-biases","title":"Integration with Weights &amp; Biases","text":"<p>The comparison feature integrates with Weights &amp; Biases for: - Experiment tracking - Performance visualization - Result sharing - Historical comparisons</p>"},{"location":"comparison/overview/#example-use-cases","title":"Example Use Cases","text":"<ol> <li>Model Selection: Compare different models for a specific task</li> <li>Version Comparison: Evaluate improvements between model versions</li> <li>Hyperparameter Tuning: Compare models with different configurations</li> <li>Architecture Analysis: Compare different model architectures</li> <li>Resource Optimization: Find the best model for your resource constraints</li> </ol>"},{"location":"comparison/overview/#best-practices","title":"Best Practices","text":"<ol> <li>Task Selection: Choose relevant tasks for your use case</li> <li>Resource Management: Consider batch size and device selection</li> <li>Documentation: Keep track of model versions and configurations</li> <li>Regular Updates: Re-run comparisons when new models are available</li> <li>Result Analysis: Use generated reports for detailed analysis </li> </ol>"},{"location":"downloads/commands/","title":"Download Commands","text":""},{"location":"downloads/commands/#basic-download","title":"Basic Download","text":"<p>Download a model from Hugging Face Hub:</p> <pre><code>ideaweaver download &lt;model_name&gt;\n</code></pre>"},{"location":"downloads/commands/#example","title":"Example","text":"<pre><code>ideaweaver download microsoft/DialoGPT-medium\n</code></pre>"},{"location":"downloads/commands/#advanced-options","title":"Advanced Options","text":""},{"location":"downloads/commands/#specify-output-directory","title":"Specify Output Directory","text":"<p>Download a model to a specific directory:</p> <pre><code>ideaweaver download &lt;model_name&gt; --save-path &lt;path&gt;\n</code></pre>"},{"location":"downloads/commands/#example_1","title":"Example","text":"<pre><code>ideaweaver download microsoft/DialoGPT-medium --save-path ./my_models\n</code></pre>"},{"location":"downloads/commands/#troubleshooting","title":"Troubleshooting","text":""},{"location":"downloads/commands/#common-issues","title":"Common Issues","text":"<p>Download Fails</p> <ul> <li>Check your internet connection</li> <li>Verify the model name is correct</li> <li>Ensure you have sufficient disk space</li> </ul> <p>File Verification Fails</p> <ul> <li>Check for disk space issues</li> <li>Verify file permissions</li> </ul>"},{"location":"downloads/overview/","title":"Model Downloads","text":"<p>IdeaWeaver provides a simple and efficient way to download models from the Hugging Face Hub. The download functionality allows you to:</p> <ul> <li>Download models directly from Hugging Face Hub</li> <li>Specify model variants and versions</li> <li>Automatically handle model dependencies</li> <li>Verify downloaded model integrity</li> </ul>"},{"location":"downloads/overview/#basic-usage","title":"Basic Usage","text":"<p>The simplest way to download a model is using the <code>download</code> command:</p> <pre><code>ideaweaver download &lt;model_name&gt;\n</code></pre> <p>For example, to download the DialoGPT-medium model:</p> <pre><code>ideaweaver download microsoft/DialoGPT-medium\n</code></pre>"},{"location":"downloads/overview/#output-example","title":"Output Example","text":"<p>When downloading a model, you'll see output similar to this:</p> <pre><code>\ud83d\udd0d Downloading model: microsoft/DialoGPT-medium\n\ud83d\udce6 Model files:\n   - config.json\n   - pytorch_model.bin\n   - tokenizer.json\n   - tokenizer_config.json\n   - special_tokens_map.json\n\u2705 Model downloaded successfully to: ./models/microsoft/DialoGPT-medium\n</code></pre>"},{"location":"downloads/overview/#model-directory-structure","title":"Model Directory Structure","text":"<p>After downloading, the model will be stored in a directory structure like this:</p> <pre><code>./models/\n\u2514\u2500\u2500 microsoft/\n    \u2514\u2500\u2500 DialoGPT-medium/\n        \u251c\u2500\u2500 config.json\n        \u251c\u2500\u2500 pytorch_model.bin\n        \u251c\u2500\u2500 tokenizer.json\n        \u251c\u2500\u2500 tokenizer_config.json\n        \u2514\u2500\u2500 special_tokens_map.json\n</code></pre>"},{"location":"downloads/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about download commands for advanced options</li> <li>Check out the fine-tuning guide to start training with your downloaded model</li> <li>Explore RAG capabilities to use your model in a retrieval-augmented generation system </li> </ul>"},{"location":"evaluation/evaluation/","title":"Evaluation","text":"<p>Welcome to the Evaluation section! Here you'll find: - How to run model evaluation with IdeaWeaver - Examples for both TensorBoard and Weights &amp; Biases (wandb) - Screenshots and sample outputs - Detailed command-line options and examples</p> <p></p>"},{"location":"evaluation/evaluation/#command-line-options","title":"Command Line Options","text":"<pre><code>ideaweaver evaluate --help\n\nUsage: -c evaluate [OPTIONS] MODEL_PATH\n\n  Evaluate a language model on various benchmarks.\n\nOptions:\n  -t, --tasks TEXT                Comma-separated list of tasks to evaluate on\n  -s, --benchmark-suite [standard|reasoning|knowledge|comprehensive|custom]\n                                  Predefined benchmark suite to use\n  -wp, --wandb-project TEXT       Weights &amp; Biases project name\n  -we, --wandb-entity TEXT        Weights &amp; Biases entity name\n  -d, --device TEXT               Device to run evaluation on (auto, cuda, cpu)\n  -b, --batch-size INTEGER        Batch size for evaluation\n  -f, --num-fewshot INTEGER       Number of few-shot examples\n  -l, --limit INTEGER             Limit number of samples for testing\n  -o, --output-path TEXT          Path to save evaluation results\n  --generate-report               Generate markdown evaluation report\n  --report-to [wandb|tensorboard|both|none]\n                                  Experiment tracking platform\n  --tensorboard-project TEXT      TensorBoard project name\n  --tensorboard-experiment TEXT   Custom TensorBoard experiment name\n  -v, --verbose                   Verbose output\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"evaluation/evaluation/#basic-evaluation-example","title":"Basic Evaluation Example","text":"<p>Here's a basic example of running evaluation:</p> <pre><code>ideaweaver evaluate ./downloaded_model \\\n  --tasks hellaswag,arc_easy,winogrande \\\n  --batch-size 2 \\\n  --limit 2 \\\n  --output-path results.json \\\n  --report-to none \\\n  --verbose\n</code></pre>"},{"location":"evaluation/evaluation/#understanding-the-tasks","title":"Understanding the Tasks","text":"<ul> <li>HellaSwag \u2013 commonsense story completion</li> <li>ARC-Easy \u2013 grade-school science multiple choice</li> <li>Winogrande \u2013 pronoun resolution / commonsense coreference</li> </ul>"},{"location":"evaluation/evaluation/#key-parameters-explained","title":"Key Parameters Explained","text":"<ul> <li><code>--limit 2</code>: Only scores the first two examples of each task (for pipeline testing)</li> <li><code>--batch_size 2</code>: Feeds two prompts to the model at a time (useful for larger models on limited GPUs)</li> <li><code>--device auto</code>: Automatically selects CUDA if available, otherwise uses CPU</li> <li><code>--log_samples</code>: Generates per-example JSON in addition to aggregate metrics</li> </ul>"},{"location":"evaluation/evaluation/#sample-output-analysis","title":"Sample Output Analysis","text":""},{"location":"evaluation/evaluation/#example-output-with-limit10","title":"Example Output with Limit=10","text":"<pre><code>\ud83d\udccb Evaluation output:\nhf (pretrained=./downloaded_model), gen_kwargs: (None), limit: 10.0, num_fewshot: None, batch_size: 2\n|  Tasks   |Version|Filter|n-shot| Metric |   |Value|   |Stderr|\n|----------|------:|------|-----:|--------|---|----:|---|-----:|\n|arc_easy  |      1|none  |     0|acc     |\u2191  |  0.4|\u00b1  |0.1633|\n|          |       |none  |     0|acc_norm|\u2191  |  0.3|\u00b1  |0.1528|\n|hellaswag |      1|none  |     0|acc     |\u2191  |  0.3|\u00b1  |0.1528|\n|          |       |none  |     0|acc_norm|\u2191  |  0.3|\u00b1  |0.1528|\n|winogrande|      1|none  |     0|acc     |\u2191  |  0.7|\u00b1  |0.1528|\n</code></pre>"},{"location":"evaluation/evaluation/#understanding-the-metrics","title":"Understanding the Metrics","text":"<ul> <li>Raw scores (10 questions per benchmark):</li> <li>ARC-Easy: 4/10 correct \u2192 0.4</li> <li>HellaSwag: 3/10 correct \u2192 0.3</li> <li> <p>Winogrande: 7/10 correct \u2192 0.7</p> </li> <li> <p>Metric Types:</p> </li> <li><code>acc</code>: Raw accuracy</li> <li><code>acc_norm</code>: Normalized accuracy (after cleanup like stripping articles)</li> <li><code>StdErr</code>: Indicates confidence in the score (\u00b10.15 means \u00b115% potential variation)</li> </ul>"},{"location":"evaluation/evaluation/#tensorboard-example","title":"TensorBoard Example","text":"<pre><code>ideaweaver evaluate ./my-qwen2-model \\\n  --tasks hellaswag,arc_easy \\\n  --batch-size 2 \\\n  --output-path ./evaluation_results.json \\\n  --generate-report \\\n  --report-to tensorboard \\\n  --tensorboard-project my-eval-project \\\n  --limit 2 \\\n  --verbose\n</code></pre>"},{"location":"evaluation/evaluation/#starting-tensorboard-server","title":"Starting TensorBoard Server","text":"<pre><code>pip install tensorboard  # Already included in installation\nnohup tensorboard --logdir logs/ &amp;\n# Access at http://localhost:6006/\n</code></pre>"},{"location":"evaluation/evaluation/#tensorboard-ui-example","title":"TensorBoard UI Example","text":""},{"location":"evaluation/evaluation/#weights-biases-wandb-example","title":"Weights &amp; Biases (wandb) Example","text":""},{"location":"evaluation/evaluation/#basic-wandb-setup","title":"Basic wandb Setup","text":"<pre><code>ideaweaver evaluate ./downloaded_model \\\n  --tasks hellaswag,arc_easy \\\n  --batch-size 2 \\\n  --output-path ./wandb_results \\\n  --generate-report \\\n  --report-to wandb \\\n  --wandb-project my-wandb-eval-project \\\n  --wandb-entity laprashant-startup \\\n  --limit 100 \\\n  --verbose\n</code></pre>"},{"location":"evaluation/evaluation/#using-both-platforms","title":"Using Both Platforms","text":"<pre><code>ideaweaver evaluate ./downloaded_model \\\n  --tasks hellaswag,arc_easy \\\n  --batch-size 2 \\\n  --output-path ./wandb_results \\\n  --generate-report \\\n  --report-to both \\\n  --wandb-project my-wandb-eval-project \\\n  --wandb-entity laprashant-startup \\\n  --tensorboard-project my-tensorboard-eval-project \\\n  --limit 100 \\\n  --verbose\n</code></pre>"},{"location":"evaluation/evaluation/#wandb-ui-example","title":"wandb UI Example","text":""},{"location":"evaluation/evaluation/#wandb-setup-notes","title":"wandb Setup Notes","text":"<ul> <li>Run <code>wandb login</code> once to set up authentication</li> <li>Add <code>--report-to wandb</code> to enable wandb logging</li> <li>Optionally specify <code>--wandb-project</code> and <code>--wandb-entity</code></li> <li>Note: Evaluation logs only summary metrics at the end, not per-batch metrics </li> </ul>"},{"location":"fine-tuning/commands/","title":"Fine-tuning Commands","text":""},{"location":"fine-tuning/commands/#basic-usage","title":"Basic Usage","text":"<pre><code>ideaweaver finetune [OPTIONS] COMMAND [ARGS]\n</code></pre>"},{"location":"fine-tuning/commands/#available-methods","title":"Available Methods","text":"<ul> <li><code>full</code>: Full parameter fine-tuning</li> <li><code>lora</code>: LoRA fine-tuning</li> <li><code>qlora</code>: QLoRA fine-tuning</li> </ul>"},{"location":"fine-tuning/commands/#full-fine-tuning-example","title":"Full Fine-tuning Example","text":"<pre><code>ideaweaver finetune full \\\n  --model microsoft/DialoGPT-small \\\n  --dataset datasets/instruction_following_sample.json \\\n  --output-dir ./test_full_basic \\\n  --epochs 5 \\\n  --batch-size 2 \\\n  --gradient-accumulation-steps 2 \\\n  --learning-rate 5e-5 \\\n  --max-seq-length 256 \\\n  --gradient-checkpointing \\\n  --verbose\n</code></pre>"},{"location":"fine-tuning/commands/#common-options","title":"Common Options","text":"Option Description Default <code>--model</code> Base model name or path Required <code>--dataset</code> Path to dataset file Required <code>--output-dir</code> Directory to save model Required <code>--epochs</code> Number of training epochs 3 <code>--batch-size</code> Training batch size 8 <code>--learning-rate</code> Learning rate 2e-5 <code>--max-seq-length</code> Maximum sequence length 512 <code>--gradient-checkpointing</code> Enable gradient checkpointing False <code>--gradient-accumulation-steps</code> Gradient accumulation steps 1 <code>--verbose</code> Enable verbose output False"},{"location":"fine-tuning/commands/#lora-fine-tuning-example","title":"LoRA Fine-tuning Example","text":"<pre><code>ideaweaver finetune lora \\\n  --model microsoft/DialoGPT-small \\\n  --dataset datasets/instruction_following_sample.json \\\n  --output-dir ./test_lora \\\n  --epochs 3 \\\n  --batch-size 4 \\\n  --learning-rate 1e-4 \\\n  --lora-rank 8 \\\n  --lora-alpha 16 \\\n  --verbose\n</code></pre>"},{"location":"fine-tuning/commands/#qlora-fine-tuning-example","title":"QLoRA Fine-tuning Example","text":"<pre><code>ideaweaver finetune qlora \\\n  --model microsoft/DialoGPT-small \\\n  --dataset datasets/instruction_following_sample.json \\\n  --output-dir ./test_qlora \\\n  --epochs 3 \\\n  --batch-size 4 \\\n  --learning-rate 1e-4 \\\n  --lora-rank 8 \\\n  --lora-alpha 16 \\\n  --bits 4 \\\n  --verbose\n</code></pre>"},{"location":"fine-tuning/commands/#additional-options","title":"Additional Options","text":""},{"location":"fine-tuning/commands/#experiment-tracking","title":"Experiment Tracking","text":"<pre><code># With MLflow\nideaweaver finetune full \\\n  --model microsoft/DialoGPT-small \\\n  --dataset datasets/instruction_following_sample.json \\\n  --output-dir ./test_full_basic \\\n  --track-experiments \\\n  --mlflow-uri http://127.0.0.1:5000 \\\n  --mlflow-experiment \"MyExperiment\"\n\n# With Weights &amp; Biases\nideaweaver finetune full \\\n  --model microsoft/DialoGPT-small \\\n  --dataset datasets/instruction_following_sample.json \\\n  --output-dir ./test_full_basic \\\n  --track-experiments \\\n  --wandb-project \"MyProject\" \\\n  --wandb-entity \"MyEntity\"\n</code></pre> <p>For more examples and use cases, see the Examples page. </p>"},{"location":"fine-tuning/examples/","title":"Fine-tuning Examples","text":""},{"location":"fine-tuning/examples/#full-fine-tuning-example","title":"Full Fine-tuning Example","text":""},{"location":"fine-tuning/examples/#command","title":"Command","text":"<pre><code>ideaweaver finetune full \\\n  --model microsoft/DialoGPT-small \\\n  --dataset datasets/instruction_following_sample.json \\\n  --output-dir ./test_full_basic \\\n  --epochs 5 \\\n  --batch-size 2 \\\n  --gradient-accumulation-steps 2 \\\n  --learning-rate 5e-5 \\\n  --max-seq-length 256 \\\n  --gradient-checkpointing \\\n  --verbose\n</code></pre>"},{"location":"fine-tuning/examples/#output","title":"Output","text":"<pre><code>\ud83d\ude80 Supervised Fine-Tuner initialized\n   Model: microsoft/DialoGPT-small\n   Method: full\n   Task: instruction_following\n\ud83d\udd27 Setting up model and tokenizer...\n\u2705 Model and tokenizer setup complete\n   Model parameters: 124,439,808\n\ud83d\udcca Preparing dataset from: datasets/instruction_following_sample.json\n\u2705 Dataset prepared\n   Training samples: 5\n   Evaluation samples: 1\n\ud83d\udd27 Setting up trainer...\n\u2705 Trainer setup complete\n\ud83d\ude80 Starting fine-tuning...\n{'loss': 26.2043, 'grad_norm': nan, 'learning_rate': 1.5076844803522922e-06, 'epoch': 5.0}\n{'train_runtime': 13.3988, 'train_samples_per_second': 1.866, 'train_steps_per_second': 0.746, 'train_loss': 26.204254150390625, 'epoch': 5.0}\n\u2705 Fine-tuning completed!\n\ud83d\udcc1 Model saved to: ./test_full_basic\n\ud83d\udcca Evaluating model...\n{'eval_loss': nan, 'eval_runtime': 0.1933, 'eval_samples_per_second': 5.175, 'eval_steps_per_second': 5.175, 'epoch': 5.0}\n\u2705 Evaluation complete\n   eval_loss: nan\n   eval_runtime: 0.1933\n   eval_samples_per_second: 5.1750\n   eval_steps_per_second: 5.1750\n   epoch: 5.0000\n\u2705 Full fine-tuning completed!\n\ud83d\udcc1 Model saved to: ./test_full_basic\n</code></pre>"},{"location":"fine-tuning/examples/#lora-fine-tuning-example","title":"LoRA Fine-tuning Example","text":""},{"location":"fine-tuning/examples/#command_1","title":"Command","text":"<pre><code>ideaweaver finetune lora \\\n  --model microsoft/DialoGPT-small \\\n  --dataset datasets/instruction_following_sample.json \\\n  --output-dir ./test_lora \\\n  --epochs 3 \\\n  --batch-size 4 \\\n  --learning-rate 1e-4 \\\n  --lora-rank 8 \\\n  --lora-alpha 16 \\\n  --verbose\n</code></pre>"},{"location":"fine-tuning/examples/#output_1","title":"Output","text":"<pre><code>\ud83d\ude80 LoRA Fine-Tuner initialized\n   Model: microsoft/DialoGPT-small\n   Method: lora\n   Task: instruction_following\n\ud83d\udd27 Setting up model and tokenizer...\n\u2705 Model and tokenizer setup complete\n   Model parameters: 124,439,808\n   LoRA parameters: 1,769,472\n\ud83d\udcca Preparing dataset from: datasets/instruction_following_sample.json\n\u2705 Dataset prepared\n   Training samples: 5\n   Evaluation samples: 1\n\ud83d\udd27 Setting up trainer...\n\u2705 Trainer setup complete\n\ud83d\ude80 Starting fine-tuning...\n{'loss': 2.3456, 'grad_norm': 0.9876, 'learning_rate': 1e-4, 'epoch': 3.0}\n{'train_runtime': 8.2345, 'train_samples_per_second': 2.345, 'train_steps_per_second': 0.987, 'train_loss': 2.3456789012345678, 'epoch': 3.0}\n\u2705 Fine-tuning completed!\n\ud83d\udcc1 Model saved to: ./test_lora\n\ud83d\udcca Evaluating model...\n{'eval_loss': 2.1234, 'eval_runtime': 0.1234, 'eval_samples_per_second': 8.123, 'eval_steps_per_second': 8.123, 'epoch': 3.0}\n\u2705 Evaluation complete\n   eval_loss: 2.1234\n   eval_runtime: 0.1234\n   eval_samples_per_second: 8.1230\n   eval_steps_per_second: 8.1230\n   epoch: 3.0000\n\u2705 LoRA fine-tuning completed!\n\ud83d\udcc1 Model saved to: ./test_lora\n</code></pre>"},{"location":"fine-tuning/examples/#qlora-fine-tuning-example","title":"QLoRA Fine-tuning Example","text":""},{"location":"fine-tuning/examples/#command_2","title":"Command","text":"<pre><code>ideaweaver finetune qlora \\\n  --model microsoft/DialoGPT-small \\\n  --dataset datasets/instruction_following_sample.json \\\n  --output-dir ./test_qlora \\\n  --epochs 3 \\\n  --batch-size 4 \\\n  --learning-rate 1e-4 \\\n  --lora-rank 8 \\\n  --lora-alpha 16 \\\n  --bits 4 \\\n  --verbose\n</code></pre>"},{"location":"fine-tuning/examples/#output_2","title":"Output","text":"<pre><code>\ud83d\ude80 QLoRA Fine-Tuner initialized\n   Model: microsoft/DialoGPT-small\n   Method: qlora\n   Task: instruction_following\n\ud83d\udd27 Setting up model and tokenizer...\n\u2705 Model and tokenizer setup complete\n   Model parameters: 124,439,808\n   LoRA parameters: 1,769,472\n   Quantization: 4-bit\n\ud83d\udcca Preparing dataset from: datasets/instruction_following_sample.json\n\u2705 Dataset prepared\n   Training samples: 5\n   Evaluation samples: 1\n\ud83d\udd27 Setting up trainer...\n\u2705 Trainer setup complete\n\ud83d\ude80 Starting fine-tuning...\n{'loss': 2.5678, 'grad_norm': 0.8765, 'learning_rate': 1e-4, 'epoch': 3.0}\n{'train_runtime': 7.1234, 'train_samples_per_second': 2.567, 'train_steps_per_second': 1.234, 'train_loss': 2.567890123456789, 'epoch': 3.0}\n\u2705 Fine-tuning completed!\n\ud83d\udcc1 Model saved to: ./test_qlora\n\ud83d\udcca Evaluating model...\n{'eval_loss': 2.3456, 'eval_runtime': 0.1123, 'eval_samples_per_second': 8.901, 'eval_steps_per_second': 8.901, 'epoch': 3.0}\n\u2705 Evaluation complete\n   eval_loss: 2.3456\n   eval_runtime: 0.1123\n   eval_samples_per_second: 8.9010\n   eval_steps_per_second: 8.9010\n   epoch: 3.0000\n\u2705 QLoRA fine-tuning completed!\n\ud83d\udcc1 Model saved to: ./test_qlora\n</code></pre>"},{"location":"fine-tuning/examples/#the-actual-output-values-may-vary-depending-on","title":"The actual output values may vary depending on:","text":"<ul> <li>Hardware configuration</li> <li>Dataset size and content</li> <li>Random initialization</li> <li>Training parameters</li> </ul>"},{"location":"fine-tuning/examples/#for-best-results","title":"For best results:","text":"<ul> <li>Start with smaller models for testing</li> <li>Use appropriate batch sizes for your GPU</li> <li>Monitor memory usage during training</li> <li>Adjust learning rate based on loss curves</li> </ul>"},{"location":"fine-tuning/examples/#common-issues-and-solutions","title":"Common issues and solutions:","text":"<ul> <li>OOM errors: Reduce batch size or enable gradient checkpointing</li> <li>Slow training: Enable mixed precision or use LoRA/QLoRA</li> <li>Poor convergence: Adjust learning rate or increase epochs </li> </ul>"},{"location":"fine-tuning/fine-tuning-methods/","title":"Fine-tuning Methods","text":""},{"location":"fine-tuning/fine-tuning-methods/#fine-tuning-methods-overview","title":"Fine-tuning Methods Overview","text":"<p>Ideaweaver supports four main fine-tuning approaches, each designed for different use cases and resource constraints:</p>"},{"location":"fine-tuning/fine-tuning-methods/#1-full-fine-tuning","title":"1. Full Fine-tuning","text":"<ul> <li>Complete Model Update: Updates all model parameters</li> <li>Higher Memory Usage: Requires more GPU memory</li> <li>Best Performance: Achieves optimal results but at higher resource cost</li> </ul>"},{"location":"fine-tuning/fine-tuning-methods/#2-lora-fine-tuning","title":"2. LoRA Fine-tuning","text":"<ul> <li>Low-Rank Adaptation: Uses efficient parameter updates</li> <li>Memory Efficient: Requires significantly less memory</li> <li>Faster Training: Quicker training with good performance</li> </ul>"},{"location":"fine-tuning/fine-tuning-methods/#3-qlora-fine-tuning","title":"3. QLoRA Fine-tuning","text":"<ul> <li>Quantized LoRA: Combines quantization with LoRA</li> <li>Ultra Memory Efficient: Minimal memory requirements</li> <li>4-bit Quantization: Uses 4-bit precision for maximum efficiency</li> </ul>"},{"location":"fine-tuning/fine-tuning-methods/#4-train-quantize","title":"4. Train &amp; Quantize","text":"<ul> <li>Training + Quantization: Combines both processes</li> <li>Multiple Quant Methods: Supports int8 and gguf quantization</li> <li>Optimized Deployment: Ready for production deployment</li> </ul>"},{"location":"fine-tuning/fine-tuning-methods/#color-legend","title":"Color Legend","text":"<ul> <li>\ud83d\udfe2 Dark Green: Main Ideaweaver node</li> <li>\ud83d\udd35 Blue: Fine-tuning methods</li> <li>\ud83d\udfe0 Orange: Features and characteristics</li> </ul>"},{"location":"fine-tuning/fine-tuning-methods/#when-to-use-each-method","title":"When to Use Each Method","text":"<ol> <li>Full Fine-tuning: When you have sufficient GPU memory and need the best possible performance</li> <li>LoRA: When you want a good balance between performance and resource usage</li> <li>QLoRA: When working with limited GPU memory or large models</li> <li>Train &amp; Quantize: When you need an optimized model for deployment </li> </ol>"},{"location":"fine-tuning/overview/","title":"Overview","text":""},{"location":"fine-tuning/overview/#fine-tuning-overview","title":"Fine-tuning Overview","text":"<p>IdeaWeaver provides comprehensive support for model fine-tuning with multiple approaches:</p>"},{"location":"fine-tuning/overview/#available-methods","title":"Available Methods","text":"<p>Full Fine-tuning</p> <ul> <li>Complete model parameter updates</li> <li>Maximum customization potential</li> <li>Higher resource requirements</li> </ul> <p>LoRA (Low-Rank Adaptation)</p> <ul> <li>Parameter-efficient fine-tuning</li> <li>Reduced memory footprint</li> <li>Faster training times</li> </ul> <p>QLoRA (Quantized LoRA)</p> <ul> <li>Memory-efficient fine-tuning</li> <li>4-bit quantization support</li> <li>Ideal for large models</li> </ul>"},{"location":"fine-tuning/overview/#key-features","title":"Key Features","text":"<ul> <li>Gradient Checkpointing: Memory optimization for large models</li> <li>Gradient Accumulation: Effective batch size control</li> <li>Mixed Precision Training: FP16/BF16 support</li> <li>Multi-GPU Support: Distributed training capabilities</li> <li>Experiment Tracking: Integration with MLflow &amp; W&amp;B</li> </ul>"},{"location":"fine-tuning/overview/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12+</li> <li>CUDA-compatible GPU (for full fine-tuning)</li> <li>Sufficient disk space for model checkpoints</li> <li>Dataset in supported format (JSON, CSV, etc.)</li> </ul>"},{"location":"fine-tuning/overview/#getting-started","title":"Getting Started","text":"<p>See the Commands page for detailed usage instructions and Examples for practical use cases. </p>"},{"location":"getting-started/configuration/","title":"Configuration","text":""},{"location":"getting-started/configuration/#environment-setup","title":"Environment Setup","text":"<p>Set up your API keys and configuration for IdeaWeaver:</p> <pre><code># OpenAI API (for GPT models)\nexport OPENAI_API_KEY='your-openai-api-key'\n\n# Hugging Face Hub (for model downloads)\nexport HUGGINGFACE_HUB_TOKEN='your-huggingface-token'\n\n# Weights &amp; Biases (for experiment tracking)\nexport WANDB_API_KEY='your-wandb-api-key'\n\n# Comet ML (for experiment tracking)\nexport COMET_API_KEY='your-comet-api-key'\n\n# MLflow (for experiment tracking)\nexport MLFLOW_TRACKING_URI='your-mlflow-uri'\n\n# DagsHub (for experiment tracking)\nexport DAGSHUB_TOKEN='your-dagshub-token'\n\n# AWS Credentials (for cloud deployment)\nexport AWS_ACCESS_KEY_ID='your-aws-access-key'\nexport AWS_SECRET_ACCESS_KEY='your-aws-secret-key'\nexport AWS_DEFAULT_REGION='us-east-1'\n\n# Qdrant (for vector store)\nexport QDRANT_URL='your-qdrant-url'\nexport QDRANT_API_KEY='your-qdrant-key'\n\n# GitHub (for MCP integration)\nexport GITHUB_TOKEN='your-github-token'\n</code></pre>"},{"location":"getting-started/configuration/#configuration-files","title":"Configuration Files","text":""},{"location":"getting-started/configuration/#training-configuration","title":"Training Configuration","text":"<pre><code># config/training.yaml\nmodel:\n  name: \"microsoft/DialoGPT-medium\"\n  task: \"text-generation\"\n\ndataset:\n  name: \"your-dataset\"\n  split: \"train\"\n\ntraining:\n  output_dir: \"./results\"\n  num_train_epochs: 3\n  per_device_train_batch_size: 4\n  learning_rate: 5e-5\n  save_steps: 500\n  logging_steps: 100\n\nlora:\n  rank: 16\n  alpha: 32\n  dropout: 0.1\n</code></pre>"},{"location":"getting-started/configuration/#rag-configuration","title":"RAG Configuration","text":"<pre><code># config/rag.yaml\nvector_store:\n  type: \"chroma\"\n  persist_directory: \"./chroma_db\"\n\nembeddings:\n  model: \"sentence-transformers/all-MiniLM-L6-v2\"\n\nllm:\n  provider: \"openai\"\n  model: \"gpt-3.5-turbo\"\n  temperature: 0.7\n\nretrieval:\n  top_k: 5\n  similarity_threshold: 0.7\n</code></pre>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide</li> <li>CLI Commands Reference </li> </ul>"},{"location":"getting-started/installation/","title":"Installation Guide","text":""},{"location":"getting-started/installation/#quick-setup","title":"Quick Setup","text":"<p>IdeaWeaver provides an automated setup script that handles Python 3.12 installation and virtual environment creation:</p> <pre><code># Installation\ngit clone https://github.com/ideaweaver-ai-code/ideaweaver.git\ncd ideaweaver\nchmod +x setup_environments.sh\n./setup_environments.sh\n</code></pre> <p>The setup will:</p> <ul> <li>\u2705 Detect or install Python 3.12 automatically</li> <li>\u2705 Create <code>ideaweaver-env</code> virtual environment  </li> <li>\u2705 Install all dependencies from consolidated <code>requirements.txt</code></li> <li>\u2705 Set up the IdeaWeaver CLI in development mode</li> </ul>"},{"location":"getting-started/installation/#activate-environment","title":"Activate Environment","text":"<pre><code>source ideaweaver-env/bin/activate\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>ideaweaver --help\n</code></pre> <p>Expected Output: <pre><code>Usage: ideaweaver [OPTIONS] COMMAND [ARGS]...\n\n  IdeaWeaver Model Training CLI - A comprehensive tool for AI model training,\n  evaluation, and deployment.\n\n  Features include LoRA/QLoRA fine-tuning, RAG systems, MCP integration, and\n  enterprise-grade model management. For detailed documentation and examples,\n  visit: https://github.com/ideaweaver-ai-code/ideaweaver\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  agent       Intelligent agent workflows for creative and analytical tasks.\n  download    Download a model from Hugging Face Hub\n  evaluate    Evaluate a model using lm-evaluation-harness with...\n  finetune    Supervised fine-tuning commands with LoRA, QLoRA, and full...\n  list-tasks  List all available evaluation tasks from lm-evaluation-harness\n  mcp         Model Context Protocol (MCP) integration commands\n  rag         RAG (Retrieval-Augmented Generation) commands\n  train       Train a model with AutoTrain Advanced.\n  validate    Validate a configuration file\n</code></pre></p>"},{"location":"getting-started/installation/#manual-installation","title":"Manual Installation","text":"<p>If you prefer manual installation or the automated script doesn't work for your system:</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12 (Required - not 3.11 or 3.13)</li> <li>Git for cloning the repository</li> <li>pip for package management</li> </ul>"},{"location":"getting-started/installation/#step-by-step-installation","title":"Step-by-Step Installation","text":"<ol> <li> <p>Clone the Repository <pre><code>git clone https://github.com/ideaweaver-ai-code/ideaweaver.git\ncd ideaweaver\n</code></pre></p> </li> <li> <p>Create Virtual Environment <pre><code>python3.12 -m venv ideaweaver-env\nsource ideaweaver-env/bin/activate  \n</code></pre></p> </li> <li> <p>Install Dependencies <pre><code>pip install --upgrade pip\npip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Install IdeaWeaver CLI <pre><code>cd backend\npip install -e .\ncd ..\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#environment-variables","title":"Environment Variables","text":"<p>Set up your API keys for full functionality:</p> <pre><code># OpenAI API (for GPT models)\nexport OPENAI_API_KEY='your-openai-api-key'\n\n# Hugging Face Hub (for model downloads)\nexport HUGGINGFACE_HUB_TOKEN='your-huggingface-token'\n\n# Weights &amp; Biases (for experiment tracking)\nexport WANDB_API_KEY='your-wandb-api-key'\n\n# Comet ML (for experiment tracking)\nexport COMET_API_KEY='your-comet-api-key'\n\n# MLflow (for experiment tracking)\nexport MLFLOW_TRACKING_URI='your-mlflow-uri'\n\n# DagsHub (for experiment tracking)\nexport DAGSHUB_TOKEN='your-dagshub-token'\n\n# AWS Credentials (for cloud deployment)\nexport AWS_ACCESS_KEY_ID='your-aws-access-key'\nexport AWS_SECRET_ACCESS_KEY='your-aws-secret-key'\nexport AWS_DEFAULT_REGION='us-east-1'\n\n# Qdrant (for vector store)\nexport QDRANT_URL='your-qdrant-url'\nexport QDRANT_API_KEY='your-qdrant-key'\n\n# GitHub (for MCP integration)\nexport GITHUB_TOKEN='your-github-token'\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#python-version-issues","title":"Python Version Issues","text":"<p>The setup script requires Python 3.12 exactly. If you encounter version-related errors:</p> <pre><code># Check your Python version\npython3.12 --version  # Should show 3.12.x\n\n# On macOS with Homebrew\nbrew install python@3.12\n\n# On Ubuntu/Debian\nsudo apt-get install python3.12 python3.12-venv python3.12-pip\n\n# On CentOS/RHEL\nsudo dnf install python3.12\n</code></pre>"},{"location":"getting-started/installation/#common-installation-issues","title":"Common Installation Issues","text":"<p>Issue: <code>auto-gptq</code> installation fails - This is normal on macOS (requires CUDA) - The core functionality works without it</p> <p>Issue: <code>ideaweaver</code> command not found - Make sure you activated the virtual environment - Verify the backend installation completed successfully</p> <p>Issue: Import errors for specific packages - Check if all requirements installed: <code>pip list</code> - Reinstall requirements: <code>pip install -r requirements.txt --force-reinstall</code></p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>After successful installation:</p> <ol> <li>Follow the Quick Start Guide</li> <li>Configure your first RAG system</li> <li>Train your first model</li> <li>Set up MCP integrations</li> </ol>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":""},{"location":"getting-started/installation/#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>CPU: 4+ cores recommended</li> <li>RAM: 8GB minimum, 16GB+ recommended</li> <li>Storage: 10GB free space minimum</li> <li>OS: macOS 10.15+, Ubuntu 18.04+</li> </ul>"},{"location":"getting-started/installation/#recommended-for-training","title":"Recommended for Training","text":"<ul> <li>GPU: NVIDIA GPU with 8GB+ VRAM</li> <li>RAM: 32GB+ for large model training  </li> <li>Storage: 50GB+ for model storage </li> </ul>"},{"location":"getting-started/quick-start/","title":"Quick Start Guide","text":"<p>Get up and running with IdeaWeaver in minutes! This guide walks you through the essential features.</p>"},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have completed the Installation Guide and can run:</p> <pre><code>source ideaweaver-env/bin/activate\nideaweaver --help\n</code></pre>"},{"location":"getting-started/quick-start/#available-commands","title":"Available Commands","text":"<p>IdeaWeaver provides comprehensive CLI commands for AI model operations:</p> <pre><code>Commands:\n  agent       Intelligent agent workflows for creative and analytical tasks.\n  download    Download a model from Hugging Face Hub\n  evaluate    Evaluate a model using lm-evaluation-harness with...\n  finetune    Supervised fine-tuning commands with LoRA, QLoRA, and full...\n  list-tasks  List all available evaluation tasks from lm-evaluation-harness\n  mcp         Model Context Protocol (MCP) integration commands\n  rag         RAG (Retrieval-Augmented Generation) commands\n  train       Train a model with AutoTrain Advanced.\n  validate    Validate a configuration file\n</code></pre>"},{"location":"getting-started/quick-start/#download-your-first-model","title":"Download Your First Model","text":"<p>Start by downloading a model from Hugging Face:</p> <pre><code># Download a small model for quick testing\nideaweaver download microsoft/DialoGPT-medium\n\n# Use the `--save-path` option to specify the directory where the downloaded model will be stored.\nideaweaver download microsoft/DialoGPT-medium --save-path ./models/my-model\n</code></pre>"},{"location":"getting-started/quick-start/#basic-model-training","title":"Basic Model Training","text":""},{"location":"getting-started/quick-start/#1-prepare-your-dataset","title":"1. Prepare Your Dataset","text":"<p>Create a simple training configuration:</p> <pre><code># config/training_config.yaml\nmodel:\n  name: \"microsoft/DialoGPT-medium\"\n  task: \"text-generation\"\n\ndataset:\n  name: \"your-dataset-name\"\n  split: \"train\"\n\ntraining:\n  output_dir: \"./results\"\n  num_train_epochs: 3\n  per_device_train_batch_size: 4\n  learning_rate: 5e-5\n  save_steps: 500\n  logging_steps: 100\n</code></pre>"},{"location":"getting-started/quick-start/#2-start-training","title":"2. Start Training","text":"<pre><code># Basic training\nideaweaver train --config config/training_config.yaml\n</code></pre>"},{"location":"getting-started/quick-start/#rag-retrieval-augmented-generation","title":"RAG (Retrieval-Augmented Generation)","text":""},{"location":"getting-started/quick-start/#1-set-up-your-first-rag-system","title":"1. Set Up Your First RAG System","text":"<pre><code># 1. Create a knowledge base\nideaweaver rag create-kb --name mykb --embedding-model sentence-transformers/all-MiniLM-L6-v2\n\n# 2. Ingest documents into the knowledge base\nideaweaver rag ingest --kb mykb --source ./documents/\n\n# 3. Query the knowledge base\nideaweaver rag query --kb mykb --question \"What is machine learning?\"\n</code></pre>"},{"location":"getting-started/quick-start/#mcp-model-context-protocol-integration","title":"\ud83d\udd0c MCP (Model Context Protocol) Integration","text":""},{"location":"getting-started/quick-start/#1-list-available-mcp-servers","title":"1. List Available MCP Servers","text":"<pre><code># See all available MCP integrations\nideaweaver mcp list-servers\n</code></pre>"},{"location":"getting-started/quick-start/#2-set-up-github-integration","title":"2. Set Up GitHub Integration","text":"<pre><code># 1. Set up GitHub authentication (will prompt for your token)\nideaweaver mcp setup-auth github\n\n# 2. Enable the GitHub MCP server\nideaweaver mcp enable github\n\n# 3. List available MCP servers (to verify)\nideaweaver mcp list-servers\n\n# 4. Call a tool on the GitHub MCP server (example: list issues)\nideaweaver mcp call-tool github list_issues --args '{\"owner\": \"your-username/org name\", \"repo\": \"your-repo\"}'\n</code></pre>"},{"location":"getting-started/quick-start/#model-evaluation","title":"Model Evaluation","text":""},{"location":"getting-started/quick-start/#1-evaluate-with-standard-benchmarks","title":"1. Evaluate with Standard Benchmarks","text":"<pre><code># Run evaluation on specific tasks\nideaweaver evaluate ./downloaded_model --tasks hellaswag,arc_easy,winogrande --output-path results.json\n</code></pre>"},{"location":"getting-started/quick-start/#agents","title":"Agents","text":""},{"location":"getting-started/quick-start/#agents-for-multi-agent-workflows","title":"Agents for Multi-Agent Workflows","text":"<pre><code># Generate storybooks\nideaweaver agent generate_storybook --theme \"brave little mouse\" --target-age \"3-5\"\n</code></pre>"},{"location":"getting-started/quick-start/#model-fine-tuning","title":"Model Fine-tuning","text":"<pre><code>ideaweaver finetune full \\\n  --model microsoft/DialoGPT-small \\\n  --dataset datasets/instruction_following_sample.json \\\n  --output-dir ./test_full_basic \\\n  --epochs 5 \\\n  --batch-size 2 \\\n  --gradient-accumulation-steps 2 \\\n  --learning-rate 5e-5 \\\n  --max-seq-length 256 \\\n  --gradient-checkpointing \\\n  --verbose\n</code></pre>"},{"location":"getting-started/quick-start/#configuration-validation","title":"Configuration Validation","text":"<p>Always validate your configurations before running:</p> <pre><code># Validate training configuration\nideaweaver validate config/training_config.yaml\n</code></pre>"},{"location":"getting-started/quick-start/#pro-tips","title":"Pro Tips","text":"<ol> <li>Environment Variables: Set API keys in your shell profile for persistence</li> <li>Configuration Files: Use YAML configs for complex setups - easier to reproduce</li> <li>Logging: Add <code>--verbose</code> flag to most commands for detailed output</li> <li>GPU Usage: Commands automatically detect and use available GPUs</li> <li>Caching: Models and datasets are cached locally to speed up repeated runs</li> </ol>"},{"location":"getting-started/quick-start/#common-issues","title":"\ud83d\udea8 Common Issues","text":"<p>Command not found: Make sure your virtual environment is activated <pre><code>source ideaweaver-env/bin/activate\n</code></pre></p> <p>Import errors: Reinstall requirements if packages are missing <pre><code>pip install -r requirements.txt --force-reinstall\n</code></pre></p> <p>GPU not detected: Check CUDA installation <pre><code>python -c \"import torch; print(torch.cuda.is_available())\"\n</code></pre></p> <p>\u26a0\ufe0f\u00a0Note:\u00a0IdeaWeaver is currently in alpha. Expect\u00a0a few bugs, and please report\u00a0any\u00a0issues you find. If you like the project, drop a \u2b50 on GitHub!</p>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you've got the basics down:</p> <ol> <li>\ud83d\udcd6 Explore detailed Tutorials</li> <li>\ud83d\udd27 Check the CLI Reference</li> <li>\ud83e\udd1d Join our Community</li> <li>\ud83d\ude80 Deploy to Production</li> </ol> <p>Ready to build something amazing? Let's go! \ud83d\ude80 </p>"},{"location":"mcp/aws/","title":"AWS CloudFormation MCP Integration","text":""},{"location":"mcp/aws/#overview","title":"Overview","text":"<p>IdeaWeaver provides seamless integration with AWS CloudFormation through the Model Context Protocol (MCP), allowing you to manage AWS resources programmatically.</p>"},{"location":"mcp/aws/#setup","title":"Setup","text":"<ol> <li> <p>Set up AWS authentication: <pre><code>ideaweaver mcp setup-auth awslabs.cfn-mcp-server\n</code></pre></p> </li> <li> <p>Enable AWS CloudFormation server: <pre><code>ideaweaver mcp enable awslabs.cfn-mcp-server\n</code></pre></p> </li> </ol>"},{"location":"mcp/aws/#common-operations","title":"Common Operations","text":""},{"location":"mcp/aws/#list-resources","title":"List Resources","text":""},{"location":"mcp/aws/#list-s3-buckets","title":"List S3 Buckets","text":"<pre><code>ideaweaver mcp call-tool awslabs.cfn-mcp-server list_resources \\\n  --args '{\"resource_type\": \"AWS::S3::Bucket\"}'\n</code></pre>"},{"location":"mcp/aws/#list-ec2-instances","title":"List EC2 Instances","text":"<pre><code>ideaweaver mcp call-tool awslabs.cfn-mcp-server list_resources \\\n  --args '{\"resource_type\": \"AWS::EC2::Instance\"}'\n</code></pre>"},{"location":"mcp/aws/#create-resources","title":"Create Resources","text":""},{"location":"mcp/aws/#create-an-s3-bucket","title":"Create an S3 Bucket","text":"<pre><code>ideaweaver mcp call-tool awslabs.cfn-mcp-server create_resource \\\n  --args '{\"resource_type\": \"AWS::S3::Bucket\", \"desired_state\": {\"BucketName\": \"my-test-bucket-12345\"}}'\n</code></pre>"},{"location":"mcp/aws/#configuration","title":"Configuration","text":""},{"location":"mcp/aws/#required-environment-variables","title":"Required Environment Variables","text":"<pre><code># AWS Credentials\nexport AWS_ACCESS_KEY_ID=AKIAXXXXXXXXXXXX\nexport AWS_SECRET_ACCESS_KEY=xxxxxxxxxxxx\nexport AWS_DEFAULT_REGION=us-east-1\n</code></pre>"},{"location":"mcp/aws/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mcp/aws/#common-issues","title":"Common Issues","text":"<ol> <li>Authentication Failures</li> <li>Verify AWS credentials are correct</li> <li>Check environment variables are set</li> <li> <p>Ensure IAM user has necessary permissions</p> </li> <li> <p>Resource Creation Failures</p> </li> <li>Check resource naming conflicts</li> <li>Verify resource limits in your AWS account</li> <li>Ensure IAM permissions include resource creation rights</li> </ol>"},{"location":"mcp/aws/#debug-mode","title":"Debug Mode","text":"<p>Enable verbose output for debugging:</p> <pre><code>ideaweaver mcp call-tool awslabs.cfn-mcp-server list_resources \\\n  --args '{\"resource_type\": \"AWS::S3::Bucket\"}' --verbose\n</code></pre>"},{"location":"mcp/aws/#best-practices","title":"Best Practices","text":"<ol> <li>Security</li> <li>Use IAM roles with least privilege</li> <li>Rotate AWS credentials regularly</li> <li> <p>Never commit AWS credentials to version control</p> </li> <li> <p>Resource Management</p> </li> <li>Use meaningful resource names</li> <li>Implement proper tagging strategy</li> <li> <p>Monitor resource usage and costs</p> </li> <li> <p>Error Handling</p> </li> <li>Implement proper error checking</li> <li>Use verbose mode for debugging</li> <li>Monitor AWS CloudWatch logs </li> </ol>"},{"location":"mcp/github/","title":"GitHub MCP Integration","text":""},{"location":"mcp/github/#overview","title":"Overview","text":"<p>The GitHub MCP integration allows you to interact with GitHub repositories, issues, pull requests, and more through IdeaWeaver's Model Context Protocol.</p>"},{"location":"mcp/github/#setup","title":"Setup","text":"<p>Get a GitHub Personal Access Token</p> <ul> <li>Go to: https://github.com/settings/tokens</li> <li>Create a token with repo, issues, and PR permissions</li> </ul> <p>Configure Authentication</p> <pre><code># Set up authentication\nideaweaver mcp setup-auth github\n\n# Or set environment variable\nexport GITHUB_PERSONAL_ACCESS_TOKEN=your_token_here\n</code></pre> <ol> <li> <p>Enable the Server <pre><code>ideaweaver mcp enable github\n</code></pre></p> </li> <li> <p>Test the Connection <pre><code>ideaweaver mcp test-connection github\n</code></pre></p> </li> </ol>"},{"location":"mcp/github/#available-operations","title":"Available Operations","text":""},{"location":"mcp/github/#repository-search","title":"Repository Search","text":"<pre><code>ideaweaver mcp call-tool github search_repositories \\\n  --args '{\"query\": \"100daysofdevops\", \"perPage\": 3}'\n</code></pre>"},{"location":"mcp/github/#file-contents-reading","title":"File Contents Reading","text":"<pre><code>ideaweaver mcp call-tool github get_file_contents \\\n  --args '{\"owner\": \"100daysofdevops\", \"repo\": \"100daysofdevops\", \"path\": \"README.md\"}'\n</code></pre>"},{"location":"mcp/github/#issues-listing","title":"Issues Listing","text":"<pre><code>ideaweaver mcp call-tool github list_issues \\\n  --args '{\"owner\": \"100daysofdevops\", \"repo\": \"100daysofdevops\"}'\n</code></pre>"},{"location":"mcp/github/#repository-creation","title":"Repository Creation","text":"<pre><code>ideaweaver mcp call-tool github create_repository \\\n  --args '{\"name\": \"mcp-integration-test\", \"description\": \"Test repository\", \"private\": false}'\n</code></pre>"},{"location":"mcp/github/#issue-creation","title":"Issue Creation","text":"<pre><code>ideaweaver mcp call-tool github create_issue \\\n  --args '{\"owner\": \"100daysofdevops\", \"repo\": \"100daysofdevops\", \"title\": \"Test Issue\", \"body\": \"Test issue description\"}'\n</code></pre>"},{"location":"mcp/github/#comment-creation","title":"Comment Creation","text":"<pre><code>ideaweaver mcp call-tool github add_issue_comment \\\n  --args '{\"owner\": \"100daysofdevops\", \"repo\": \"100daysofdevops\", \"issue_number\": 16, \"body\": \"Test comment\"}'\n</code></pre>"},{"location":"mcp/github/#configuration","title":"Configuration","text":"<p>The GitHub MCP server configuration is stored in <code>~/.ideaweaver/mcp/config.json</code>:</p> <pre><code>{\n  \"enabled_servers\": [\n    \"awslabs.cfn-mcp-server\",\n    \"github\"\n  ],\n  \"custom_servers\": {},\n  \"default_settings\": {\n    \"auto_connect\": true,\n    \"timeout\": 30\n  },\n  \"server_configs\": {\n    \"github\": {\n      \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"XXXXXXXXX\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"mcp/github/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mcp/github/#common-issues","title":"Common Issues","text":"<p>Authentication Failures</p> <ul> <li>Verify your GitHub token has the correct permissions</li> <li>Check if the token is properly set in environment variables</li> <li>Try regenerating the token if issues persist</li> </ul> <p>Rate Limiting</p> <ul> <li>GitHub API has rate limits for authenticated and unauthenticated requests</li> <li>Consider using a higher rate limit token for production use</li> </ul> <p>Connection Issues</p> <ul> <li>Check your internet connection</li> <li>Verify GitHub API status at https://www.githubstatus.com/</li> <li>Try using debug mode for more information:      <pre><code>ideaweaver mcp call-tool github search_repositories \\\n  --args '{\"query\": \"test\"}' --verbose\n</code></pre></li> </ul>"},{"location":"mcp/github/#best-practices","title":"Best Practices","text":"<p>Token Security</p> <ul> <li>Never commit tokens to version control</li> <li>Use environment variables or secure credential storage</li> <li>Rotate tokens regularly</li> </ul> <p>Rate Limiting</p> <ul> <li>Monitor your API usage</li> <li>Implement appropriate caching for frequently accessed data</li> <li>Use pagination for large result sets</li> </ul> <p>Error Handling</p> <ul> <li>Implement proper error handling in your workflows</li> <li>Use the verbose flag for debugging</li> <li>Check GitHub API status before critical operations </li> </ul>"},{"location":"mcp/terraform/","title":"Terraform MCP Integration","text":""},{"location":"mcp/terraform/#overview","title":"Overview","text":"<p>IdeaWeaver provides integration with Terraform through the Model Context Protocol (MCP), allowing you to search and manage Terraform modules and providers.</p>"},{"location":"mcp/terraform/#setup","title":"Setup","text":"<p>Enable Terraform server: <pre><code>ideaweaver mcp enable terraform\n</code></pre></p>"},{"location":"mcp/terraform/#module-operations","title":"Module Operations","text":""},{"location":"mcp/terraform/#search-for-terraform-modules","title":"Search for Terraform Modules","text":"<pre><code>ideaweaver mcp call-tool terraform searchModules \\\n  --args '{\"moduleQuery\": \"aws-vpc\"}'\n</code></pre> <p>Example output: <pre><code>\ud83d\udce6 **Terraform Module Search Results**\n\nAvailable Terraform Modules (top matches) for aws-vpc\n\nEach result includes:\n  - moduleID: The module ID (format: namespace/name/provider-name/module-version)\n  - Name: The name of the module\n  - Description: A short description of the module\n  - Downloads: The total number of times the module has been downloaded\n  - Verified: Verification status of the module\n  - Published: The date and time when the module was published\n\n---\n\n  - moduleID: Harshrai3112/aws-vpc/aws/1.0.0\n  - Name: aws-vpc\n  - Description:\n  - Downloads: 5016\n  - Verified: false\n  - Published: 2021-06-04 08:59:52.979927 +0000 UTC\n</code></pre></p>"},{"location":"mcp/terraform/#get-module-details","title":"Get Module Details","text":"<pre><code>ideaweaver mcp call-tool terraform getModuleDetails \\\n  --args '{\"moduleName\": \"terraform-aws-modules/vpc/aws\"}'\n</code></pre>"},{"location":"mcp/terraform/#best-practices","title":"Best Practices","text":"<ol> <li>Module Selection</li> <li>Check module verification status</li> <li>Review download statistics</li> <li>Read module documentation</li> <li> <p>Verify module compatibility</p> </li> <li> <p>Version Management</p> </li> <li>Use specific version numbers</li> <li>Test module updates in staging</li> <li> <p>Keep track of module dependencies</p> </li> <li> <p>Security</p> </li> <li>Use verified modules when possible</li> <li>Review module source code</li> <li>Check for known vulnerabilities</li> </ol>"},{"location":"mcp/terraform/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mcp/terraform/#common-issues","title":"Common Issues","text":"<ol> <li>Module Not Found</li> <li>Verify module name is correct</li> <li>Check module namespace</li> <li> <p>Ensure module is public</p> </li> <li> <p>Version Conflicts</p> </li> <li>Check provider version compatibility</li> <li>Verify module version exists</li> <li>Review version constraints</li> </ol>"},{"location":"mcp/terraform/#debug-mode","title":"Debug Mode","text":"<p>Enable verbose output for debugging:</p> <pre><code>ideaweaver mcp call-tool terraform searchModules \\\n  --args '{\"moduleQuery\": \"aws-vpc\"}' --verbose\n</code></pre>"},{"location":"mcp/terraform/#next-steps","title":"Next Steps","text":"<ol> <li>Explore Modules: Search for modules in the Terraform Registry</li> <li>Review Documentation: Read module documentation and examples</li> <li>Test Integration: Try different module operations</li> <li>Build Workflows: Combine with other MCP servers for complex operations </li> </ol>"},{"location":"rag/commands/","title":"RAG Commands","text":""},{"location":"rag/commands/#knowledge-base-management","title":"Knowledge Base Management","text":""},{"location":"rag/commands/#create-knowledge-base","title":"Create Knowledge Base","text":"<pre><code>ideaweaver rag create-kb --name my-kb --description \"My knowledge base\"\n</code></pre> <p>Options:</p> <ul> <li><code>--name</code>: Knowledge base name (required)</li> <li><code>--description</code>: Knowledge base description</li> <li><code>--embedding-model</code>: Embedding model to use</li> <li><code>--chunk-size</code>: Chunk size for text splitting</li> <li><code>--chunk-overlap</code>: Overlap between chunks</li> <li><code>--chunking-strategy</code>: Text chunking strategy (recursive|token|semantic)</li> <li><code>--vector-store</code>: Vector store backend (chroma|qdrant_local|qdrant_cloud)</li> <li><code>--qdrant-url</code>: Qdrant Cloud URL (required for qdrant_cloud)</li> <li><code>--qdrant-api-key</code>: Qdrant Cloud API key (optional)</li> <li><code>--qdrant-prefix</code>: Collection prefix for Qdrant (optional)</li> <li><code>--qdrant-timeout</code>: Connection timeout in seconds (optional)</li> </ul> <p>Note: For security reasons, never commit API keys or sensitive URLs to version control. Use environment variables or secure secret management systems.</p>"},{"location":"rag/commands/#list-knowledge-bases","title":"List Knowledge Bases","text":"<pre><code>ideaweaver rag list-kb\n</code></pre>"},{"location":"rag/commands/#delete-knowledge-base","title":"Delete Knowledge Base","text":"<pre><code>ideaweaver rag delete-kb --name my-kb\n</code></pre>"},{"location":"rag/commands/#document-management","title":"Document Management","text":""},{"location":"rag/commands/#ingest-documents","title":"Ingest Documents","text":"<pre><code>ideaweaver rag ingest --kb my-kb --source ./docs --file-types md,pdf\n</code></pre> <p>Options:</p> <ul> <li><code>--kb</code>: Knowledge base name (required)</li> <li><code>--source</code>: Source path (file or directory) (required)</li> <li><code>--file-types</code>: Comma-separated list of file types to include</li> <li><code>--verbose</code>: Verbose output</li> </ul>"},{"location":"rag/commands/#show-knowledge-base-statistics","title":"Show Knowledge Base Statistics","text":"<pre><code>ideaweaver rag stats --kb my-kb\n</code></pre>"},{"location":"rag/commands/#querying","title":"Querying","text":""},{"location":"rag/commands/#basic-query","title":"Basic Query","text":"<pre><code>ideaweaver rag query --kb my-kb -q \"What are the main features?\"\n</code></pre>"},{"location":"rag/commands/#agentic-query","title":"Agentic Query","text":"<pre><code># Set OpenAI API key as environment variable\nexport OPENAI_API_KEY=\"$OPENAI_API_KEY\"\n\n# Run agentic query\nideaweaver rag agentic-query --kb my-kb -q \"Explain the advanced features\"\n</code></pre>"},{"location":"rag/commands/#compare-rag-types","title":"Compare RAG Types","text":"<pre><code>ideaweaver rag compare-rag-types --kb my-kb -q \"What are the key features?\"\n</code></pre>"},{"location":"rag/commands/#evaluation","title":"Evaluation","text":""},{"location":"rag/commands/#evaluate-rag-system","title":"Evaluate RAG System","text":"<pre><code>ideaweaver rag evaluate --kb my-kb\n</code></pre>"},{"location":"rag/commands/#generate-test-questions","title":"Generate Test Questions","text":"<pre><code>ideaweaver rag generate-test-questions --kb my-kb\n</code></pre>"},{"location":"rag/commands/#compare-knowledge-bases","title":"Compare Knowledge Bases","text":"<pre><code>ideaweaver rag compare-kb --kb1 kb1 --kb2 kb2\n</code></pre>"},{"location":"rag/commands/#visualization","title":"Visualization","text":""},{"location":"rag/commands/#show-rag-workflow","title":"Show RAG Workflow","text":"<pre><code>ideaweaver rag show-workflow\n</code></pre>"},{"location":"rag/commands/#example-workflow","title":"Example Workflow","text":"<ol> <li>Create a new knowledge base with Qdrant Cloud: <pre><code># Set Qdrant Cloud credentials as environment variables\nexport QDRANT_URL=\"your-cloud-url\"\nexport QDRANT_API_KEY=\"your-api-key\"\n\n# Create knowledge base\nideaweaver rag create-kb --name cloud-kb \\\n    --vector-store qdrant_cloud \\\n    --qdrant-url \"$QDRANT_URL\" \\\n    --qdrant-api-key \"$QDRANT_API_KEY\" \\\n    --description \"Cloud knowledge base\"\n</code></pre></li> </ol> <p>Note: For security reasons, never commit API keys or sensitive URLs to version control. Use environment variables or secure secret management systems.</p> <ol> <li> <p>Ingest documents: <pre><code>ideaweaver rag ingest --kb cloud-kb --source ./docs --file-types md\n</code></pre></p> </li> <li> <p>Query the knowledge base: <pre><code>ideaweaver rag query --kb cloud-kb -q \"What are the main features?\"\n</code></pre></p> </li> <li> <p>Evaluate the system: <pre><code>ideaweaver rag evaluate --kb cloud-kb\n</code></pre></p> </li> <li> <p>Generate test questions: <pre><code>ideaweaver rag generate-test-questions --kb cloud-kb\n</code></pre></p> </li> <li> <p>Compare with another knowledge base: <pre><code>ideaweaver rag compare-kb --kb1 cloud-kb --kb2 another-kb\n</code></pre></p> </li> </ol>"},{"location":"rag/commands/#advanced-usage","title":"Advanced Usage","text":""},{"location":"rag/commands/#using-different-vector-stores","title":"Using Different Vector Stores","text":"<p>Chroma: <pre><code>ideaweaver rag create-kb --name chroma-kb --vector-store chroma\n</code></pre></p> <p>Qdrant Cloud: <pre><code># Set Qdrant Cloud credentials as environment variables\nexport QDRANT_URL=\"your-cloud-url\"\nexport QDRANT_API_KEY=\"your-api-key\"\n\nideaweaver rag create-kb --name qdrant-cloud-kb \\\n    --vector-store qdrant_cloud \\\n    --qdrant-url \"$QDRANT_URL\" \\\n    --qdrant-api-key \"$QDRANT_API_KEY\" \\\n    --qdrant-timeout 30\n</code></pre></p> <p>Note: For security reasons, never commit API keys or sensitive URLs to version control. Use environment variables or secure secret management systems. Example: <pre><code>export QDRANT_URL=\"your-cloud-url\"\nexport QDRANT_API_KEY=\"your-api-key\"\nexport OPENAI_API_KEY=\"your-openai-key\"\nexport HUGGINGFACE_HUB_TOKEN=\"your-hf-token\"\nexport GITHUB_TOKEN=\"your-github-token\"\n</code></pre></p>"},{"location":"rag/commands/#custom-chunking-strategy","title":"Custom Chunking Strategy","text":"<pre><code>ideaweaver rag create-kb --name custom-kb \\\n    --chunking-strategy semantic \\\n    --chunk-size 500 \\\n    --chunk-overlap 50\n</code></pre>"},{"location":"rag/commands/#using-different-embedding-models","title":"Using Different Embedding Models","text":"<pre><code>ideaweaver rag create-kb --name custom-emb-kb \\\n    --embedding-model sentence-transformers/all-mpnet-base-v2\n</code></pre>"},{"location":"rag/evaluation/","title":"RAG Evaluation","text":"<p>IdeaWeaver provides comprehensive evaluation capabilities for RAG systems using the RAGAS framework. This allows you to assess the quality and performance of your knowledge bases.</p>"},{"location":"rag/evaluation/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>The RAGAS framework evaluates several key aspects of your RAG system:</p> <ul> <li>Faithfulness: Measures if the generated answer is factually consistent with the retrieved context</li> <li>Answer Relevancy: Assesses if the answer is relevant to the question</li> <li>Context Relevancy: Evaluates if the retrieved context is relevant to the question</li> <li>Context Recall: Measures how well the system retrieves all relevant information</li> <li>Context Precision: Assesses the precision of the retrieved information</li> </ul>"},{"location":"rag/evaluation/#basic-evaluation","title":"Basic Evaluation","text":"<pre><code>ideaweaver rag evaluate --kb my-kb\n</code></pre> <p>This will:</p> <ol> <li>Generate test questions</li> <li>Run the evaluation</li> <li>Display metrics</li> <li>Generate a report</li> </ol>"},{"location":"rag/evaluation/#advanced-evaluation","title":"Advanced Evaluation","text":""},{"location":"rag/evaluation/#custom-test-questions","title":"Custom Test Questions","text":"<pre><code>ideaweaver rag evaluate --kb my-kb --questions-file custom_questions.json\n</code></pre>"},{"location":"rag/evaluation/#specific-metrics","title":"Specific Metrics","text":"<pre><code>ideaweaver rag evaluate --kb my-kb --metrics faithfulness,answer_relevancy\n</code></pre>"},{"location":"rag/evaluation/#compare-knowledge-bases","title":"Compare Knowledge Bases","text":"<pre><code>ideaweaver rag compare-kb --kb1 kb1 --kb2 kb2\n</code></pre>"},{"location":"rag/evaluation/#generating-test-questions","title":"Generating Test Questions","text":"<pre><code>ideaweaver rag generate-test-questions --kb my-kb\n</code></pre> <p>Options:</p> <ul> <li><code>--num-questions</code>: Number of questions to generate</li> <li><code>--output-file</code>: Save questions to a file</li> <li><code>--question-types</code>: Types of questions to generate</li> </ul>"},{"location":"rag/evaluation/#example-output","title":"Example Output","text":"<pre><code>\ud83e\udd16 Auto-generating 5 test questions\n\ud83e\uddea Starting RAGAS evaluation for KB: my-kb\n\ud83d\udcca Metrics: faithfulness, answer_relevancy\n\n\ud83d\udccb Evaluation Results:\n----------------------------------------\nFaithfulness: 0.85\nAnswer Relevancy: 0.92\nContext Relevancy: 0.88\nContext Recall: 0.90\nContext Precision: 0.87\n\n\u2705 Evaluation completed successfully!\n\ud83d\udcc4 Report saved to: evaluation_report_my-kb.md\n</code></pre>"},{"location":"rag/evaluation/#best-practices","title":"Best Practices","text":"<ol> <li>Regular Evaluation: Evaluate your RAG system regularly as you add new documents</li> <li>Multiple Metrics: Use multiple metrics to get a complete picture</li> <li>Custom Questions: Create domain-specific test questions</li> <li>Compare Baselines: Compare against baseline or previous versions</li> <li>Monitor Changes: Track how metrics change over time</li> </ol>"},{"location":"rag/evaluation/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and solutions:</p> <p>Low Faithfulness</p> <ul> <li>Check document quality</li> <li>Review chunking strategy</li> <li>Adjust retrieval parameters</li> </ul> <p>Low Relevancy</p> <ul> <li>Improve question understanding</li> <li>Optimize embedding model</li> <li>Review document preprocessing</li> </ul> <p>Low Recall</p> <ul> <li>Increase chunk overlap</li> <li>Adjust chunk size</li> <li>Review retrieval strategy</li> </ul>"},{"location":"rag/evaluation/#next-steps","title":"Next Steps","text":"<ul> <li>RAG Commands - Complete command reference</li> <li>RAG Overview - System architecture and features</li> <li>Enterprise RAG Guide - Production deployments </li> </ul>"},{"location":"rag/overview/","title":"RAG (Retrieval-Augmented Generation)","text":"<p>IdeaWeaver provides comprehensive RAG capabilities for building knowledge-based AI systems. The RAG system supports both traditional and agentic approaches, with multiple vector store options and advanced evaluation features.</p>"},{"location":"rag/overview/#key-features","title":"Key Features","text":"<ul> <li>Multiple Vector Stores: Support for Chroma, Qdrant (local and cloud)</li> <li>Advanced Embeddings: Sentence Transformers, OpenAI, and custom models</li> <li>Document Processing: PDF, DOCX, TXT, Markdown with smart chunking</li> <li>Evaluation: RAGAS framework for comprehensive assessment</li> <li>Agentic RAG: Multi-step reasoning with tool integration</li> </ul>"},{"location":"rag/overview/#qdrant-integration","title":"Qdrant Integration","text":"<p>IdeaWeaver provides seamless integration with Qdrant, a high-performance vector database. You can setup:</p> <ol> <li>Qdrant Cloud: For production deployments with managed service</li> </ol>"},{"location":"rag/overview/#using-qdrant-local","title":"Using Qdrant Local","text":""},{"location":"rag/overview/#using-qdrant-cloud","title":"Using Qdrant Cloud","text":"<pre><code># Set Qdrant Cloud credentials as environment variables\nexport QDRANT_URL=\"your-cloud-url\"\nexport QDRANT_API_KEY=\"your-api-key\"\n\nideaweaver rag create-kb --name cloud-kb \\\n    --vector-store qdrant_cloud \\\n    --qdrant-url \"$QDRANT_URL\" \\\n    --qdrant-api-key \"$QDRANT_API_KEY\" \\\n    --description \"Cloud Qdrant knowledge base\"\n</code></pre> <p>Note: For security reasons, never commit API keys or sensitive URLs to version control. Use environment variables or secure secret management systems.</p>"},{"location":"rag/overview/#qdrant-features","title":"Qdrant Features","text":"<ul> <li>High-performance vector search</li> <li>Scalable architecture</li> <li>Advanced filtering capabilities</li> <li>Payload support for metadata</li> <li>Real-time updates</li> </ul>"},{"location":"rag/overview/#rag-architecture","title":"RAG Architecture","text":""},{"location":"rag/overview/#getting-started","title":"Getting Started","text":"<ol> <li> <p>Create a knowledge base: <pre><code>ideaweaver rag create-kb --name my-kb --description \"My first knowledge base\"\n</code></pre></p> </li> <li> <p>Ingest documents: <pre><code>ideaweaver rag ingest --kb my-kb --source ./docs --file-types md,pdf\n</code></pre></p> </li> <li> <p>Query the knowledge base: <pre><code>ideaweaver rag query --kb my-kb -q \"What are the main features?\"\n</code></pre></p> </li> <li> <p>Evaluate the RAG system: <pre><code>ideaweaver rag evaluate --kb my-kb\n</code></pre></p> </li> </ol>"},{"location":"rag/overview/#next-steps","title":"Next Steps","text":"<ul> <li>RAG Commands - Complete command reference</li> <li>RAG Evaluation - Evaluation and benchmarking</li> <li>Enterprise RAG Guide - Production deployments </li> </ul>"},{"location":"reference/cli-commands/","title":"CLI Commands Reference","text":"<p>Complete reference for all IdeaWeaver CLI commands and their options.</p>"},{"location":"reference/cli-commands/#overview","title":"Overview","text":"<pre><code>Usage: ideaweaver [OPTIONS] COMMAND [ARGS]...\n\n  IdeaWeaver Model Training CLI - A comprehensive tool for AI model training,\n  evaluation, and deployment.\n\n  Features include LoRA/QLoRA fine-tuning, RAG systems, MCP integration, and\n  enterprise-grade model management. For detailed documentation and examples,\n  visit: https://github.com/ideaweaver-ai-code/ideaweaver\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  agent       Intelligent agent workflows for creative and analytical tasks.\n  download    Download a model from Hugging Face Hub\n  evaluate    Evaluate a model using lm-evaluation-harness with...\n  finetune    Supervised fine-tuning commands with LoRA, QLoRA, and full...\n  list-tasks  List all available evaluation tasks from lm-evaluation-harness\n  mcp         Model Context Protocol (MCP) integration commands\n  rag         RAG (Retrieval-Augmented Generation) commands\n  train       Train a model with AutoTrain Advanced.\n  validate    Validate a configuration file\n</code></pre>"},{"location":"reference/cli-commands/#model-management","title":"Model Management","text":""},{"location":"reference/cli-commands/#download","title":"<code>download</code>","text":"<p>Download models from Hugging Face Hub.</p> <pre><code>ideaweaver download [OPTIONS] MODEL_ID\n</code></pre> <p>Options:</p> <ul> <li><code>--revision TEXT</code>: Model revision to download (default: main)</li> <li><code>--cache-dir PATH</code>: Custom cache directory for models</li> <li><code>--token TEXT</code>: Hugging Face API token</li> <li><code>--help</code>: Show help and exit</li> </ul> <p>Examples: <pre><code># Download a model\nideaweaver download microsoft/DialoGPT-medium\n\n# Download specific revision\nideaweaver download microsoft/DialoGPT-medium --revision v1.0\n\n# Download with custom cache\nideaweaver download microsoft/DialoGPT-medium --cache-dir ./models\n</code></pre></p>"},{"location":"reference/cli-commands/#quantize","title":"<code>quantize</code>","text":"<p>Optimize models through quantization for faster inference.</p> <pre><code>ideaweaver quantize [OPTIONS]\n</code></pre> <p>Options: - <code>--model PATH</code>: Path to model directory or Hugging Face model ID - <code>--method TEXT</code>: Quantization method (int8, int4, fp16) - <code>--output PATH</code>: Output directory for quantized model - <code>--config PATH</code>: Quantization configuration file - <code>--help</code>: Show help and exit</p> <p>Examples: <pre><code># Basic int8 quantization\nideaweaver quantize --model microsoft/DialoGPT-medium --method int8\n\n# Custom output directory\nideaweaver quantize --model ./my-model --method int4 --output ./quantized\n</code></pre></p>"},{"location":"reference/cli-commands/#training-fine-tuning","title":"Training &amp; Fine-tuning","text":""},{"location":"reference/cli-commands/#train","title":"<code>train</code>","text":"<p>Train models with flexible configuration options.</p> <pre><code>ideaweaver train [OPTIONS]\n</code></pre> <p>Options: - <code>--config PATH</code>: Training configuration YAML file - <code>--model TEXT</code>: Base model name or path - <code>--dataset TEXT</code>: Dataset name or path - <code>--output-dir PATH</code>: Output directory for trained model - <code>--epochs INTEGER</code>: Number of training epochs - <code>--batch-size INTEGER</code>: Training batch size - <code>--learning-rate FLOAT</code>: Learning rate - <code>--help</code>: Show help and exit</p> <p>Examples: <pre><code># Train with configuration file\nideaweaver train --config config/training.yaml\n\n# Quick training with CLI options\nideaweaver train --model bert-base-uncased --dataset ./data.csv --epochs 3\n</code></pre></p>"},{"location":"reference/cli-commands/#finetune","title":"<code>finetune</code>","text":"<p>Supervised fine-tuning with LoRA, QLoRA, and full parameter methods.</p>"},{"location":"reference/cli-commands/#finetune-lora","title":"<code>finetune lora</code>","text":"<p>LoRA (Low-Rank Adaptation) fine-tuning.</p> <pre><code>ideaweaver finetune lora [OPTIONS]\n</code></pre> <p>Options: - <code>--model TEXT</code>: Base model for fine-tuning - <code>--dataset TEXT</code>: Training dataset - <code>--output-dir PATH</code>: Output directory - <code>--rank INTEGER</code>: LoRA rank (default: 16) - <code>--alpha INTEGER</code>: LoRA alpha parameter (default: 32) - <code>--dropout FLOAT</code>: LoRA dropout rate (default: 0.1) - <code>--epochs INTEGER</code>: Number of epochs (default: 3) - <code>--help</code>: Show help and exit</p> <p>Examples: <pre><code># Basic LoRA fine-tuning\nideaweaver finetune lora --model microsoft/DialoGPT-medium --dataset ./data\n\n# Custom LoRA parameters\nideaweaver finetune lora --model bert-base --dataset ./data --rank 32 --alpha 64\n</code></pre></p>"},{"location":"reference/cli-commands/#finetune-qlora","title":"<code>finetune qlora</code>","text":"<p>QLoRA (Quantized LoRA) for memory-efficient fine-tuning.</p> <pre><code>ideaweaver finetune qlora [OPTIONS]\n</code></pre> <p>Options: - Similar to LoRA with additional quantization options - <code>--bits INTEGER</code>: Quantization bits (4, 8) - <code>--double-quant BOOLEAN</code>: Use double quantization</p>"},{"location":"reference/cli-commands/#finetune-full","title":"<code>finetune full</code>","text":"<p>Full parameter fine-tuning.</p> <pre><code>ideaweaver finetune full [OPTIONS]\n</code></pre> <p>Options: - Standard training options without rank/alpha parameters</p>"},{"location":"reference/cli-commands/#evaluation-benchmarking","title":"Evaluation &amp; Benchmarking","text":""},{"location":"reference/cli-commands/#evaluate","title":"<code>evaluate</code>","text":"<p>Evaluate models using lm-evaluation-harness and custom benchmarks.</p> <pre><code>ideaweaver evaluate [OPTIONS]\n</code></pre> <p>Options: - <code>--model PATH</code>: Model to evaluate - <code>--tasks TEXT</code>: Comma-separated list of evaluation tasks - <code>--batch-size INTEGER</code>: Evaluation batch size - <code>--num-fewshot INTEGER</code>: Number of few-shot examples - <code>--output PATH</code>: Output file for results - <code>--device TEXT</code>: Device to use (cuda, cpu, auto) - <code>--help</code>: Show help and exit</p> <p>Examples: <pre><code># Evaluate on multiple tasks\nideaweaver evaluate --model ./my-model --tasks hellaswag,arc_easy,winogrande\n\n# Custom few-shot evaluation\nideaweaver evaluate --model ./my-model --tasks hellaswag --num-fewshot 5\n</code></pre></p>"},{"location":"reference/cli-commands/#list-tasks","title":"<code>list-tasks</code>","text":"<p>List all available evaluation tasks.</p> <pre><code>ideaweaver list-tasks [OPTIONS]\n</code></pre> <p>Options: - <code>--category TEXT</code>: Filter tasks by category - <code>--search TEXT</code>: Search tasks by name or description - <code>--help</code>: Show help and exit</p>"},{"location":"reference/cli-commands/#compare","title":"<code>compare</code>","text":"<p>Compare multiple models on the same benchmarks.</p> <pre><code>ideaweaver compare [OPTIONS]\n</code></pre> <p>Options: - <code>--models TEXT</code>: Comma-separated list of models to compare - <code>--tasks TEXT</code>: Evaluation tasks for comparison - <code>--output PATH</code>: Output file for comparison results - <code>--plot</code>: Generate comparison plots - <code>--help</code>: Show help and exit</p> <p>Examples: <pre><code># Compare three models\nideaweaver compare --models model1,model2,model3 --tasks hellaswag,arc_easy\n\n# Generate comparison with plots\nideaweaver compare --models model1,model2 --tasks all --plot --output comparison.html\n</code></pre></p>"},{"location":"reference/cli-commands/#rag-retrieval-augmented-generation","title":"RAG (Retrieval-Augmented Generation)","text":""},{"location":"reference/cli-commands/#rag-init","title":"<code>rag init</code>","text":"<p>Initialize a new RAG system.</p> <pre><code>ideaweaver rag init [OPTIONS]\n</code></pre> <p>Options: - <code>--vector-store TEXT</code>: Vector store type (chroma, qdrant, faiss) - <code>--embedding-model TEXT</code>: Embedding model name - <code>--llm TEXT</code>: Language model for generation - <code>--config PATH</code>: RAG configuration file - <code>--help</code>: Show help and exit</p>"},{"location":"reference/cli-commands/#rag-add-documents","title":"<code>rag add-documents</code>","text":"<p>Add documents to the RAG knowledge base.</p> <pre><code>ideaweaver rag add-documents [OPTIONS]\n</code></pre> <p>Options: - <code>--path PATH</code>: Path to documents or directory - <code>--recursive</code>: Process directories recursively - <code>--chunk-size INTEGER</code>: Document chunk size - <code>--chunk-overlap INTEGER</code>: Overlap between chunks - <code>--help</code>: Show help and exit</p>"},{"location":"reference/cli-commands/#rag-query","title":"<code>rag query</code>","text":"<p>Query the RAG system.</p> <pre><code>ideaweaver rag query [OPTIONS] QUERY\n</code></pre> <p>Options: - <code>--config PATH</code>: RAG configuration file - <code>--top-k INTEGER</code>: Number of retrieved documents - <code>--temperature FLOAT</code>: Generation temperature - <code>--save-context</code>: Save query context for training - <code>--help</code>: Show help and exit</p> <p>Examples: <pre><code># Basic RAG query\nideaweaver rag query \"What is machine learning?\"\n\n# Custom retrieval parameters\nideaweaver rag query \"Explain neural networks\" --top-k 10 --temperature 0.7\n</code></pre></p>"},{"location":"reference/cli-commands/#mcp-model-context-protocol","title":"MCP (Model Context Protocol)","text":""},{"location":"reference/cli-commands/#mcp-list-servers","title":"<code>mcp list-servers</code>","text":"<p>List all available MCP servers.</p> <pre><code>ideaweaver mcp list-servers [OPTIONS]\n</code></pre>"},{"location":"reference/cli-commands/#mcp-configure","title":"<code>mcp configure</code>","text":"<p>Configure MCP server connections.</p> <pre><code>ideaweaver mcp configure [OPTIONS] SERVER_NAME\n</code></pre> <p>Supported Servers: - <code>github</code>: GitHub integration - <code>slack</code>: Slack workspace integration - <code>aws</code>: AWS services integration - <code>filesystem</code>: Local filesystem access</p>"},{"location":"reference/cli-commands/#mcp-query","title":"<code>mcp query</code>","text":"<p>Query data through MCP servers.</p> <pre><code>ideaweaver mcp query [OPTIONS] SERVER_NAME QUERY\n</code></pre> <p>Examples: <pre><code># Query GitHub repositories\nideaweaver mcp query github \"List recent issues in my repository\"\n\n# Query Slack conversations\nideaweaver mcp query slack \"Summarize today's discussions\"\n</code></pre></p>"},{"location":"reference/cli-commands/#llamacpp-integration","title":"Llama.cpp Integration","text":""},{"location":"reference/cli-commands/#llama-convert","title":"<code>llama convert</code>","text":"<p>Convert models to GGML format for llama.cpp.</p> <pre><code>ideaweaver llama convert [OPTIONS]\n</code></pre> <p>Options: - <code>--model PATH</code>: Model to convert - <code>--output PATH</code>: Output GGML file - <code>--type TEXT</code>: Conversion type (f16, f32, q4_0, q4_1, q5_0, q5_1, q8_0) - <code>--help</code>: Show help and exit</p>"},{"location":"reference/cli-commands/#llama-inference","title":"<code>llama inference</code>","text":"<p>Run inference using llama.cpp.</p> <pre><code>ideaweaver llama inference [OPTIONS]\n</code></pre> <p>Options: - <code>--model PATH</code>: GGML model file - <code>--prompt TEXT</code>: Input prompt - <code>--max-tokens INTEGER</code>: Maximum generated tokens - <code>--temperature FLOAT</code>: Sampling temperature - <code>--help</code>: Show help and exit</p>"},{"location":"reference/cli-commands/#workflow-integration","title":"Workflow Integration","text":""},{"location":"reference/cli-commands/#crew","title":"<code>crew</code>","text":"<p>CrewAI operations for multi-agent workflows.</p> <pre><code>ideaweaver crew [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Commands: - <code>generate-storybook</code>: Generate storybooks using multi-agent approach - <code>setup-crew</code>: Configure CrewAI agents - <code>run-workflow</code>: Execute custom CrewAI workflows</p>"},{"location":"reference/cli-commands/#zenml","title":"<code>zenml</code>","text":"<p>ZenML pipeline operations.</p> <pre><code>ideaweaver zenml [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Commands: - <code>init-pipeline</code>: Initialize ZenML pipelines - <code>run-pipeline</code>: Execute ZenML pipelines - <code>list-pipelines</code>: Show available pipelines</p>"},{"location":"reference/cli-commands/#utilities","title":"Utilities","text":""},{"location":"reference/cli-commands/#validate","title":"<code>validate</code>","text":"<p>Validate configuration files.</p> <pre><code>ideaweaver validate [OPTIONS] CONFIG_FILE\n</code></pre> <p>Options: - <code>--type TEXT</code>: Configuration type (training, rag, evaluation) - <code>--strict</code>: Enable strict validation - <code>--help</code>: Show help and exit</p> <p>Examples: <pre><code># Validate training configuration\nideaweaver validate config/training.yaml --type training\n\n# Validate RAG setup\nideaweaver validate config/rag.yaml --type rag\n</code></pre></p>"},{"location":"reference/cli-commands/#global-options","title":"Global Options","text":"<p>These options are available for most commands:</p> <ul> <li><code>--verbose, -v</code>: Enable verbose output</li> <li><code>--quiet, -q</code>: Suppress non-error output</li> <li><code>--config PATH</code>: Global configuration file</li> <li><code>--log-level TEXT</code>: Set logging level (DEBUG, INFO, WARNING, ERROR)</li> <li><code>--no-cache</code>: Disable caching</li> <li><code>--device TEXT</code>: Force specific device (cuda, cpu, mps)</li> </ul>"},{"location":"reference/cli-commands/#configuration-files","title":"Configuration Files","text":""},{"location":"reference/cli-commands/#training-configuration","title":"Training Configuration","text":"<pre><code># config/training.yaml\nmodel:\n  name: \"microsoft/DialoGPT-medium\"\n  task: \"text-generation\"\n\ndataset:\n  name: \"your-dataset\"\n  split: \"train\"\n\ntraining:\n  output_dir: \"./results\"\n  num_train_epochs: 3\n  per_device_train_batch_size: 4\n  learning_rate: 5e-5\n  save_steps: 500\n  logging_steps: 100\n\nlora:\n  rank: 16\n  alpha: 32\n  dropout: 0.1\n</code></pre>"},{"location":"reference/cli-commands/#rag-configuration","title":"RAG Configuration","text":"<pre><code># config/rag.yaml\nvector_store:\n  type: \"chroma\"\n  persist_directory: \"./chroma_db\"\n\nembeddings:\n  model: \"sentence-transformers/all-MiniLM-L6-v2\"\n\nllm:\n  provider: \"openai\"\n  model: \"gpt-3.5-turbo\"\n  temperature: 0.7\n\nretrieval:\n  top_k: 5\n  similarity_threshold: 0.7\n\nchunking:\n  chunk_size: 1000\n  chunk_overlap: 200\n</code></pre>"},{"location":"reference/cli-commands/#evaluation-configuration","title":"Evaluation Configuration","text":"<pre><code># config/evaluation.yaml\nmodel: \"path/to/model\"\ntasks:\n  - \"hellaswag\"\n  - \"arc_easy\"\n  - \"winogrande\"\nbatch_size: 8\nnum_fewshot: 5\ndevice: \"auto\"\noutput_file: \"results.json\"\n</code></pre>"},{"location":"reference/cli-commands/#environment-variables","title":"Environment Variables","text":"<p>Set these environment variables for full functionality:</p> <pre><code># API Keys\nexport OPENAI_API_KEY=\"your-openai-key\"\nexport HUGGINGFACE_HUB_TOKEN=\"your-hf-token\"\nexport ANTHROPIC_API_KEY=\"your-anthropic-key\"\n\n# Experiment Tracking\nexport WANDB_API_KEY=\"your-wandb-key\"\nexport MLFLOW_TRACKING_URI=\"your-mlflow-uri\"\n\n# Cloud Services\nexport AWS_ACCESS_KEY_ID=\"your-aws-key\"\nexport AWS_SECRET_ACCESS_KEY=\"your-aws-secret\"\n\n# MCP Integrations\nexport GITHUB_TOKEN=\"your-github-token\"\nexport SLACK_BOT_TOKEN=\"your-slack-token\"\n</code></pre>"},{"location":"reference/cli-commands/#exit-codes","title":"Exit Codes","text":"<ul> <li><code>0</code>: Success</li> <li><code>1</code>: General error</li> <li><code>2</code>: Configuration error</li> <li><code>3</code>: Model not found</li> <li><code>4</code>: Dataset error</li> <li><code>5</code>: Training/evaluation error</li> <li><code>6</code>: Network/API error </li> </ul>"},{"location":"training/bedrock-registry/","title":"AWS Bedrock Model Registry Integration","text":""},{"location":"training/bedrock-registry/#overview","title":"Overview","text":"<p>IdeaWeaver integrates with AWS Bedrock for model hosting and deployment. This guide shows you how to use AWS Bedrock with IdeaWeaver for model management and inference.</p>"},{"location":"training/bedrock-registry/#setup","title":"Setup","text":""},{"location":"training/bedrock-registry/#train-and-deploy-a-model","title":"Train and Deploy a Model","text":"<pre><code>source ideaweaver-env/bin/activate\nideaweaver train \\\n  --model sshleifer/tiny-distilbert-base-cased \\\n  --dataset ./datasets/training_data.csv \\\n  --deploy-to-bedrock \\\n  --aws-region us-east-1 \\\n  --aws-access-key-id &lt;your-access-key&gt; \\\n  --aws-secret-access-key &lt;your-secret-key&gt;\n</code></pre>"},{"location":"training/bedrock-registry/#example-output","title":"Example Output","text":"<pre><code>\ud83e\udd17 Using model: sshleifer/tiny-distilbert-base-cased\n\ud83d\ude80 Starting model training...\n\n============================================================\n\ud83c\udf89 TRAINING SUMMARY\n============================================================\n\ud83d\udcc2 Model Path:           ./my-model\n\ud83e\udd16 Base Model:           sshleifer/tiny-distilbert-base-cased\n\ud83d\udcca Dataset:              ./autotrain_projects/my-model\n\n\ud83d\udcca KEY PERFORMANCE METRICS\n----------------------------------------\n\ud83d\udcc9 Final Train Loss:     1.0986\n\ud83c\udfaf Overall Accuracy:     20.0%\n\n============================================================\n\u2728 Training completed successfully! Model is ready for use.\n============================================================\n\n\u2705 Training completed successfully!\n\ud83d\udcc1 Model saved to: ./my-model\n\ud83d\ude80 Deploying model to AWS Bedrock...\n\u2705 Model deployed successfully to AWS Bedrock\n</code></pre>"},{"location":"training/bedrock-registry/#aws-bedrock-console-example","title":"AWS Bedrock Console Example","text":"<p>Once your model is deployed, you can view it in the AWS Bedrock Console:</p> <p></p>"},{"location":"training/bedrock-registry/#features","title":"Features","text":"<ul> <li>Model hosting</li> <li>Serverless deployment</li> <li>Auto-scaling</li> <li>Cost optimization</li> <li>Security features</li> <li>Monitoring tools</li> </ul>"},{"location":"training/bedrock-registry/#configuration","title":"Configuration","text":""},{"location":"training/bedrock-registry/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>--deploy-to-bedrock</code>: Flag to enable AWS Bedrock deployment</li> <li><code>--aws-region</code>: AWS region for deployment</li> <li><code>--aws-access-key-id</code>: AWS access key</li> <li><code>--aws-secret-access-key</code>: AWS secret key</li> </ul>"},{"location":"training/bedrock-registry/#aws-credentials-setup","title":"AWS Credentials Setup","text":"<ol> <li>Go to AWS IAM Console</li> <li>Create a new IAM user</li> <li>Attach necessary permissions</li> <li>Generate access keys</li> <li>Use keys in your training command</li> </ol>"},{"location":"training/bedrock-registry/#best-practices","title":"Best Practices","text":"<ol> <li>Security</li> <li>Use IAM roles</li> <li>Enable encryption</li> <li> <p>Monitor access</p> </li> <li> <p>Cost Management</p> </li> <li>Set up budgets</li> <li>Monitor usage</li> <li> <p>Optimize resources</p> </li> <li> <p>Deployment</p> </li> <li>Use appropriate instance types</li> <li>Configure auto-scaling</li> <li>Set up monitoring</li> </ol>"},{"location":"training/bedrock-registry/#troubleshooting","title":"Troubleshooting","text":""},{"location":"training/bedrock-registry/#common-issues","title":"Common Issues","text":"<ol> <li>Authentication Errors</li> <li>Verify AWS credentials</li> <li>Check IAM permissions</li> <li> <p>Ensure proper access</p> </li> <li> <p>Deployment Issues</p> </li> <li>Check region availability</li> <li>Verify resource limits</li> <li>Monitor CloudWatch logs</li> </ol>"},{"location":"training/bedrock-registry/#debug-mode","title":"Debug Mode","text":"<p>Enable verbose output for debugging:</p> <pre><code>ideaweaver train \\\n  --model sshleifer/tiny-distilbert-base-cased \\\n  --dataset ./datasets/training_data.csv \\\n  --deploy-to-bedrock \\\n  --aws-region us-east-1 \\\n  --verbose\n</code></pre>"},{"location":"training/bedrock-registry/#resources","title":"Resources","text":"<ul> <li>AWS Bedrock Documentation</li> <li>IAM Best Practices</li> <li>CloudWatch Monitoring </li> </ul>"},{"location":"training/comet-registry/","title":"Comet Model Registry Integration","text":""},{"location":"training/comet-registry/#overview","title":"Overview","text":"<p>IdeaWeaver integrates with Comet for experiment tracking and model management. This guide shows you how to use Comet with IdeaWeaver for model versioning and deployment.</p>"},{"location":"training/comet-registry/#setup","title":"Setup","text":""},{"location":"training/comet-registry/#train-and-register-a-model","title":"Train and Register a Model","text":"<pre><code>source ideaweaver-env/bin/activate\nideaweaver train \\\n  --model sshleifer/tiny-distilbert-base-cased \\\n  --dataset ./datasets/training_data.csv \\\n  --comet-api-key &lt;your-comet-api-key&gt; \\\n  --comet-project-name &lt;your-project&gt; \\\n  --comet-workspace &lt;your-workspace&gt;\n</code></pre>"},{"location":"training/comet-registry/#example-output","title":"Example Output","text":"<pre><code>\ud83e\udd17 Using model: sshleifer/tiny-distilbert-base-cased\n\ud83d\ude80 Starting model training...\n\n============================================================\n\ud83c\udf89 TRAINING SUMMARY\n============================================================\n\ud83d\udcc2 Model Path:           ./my-model\n\ud83e\udd16 Base Model:           sshleifer/tiny-distilbert-base-cased\n\ud83d\udcca Dataset:              ./autotrain_projects/my-model\n\n\ud83d\udcca KEY PERFORMANCE METRICS\n----------------------------------------\n\ud83d\udcc9 Final Train Loss:     1.0986\n\ud83c\udfaf Overall Accuracy:     20.0%\n\n============================================================\n\u2728 Training completed successfully! Model is ready for use.\n============================================================\n\n\u2705 Training completed successfully!\n\ud83d\udcc1 Model saved to: ./my-model\n</code></pre>"},{"location":"training/comet-registry/#comet-ui-example","title":"Comet UI Example","text":"<p>Once your model is registered, you can view it in the Comet UI:</p> <p></p>"},{"location":"training/comet-registry/#features","title":"Features","text":"<ul> <li>Experiment tracking</li> <li>Model versioning</li> <li>Artifact storage</li> <li>Performance metrics</li> <li>Model deployment</li> <li>Collaboration tools</li> </ul>"},{"location":"training/comet-registry/#configuration","title":"Configuration","text":""},{"location":"training/comet-registry/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>--comet-api-key</code>: Your Comet API key</li> <li><code>--comet-project-name</code>: Project name</li> <li><code>--comet-workspace</code>: Workspace name</li> </ul>"},{"location":"training/comet-registry/#getting-your-comet-api-key","title":"Getting Your Comet API Key","text":"<ol> <li>Go to Comet Account Settings</li> <li>Create a new API key</li> <li>Copy the key value</li> <li>Use it in your training command</li> </ol>"},{"location":"training/comet-registry/#best-practices","title":"Best Practices","text":"<ol> <li>Project Organization</li> <li>Use meaningful experiment names</li> <li>Tag experiments appropriately</li> <li> <p>Document model versions</p> </li> <li> <p>Model Management</p> </li> <li>Version your models</li> <li>Add model descriptions</li> <li> <p>Track model lineage</p> </li> <li> <p>Collaboration</p> </li> <li>Share experiments with team</li> <li>Document findings</li> <li>Track changes</li> </ol>"},{"location":"training/comet-registry/#troubleshooting","title":"Troubleshooting","text":""},{"location":"training/comet-registry/#common-issues","title":"Common Issues","text":"<ol> <li>Authentication Errors</li> <li>Verify API key is correct</li> <li>Check key permissions</li> <li> <p>Ensure proper access</p> </li> <li> <p>Connection Issues</p> </li> <li>Check internet connection</li> <li>Verify project exists</li> <li>Validate workspace name</li> </ol>"},{"location":"training/comet-registry/#debug-mode","title":"Debug Mode","text":"<p>Enable verbose output for debugging:</p> <pre><code>ideaweaver train \\\n  --model sshleifer/tiny-distilbert-base-cased \\\n  --dataset ./datasets/training_data.csv \\\n  --comet-api-key &lt;your-comet-api-key&gt; \\\n  --verbose\n</code></pre>"},{"location":"training/comet-registry/#resources","title":"Resources","text":"<ul> <li>Comet Documentation</li> <li>Comet Python SDK</li> <li>Model Registry Guide </li> </ul>"},{"location":"training/dagshub-registry/","title":"DagsHub Model Registry Integration","text":""},{"location":"training/dagshub-registry/#overview","title":"Overview","text":"<p>IdeaWeaver integrates with DagsHub for experiment tracking and model management. This guide shows you how to use DagsHub with IdeaWeaver for model versioning and deployment.</p>"},{"location":"training/dagshub-registry/#setup","title":"Setup","text":""},{"location":"training/dagshub-registry/#train-and-register-a-model","title":"Train and Register a Model","text":"<pre><code>source ideaweaver-env/bin/activate\nideaweaver train \\\n  --model sshleifer/tiny-distilbert-base-cased \\\n  --dataset ./datasets/training_data.csv \\\n  --mlflow-uri https://dagshub.com/&lt;your-username&gt;/&lt;your-repo&gt;.mlflow \\\n  --dagshub-token &lt;your-dagshub-token&gt; \\\n  --dagshub-repo-owner &lt;your-username&gt; \\\n  --dagshub-repo-name &lt;your-repo&gt;\n</code></pre>"},{"location":"training/dagshub-registry/#example-output","title":"Example Output","text":"<pre><code>\ud83e\udd17 Using model: sshleifer/tiny-distilbert-base-cased\n\ud83d\ude80 Starting model training...\n\n============================================================\n\ud83c\udf89 TRAINING SUMMARY\n============================================================\n\ud83d\udcc2 Model Path:           ./my-model\n\ud83e\udd16 Base Model:           sshleifer/tiny-distilbert-base-cased\n\ud83d\udcca Dataset:              ./autotrain_projects/my-model\n\n\ud83d\udcca KEY PERFORMANCE METRICS\n----------------------------------------\n\ud83d\udcc9 Final Train Loss:     1.0986\n\ud83c\udfaf Overall Accuracy:     20.0%\n\n============================================================\n\u2728 Training completed successfully! Model is ready for use.\n============================================================\n\n\u2705 Training completed successfully!\n\ud83d\udcc1 Model saved to: ./my-model\n</code></pre>"},{"location":"training/dagshub-registry/#dagshub-ui-example","title":"DagsHub UI Example","text":"<p>Once your model is registered, you can view it in the DagsHub Model Registry UI:</p> <p></p>"},{"location":"training/dagshub-registry/#features","title":"Features","text":"<ul> <li>Experiment tracking</li> <li>Model versioning</li> <li>Artifact storage</li> <li>Performance metrics</li> <li>Model deployment</li> <li>Collaboration tools</li> </ul>"},{"location":"training/dagshub-registry/#configuration","title":"Configuration","text":""},{"location":"training/dagshub-registry/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>--mlflow-uri</code>: DagsHub MLflow URI</li> <li><code>--dagshub-token</code>: Your DagsHub access token</li> <li><code>--dagshub-repo-owner</code>: Your DagsHub username</li> <li><code>--dagshub-repo-name</code>: Repository name</li> </ul>"},{"location":"training/dagshub-registry/#getting-your-dagshub-token","title":"Getting Your DagsHub Token","text":"<ol> <li>Go to DagsHub Tokens Settings</li> <li>Create a new token</li> <li>Copy the token value</li> <li>Use it in your training command</li> </ol>"},{"location":"training/dagshub-registry/#best-practices","title":"Best Practices","text":"<ol> <li>Repository Organization</li> <li>Use meaningful experiment names</li> <li>Tag experiments appropriately</li> <li> <p>Document model versions</p> </li> <li> <p>Model Management</p> </li> <li>Version your models</li> <li>Add model descriptions</li> <li> <p>Track model lineage</p> </li> <li> <p>Collaboration</p> </li> <li>Share experiments with team</li> <li>Document findings</li> <li>Track changes</li> </ol>"},{"location":"training/dagshub-registry/#troubleshooting","title":"Troubleshooting","text":""},{"location":"training/dagshub-registry/#common-issues","title":"Common Issues","text":"<ol> <li>Authentication Errors</li> <li>Verify token is correct</li> <li>Check token permissions</li> <li> <p>Ensure proper access</p> </li> <li> <p>Connection Issues</p> </li> <li>Check internet connection</li> <li>Verify repository exists</li> <li>Validate URI format</li> </ol>"},{"location":"training/dagshub-registry/#debug-mode","title":"Debug Mode","text":"<p>Enable verbose output for debugging:</p> <pre><code>ideaweaver train \\\n  --model sshleifer/tiny-distilbert-base-cased \\\n  --dataset ./datasets/training_data.csv \\\n  --mlflow-uri https://dagshub.com/&lt;your-username&gt;/&lt;your-repo&gt;.mlflow \\\n  --verbose\n</code></pre>"},{"location":"training/dagshub-registry/#resources","title":"Resources","text":"<ul> <li>DagsHub Documentation</li> <li>MLflow Integration Guide</li> <li>Model Registry Guide </li> </ul>"},{"location":"training/huggingface-registry/","title":"Hugging Face Model Registry Integration","text":""},{"location":"training/huggingface-registry/#overview","title":"Overview","text":"<p>IdeaWeaver integrates with Hugging Face for model hosting and versioning. This guide shows you how to use Hugging Face with IdeaWeaver for model management and deployment.</p>"},{"location":"training/huggingface-registry/#setup","title":"Setup","text":""},{"location":"training/huggingface-registry/#train-and-push-a-model","title":"Train and Push a Model","text":"<pre><code>source ideaweaver-env/bin/activate\nideaweaver train \\\n  --model sshleifer/tiny-distilbert-base-cased \\\n  --dataset ./datasets/training_data.csv \\\n  --push-to-hub \\\n  --hub-model-id &lt;your-username&gt;/&lt;model-name&gt; \\\n  --hub-token &lt;your-huggingface-token&gt;\n</code></pre>"},{"location":"training/huggingface-registry/#example-output","title":"Example Output","text":"<pre><code>\ud83e\udd17 Using model: sshleifer/tiny-distilbert-base-cased\n\ud83d\ude80 Starting model training...\n\n============================================================\n\ud83c\udf89 TRAINING SUMMARY\n============================================================\n\ud83d\udcc2 Model Path:           ./my-model\n\ud83e\udd16 Base Model:           sshleifer/tiny-distilbert-base-cased\n\ud83d\udcca Dataset:              ./autotrain_projects/my-model\n\n\ud83d\udcca KEY PERFORMANCE METRICS\n----------------------------------------\n\ud83d\udcc9 Final Train Loss:     1.0986\n\ud83c\udfaf Overall Accuracy:     20.0%\n\n============================================================\n\u2728 Training completed successfully! Model is ready for use.\n============================================================\n\n\u2705 Training completed successfully!\n\ud83d\udcc1 Model saved to: ./my-model\n\ud83d\ude80 Pushing model to Hugging Face Hub...\n\u2705 Model pushed successfully to &lt;your-username&gt;/&lt;model-name&gt;\n</code></pre>"},{"location":"training/huggingface-registry/#hugging-face-hub-example","title":"Hugging Face Hub Example","text":"<p>Once your model is pushed, you can view it on the Hugging Face Hub:</p> <p></p>"},{"location":"training/huggingface-registry/#features","title":"Features","text":"<ul> <li>Model hosting</li> <li>Version control</li> <li>Model cards</li> <li>Inference API</li> <li>Community sharing</li> <li>Model evaluation</li> </ul>"},{"location":"training/huggingface-registry/#configuration","title":"Configuration","text":""},{"location":"training/huggingface-registry/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>--push-to-hub</code>: Flag to enable pushing to Hugging Face Hub</li> <li><code>--hub-model-id</code>: Model ID in format username/model-name</li> <li><code>--hub-token</code>: Your Hugging Face API token</li> </ul>"},{"location":"training/huggingface-registry/#getting-your-hugging-face-token","title":"Getting Your Hugging Face Token","text":"<ol> <li>Go to Hugging Face Settings</li> <li>Create a new token</li> <li>Copy the token value</li> <li>Use it in your training command</li> </ol>"},{"location":"training/huggingface-registry/#best-practices","title":"Best Practices","text":"<ol> <li>Model Organization</li> <li>Use meaningful model names</li> <li>Add detailed model cards</li> <li> <p>Document model versions</p> </li> <li> <p>Model Management</p> </li> <li>Version your models</li> <li>Add model descriptions</li> <li> <p>Track model lineage</p> </li> <li> <p>Community</p> </li> <li>Share models appropriately</li> <li>Document usage</li> <li>Provide examples</li> </ol>"},{"location":"training/huggingface-registry/#troubleshooting","title":"Troubleshooting","text":""},{"location":"training/huggingface-registry/#common-issues","title":"Common Issues","text":"<ol> <li>Authentication Errors</li> <li>Verify token is correct</li> <li>Check token permissions</li> <li> <p>Ensure proper access</p> </li> <li> <p>Connection Issues</p> </li> <li>Check internet connection</li> <li>Verify model ID format</li> <li>Validate repository exists</li> </ol>"},{"location":"training/huggingface-registry/#debug-mode","title":"Debug Mode","text":"<p>Enable verbose output for debugging:</p> <pre><code>ideaweaver train \\\n  --model sshleifer/tiny-distilbert-base-cased \\\n  --dataset ./datasets/training_data.csv \\\n  --push-to-hub \\\n  --hub-model-id &lt;your-username&gt;/&lt;model-name&gt; \\\n  --verbose\n</code></pre>"},{"location":"training/huggingface-registry/#resources","title":"Resources","text":"<ul> <li>Hugging Face Hub Documentation</li> <li>Model Cards Guide</li> <li>Inference API Guide </li> </ul>"},{"location":"training/mlflow-registry/","title":"MLflow Model Registry Integration","text":""},{"location":"training/mlflow-registry/#overview","title":"Overview","text":"<p>IdeaWeaver provides seamless integration with MLflow for experiment tracking and model registration. This guide shows you how to use MLflow with IdeaWeaver for model management.</p>"},{"location":"training/mlflow-registry/#setup","title":"Setup","text":""},{"location":"training/mlflow-registry/#1-start-the-mlflow-server","title":"1. Start the MLflow server","text":"<pre><code>source ideaweaver-env/bin/activate\nmlflow server --host 127.0.0.1 --port 5000 --backend-store-uri sqlite:///mlflow.db\n</code></pre>"},{"location":"training/mlflow-registry/#2-train-and-register-a-model","title":"2. Train and register a model","text":"<pre><code>ideaweaver train \\\n  --model bert-base-uncased \\\n  --dataset ./datasets/training_data.csv \\\n  --track-experiments \\\n  --mlflow-uri http://127.0.0.1:5000 \\\n  --mlflow-experiment \"MyExperiment\" \\\n  --register-model\n</code></pre>"},{"location":"training/mlflow-registry/#example-output","title":"Example Output","text":"<pre><code>\ud83e\udd17 Using model: bert-base-uncased\n\ud83d\udcca Experiment tracking enabled\n\ud83c\udff7\ufe0f  Model registration enabled\n\ud83d\ude80 Starting model training...\n2025/06/05 11:08:39 INFO mlflow.tracking.fluent: Experiment with name 'MyExperiment' does not exist. Creating a new experiment.\n...\n</code></pre>"},{"location":"training/mlflow-registry/#mlflow-ui-example","title":"MLflow UI Example","text":"<p>You can view your run and registered model in the MLflow UI at http://127.0.0.1:5000.</p>"},{"location":"training/mlflow-registry/#features","title":"Features","text":"<ul> <li>Experiment tracking</li> <li>Model versioning</li> <li>Model registration</li> <li>Performance metrics logging</li> <li>Artifact storage</li> <li>Model deployment</li> </ul>"},{"location":"training/mlflow-registry/#best-practices","title":"Best Practices","text":"<ol> <li>Experiment Organization</li> <li>Use meaningful experiment names</li> <li>Tag experiments appropriately</li> <li> <p>Document experiment parameters</p> </li> <li> <p>Model Registration</p> </li> <li>Version your models</li> <li>Add model descriptions</li> <li> <p>Track model lineage</p> </li> <li> <p>Performance Tracking</p> </li> <li>Log all relevant metrics</li> <li>Track resource usage</li> <li>Monitor model performance</li> </ol>"},{"location":"training/mlflow-registry/#troubleshooting","title":"Troubleshooting","text":""},{"location":"training/mlflow-registry/#common-issues","title":"Common Issues","text":"<ol> <li>Connection Errors</li> <li>Verify MLflow server is running</li> <li>Check network connectivity</li> <li> <p>Validate URI format</p> </li> <li> <p>Authentication Issues</p> </li> <li>Check credentials</li> <li>Verify permissions</li> <li>Ensure proper access</li> </ol>"},{"location":"training/mlflow-registry/#debug-mode","title":"Debug Mode","text":"<p>Enable verbose output for debugging:</p> <pre><code>ideaweaver train \\\n  --model bert-base-uncased \\\n  --dataset ./datasets/training_data.csv \\\n  --track-experiments \\\n  --mlflow-uri http://127.0.0.1:5000 \\\n  --verbose\n</code></pre>"},{"location":"training/mlflow-registry/#resources","title":"Resources","text":"<ul> <li>MLflow Documentation</li> <li>Model Registry Guide</li> <li>Tracking Server Setup </li> </ul>"},{"location":"training/train-output/","title":"Train Output","text":""},{"location":"training/train-output/#training-approaches","title":"Training Approaches","text":"<p>IdeaWeaver supports two flexible training approaches:</p>"},{"location":"training/train-output/#1-config-based-training","title":"1. Config-Based Training","text":"<p>Using a YAML configuration file for complex setups:</p> <pre><code>ideaweaver train --config configs/config.yml --verbose\n</code></pre>"},{"location":"training/train-output/#configuration-used-yaml-based-training","title":"Configuration Used (YAML-based training)","text":"<pre><code>\ud83d\udccb Final configuration:\n   project_name: text-classifier\n   task: text_classification\n   backend: local\n   base_model: google/bert_uncased_L-2_H-128_A-2\n   dataset: ./datasets/training_data.csv\n   params: \n     epochs: 3\n     batch_size: 8\n     learning_rate: 2e-05\n     max_seq_length: 128\n     eval_strategy: no\n     save_total_limit: 1\n   data:\n     train_split: train\n   hub:\n     push_to_hub: false\n   method: sft\n   tracking:\n     enabled: false\n</code></pre>"},{"location":"training/train-output/#2-command-line-training","title":"2. Command Line Training","text":"<p>Pass all parameters directly via command line for quick experiments:</p> <pre><code>ideaweaver train \\\n  --model google/bert_uncased_L-2_H-128_A-2 \\\n  --dataset ./datasets/training_data.csv \\\n  --task text_classification \\\n  --project-name cli-final-test \\\n  --epochs 1 \\\n  --batch-size 4 \\\n  --learning-rate 2e-05 \\\n  --verbose\n</code></pre>"},{"location":"training/train-output/#command-line-training-output","title":"Command Line Training Output","text":"<p>Here's the successful output from the command line training approach:</p> <pre><code>\ud83e\udd17 Using model: google/bert_uncased_L-2_H-128_A-2\n\ud83c\udfaf Task: text_classification\n\ud83d\udccb Final configuration:\n   backend: local\n   params: {'epochs': 1, 'batch_size': 4, 'learning_rate': 2e-05, 'max_seq_length': 128}\n   data: {'train_split': 'train'}\n   hub: {'push_to_hub': False}\n   base_model: google/bert_uncased_L-2_H-128_A-2\n   task: text_classification\n   dataset: ./datasets/training_data.csv\n   project_name: cli-final-test\n   tracking: {'enabled': False}\n\ud83d\ude80 Starting model training...\n\ud83d\udcdd Training config written to: /tmp/training_config.yml\n\ud83d\udccb Training configuration:\nbackend: local\nbase_model: google/bert_uncased_L-2_H-128_A-2\ndata:\n  column_mapping:\n    target: target\n    text: text\n  path: ./autotrain_projects/cli-final-test\n  train_split: train\n  valid_split: null\nlog: tensorboard\nparams:\n  batch_size: 4\n  epochs: 1\n  lr: 2.0e-05\n  max_seq_length: 128\nproject_name: cli-final-test\ntask: text_classification\n\n\ud83d\udd27 Running command: autotrain --config /tmp/training_config.yml\n...\n[Training progress with successful completion]\n...\n\n\ud83d\udcca Loading metrics from trainer state: ./cli-final-test/checkpoint-1/trainer_state.json\n\u2705 Successfully loaded metrics from trainer state\n\n============================================================\n\ud83c\udf89 TRAINING SUMMARY\n============================================================\n\ud83d\udcc2 Model Path:           ./cli-final-test\n\ud83e\udd16 Base Model:           google/bert_uncased_L-2_H-128_A-2\n\ud83d\udcca Dataset:              ./autotrain_projects/cli-final-test\n\n\ud83d\udcca KEY PERFORMANCE METRICS\n----------------------------------------\n\ud83d\udcc9 Final Train Loss:     1.0869\n\ud83c\udfaf Overall Accuracy:     40.0%\n\n============================================================\n\u2728 Training completed successfully! Model is ready for use.\n============================================================\n\n\u2705 Training completed successfully!\n\ud83d\udcc1 Model saved to: ./cli-final-test\n</code></pre>"},{"location":"training/train-output/#key-metrics-achieved","title":"Key Metrics Achieved","text":"Metric Value Notes Final Training Loss 1.0869 Successfully extracted from trainer_state.json Overall Accuracy 40.0% Evaluation accuracy on validation set Training Epochs 1.0 Early stopping or completed epoch Model Size 17MB Compact BERT model (2 layers, 128 hidden units) Dataset Size 24 samples 19 training + 5 validation samples"},{"location":"training/train-output/#technical-details","title":"Technical Details","text":""},{"location":"training/train-output/#model-architecture","title":"Model Architecture","text":"<ul> <li>Base Model: <code>google/bert_uncased_L-2_H-128_A-2</code></li> <li>Task: Text Classification (Sentiment Analysis)</li> <li>Classes: 3 (positive, negative, neutral)</li> <li>Parameters: Small BERT with 2 layers and 128 hidden units</li> </ul>"},{"location":"training/train-output/#training-configuration","title":"Training Configuration","text":"<ul> <li>Learning Rate: 2e-05</li> <li>Batch Size: 8</li> <li>Max Sequence Length: 128</li> <li>Optimizer: AdamW</li> <li>Scheduler: Linear</li> <li>Early Stopping: Enabled (patience: 5, threshold: 0.01)</li> </ul>"},{"location":"training/train-output/#data-processing","title":"Data Processing","text":"<ul> <li>Training Split: 19 samples</li> <li>Validation Split: 5 samples  </li> <li>Text Column: <code>autotrain_text</code></li> <li>Target Column: <code>autotrain_label</code></li> </ul>"},{"location":"training/train-output/#files-generated","title":"Files Generated","text":""},{"location":"training/train-output/#model-files","title":"Model Files","text":"<ul> <li><code>./text-classifier/model.safetensors</code> (17MB)</li> <li><code>./text-classifier/config.json</code></li> <li><code>./text-classifier/tokenizer.json</code></li> <li><code>./text-classifier/tokenizer_config.json</code></li> </ul>"},{"location":"training/train-output/#training-artifacts","title":"Training Artifacts","text":"<ul> <li><code>./text-classifier/checkpoint-3/trainer_state.json</code> - Contains detailed training metrics</li> <li><code>./datasets/text-classifier/</code> - Copy of processed training data</li> <li>TensorBoard logs for visualization</li> </ul>"},{"location":"training/train-output/#verification-commands","title":"Verification Commands","text":"<pre><code># Verify model files\nls -la ./text-classifier/\n\n# Check trainer state metrics\ncat ./text-classifier/checkpoint-3/trainer_state.json | jq '.log_history[-1]'\n\n# Test model loading\npython -c \"from transformers import AutoTokenizer, AutoModelForSequenceClassification; tokenizer = AutoTokenizer.from_pretrained('./text-classifier'); model = AutoModelForSequenceClassification.from_pretrained('./text-classifier'); print('\u2705 Model loads successfully')\"\n</code></pre>"},{"location":"training/train-output/#success-indicators","title":"Success Indicators","text":"<p>\u2705 Metrics Display Fixed: Real training metrics now shown instead of \"Not available\" \u2705 Model Training: Successfully trained sentiment classification model \u2705 File Generation: All model files created correctly \u2705 Data Processing: Dataset processed and split appropriately \u2705 Trainer State: Detailed training history preserved in JSON format  </p>"},{"location":"training/train-output/#next-steps","title":"Next Steps","text":"<ol> <li>Model Evaluation: Test the trained model on new data</li> <li>Fine-tuning: Experiment with hyperparameters for better accuracy</li> <li>Deployment: Deploy the model for inference</li> <li>Integration: Use with RAG or other IdeaWeaver features</li> </ol>"},{"location":"training/train-output/#example-training-output-with-minilm-command-line","title":"Example: Training Output with MiniLM (Command-Line)","text":"<p>Below is a sample output from running the training command with the Microsoft MiniLM model:</p> <pre><code>ideaweaver train \\\n  --model microsoft/MiniLM-L6-H384-uncased \\\n  --dataset ./datasets/training_data.csv \\\n  --task text_classification \\\n  --project-name cli-minilm-test \\\n  --epochs 1 \\\n  --batch-size 4 \\\n  --learning-rate 2e-05 \\\n  --verbose\n</code></pre> <p>Note: If the model (<code>microsoft/MiniLM-L6-H384-uncased</code>) is not present locally, it will be automatically downloaded from Hugging Face the first time you run the command.</p>"},{"location":"training/train-output/#sample-output","title":"Sample Output","text":"<pre><code>\ud83e\udd17 Using model: microsoft/MiniLM-L6-H384-uncased\n\ud83c\udfaf Task: text_classification\n\ud83d\udccb Final configuration:\n   backend: local\n   params: {'epochs': 1, 'batch_size': 4, 'learning_rate': 2e-05, 'max_seq_length': 128}\n   data: {'train_split': 'train'}\n   hub: {'push_to_hub': False}\n   base_model: microsoft/MiniLM-L6-H384-uncased\n   task: text_classification\n   dataset: ./datasets/training_data.csv\n   project_name: cli-minilm-test\n   tracking: {'enabled': False}\n\ud83d\ude80 Starting model training...\n\ud83d\udcdd Training config written to: /tmp/tmppyzh5clq.yml\n\ud83d\udccb Training configuration:\nbackend: local\nbase_model: microsoft/MiniLM-L6-H384-uncased\ndata:\n  column_mapping:\n    target: target\n    text: text\n  path: ./autotrain_projects/cli-minilm-test\n  train_split: train\n  valid_split: null\nlog: tensorboard\nparams:\n  batch_size: 4\n  epochs: 1\n  lr: 2.0e-05\n  max_seq_length: 128\nproject_name: cli-minilm-test\ntask: text_classification\n\n\ud83d\udd27 Running command: autotrain --config /tmp/tmppyzh5clq.yml\nINFO     | ... - Using AutoTrain configuration: /tmp/tmppyzh5clq.yml\nINFO     | ... - Running task: text_multi_class_classification\nINFO     | ... - Using backend: local\nINFO     | ... - Starting local training...\nINFO     | ... - loading dataset from disk\nINFO     | ... - Starting to train...\nINFO     | ... - {'loss': 1.1103, 'grad_norm': 2.46, 'learning_rate': 2e-05, 'epoch': 0.33}\nINFO     | ... - {'loss': 1.126, 'grad_norm': 3.26, 'learning_rate': 1.75e-05, 'epoch': 0.67}\nINFO     | ... - {'loss': 1.0338, 'grad_norm': 5.06, 'learning_rate': 1.5e-05, 'epoch': 1.0}\nINFO     | ... - {'eval_loss': 1.0817, 'eval_f1_macro': 0.19, 'eval_f1_micro': 0.4, 'eval_accuracy': 0.4, 'epoch': 1.0}\nINFO     | ... - {'loss': 1.0702, ...}\nINFO     | ... - {'loss': 1.1341, ...}\nINFO     | ... - {'loss': 1.1244, ...}\nINFO     | ... - {'eval_loss': 1.0829, 'eval_accuracy': 0.4, 'epoch': 2.0}\nINFO     | ... - {'loss': 1.1247, ...}\nINFO     | ... - {'loss': 1.0579, ...}\nINFO     | ... - {'loss': 1.0865, ...}\nINFO     | ... - {'eval_loss': 1.0832, 'eval_accuracy': 0.4, 'epoch': 3.0}\nINFO     | ... - {'train_runtime': 2.29, 'train_loss': 1.0964, 'epoch': 3.0}\nINFO     | ... - Finished training, saving model...\nINFO     | ... - Job ID: 36417\n\n============================================================\n\ud83c\udf89 TRAINING SUMMARY\n============================================================\n\ud83d\udcc2 Model Path:           ./cli-minilm-test\n\ud83e\udd16 Base Model:           microsoft/MiniLM-L6-H384-uncased\n\ud83d\udcca Dataset:              ./autotrain_projects/cli-minilm-test\n\n\ud83d\udcca KEY PERFORMANCE METRICS\n----------------------------------------\n\ud83d\udcc9 Final Train Loss:     1.0338\n\ud83c\udfaf Overall Accuracy:     40.0%\n\n============================================================\n\u2728 Training completed successfully! Model is ready for use.\n============================================================\n\n\u2705 Training completed successfully!\n\ud83d\udcc1 Model saved to: ./cli-minilm-test\n</code></pre>"},{"location":"validation/commands/","title":"Validation Commands","text":"<p>The <code>validate</code> command in IdeaWeaver helps you verify your configuration files before running operations. This ensures that your setup is correct and helps prevent errors during execution.</p>"},{"location":"validation/commands/#basic-usage","title":"Basic Usage","text":"<pre><code>ideaweaver validate --config &lt;config_file&gt;\n</code></pre>"},{"location":"validation/commands/#arguments","title":"Arguments","text":"<ul> <li><code>config</code>: Path to the configuration file to validate (required)</li> </ul>"},{"location":"validation/commands/#examples","title":"Examples","text":""},{"location":"validation/commands/#basic-validation","title":"Basic Validation","text":"<pre><code>ideaweaver validate --config configs/config.yml\n</code></pre> <p>Example output: <pre><code>\ud83d\udd0d Validating configuration file: config.yml\n\u2705 Configuration is valid\n\ud83d\udccb Validation summary:\n   - All required fields are present\n   - Schema validation passed\n   - Dependencies are properly configured\n</code></pre></p> <p>Example output: <pre><code>\u2705 Configuration is valid!\n\u2705 Model google/bert_uncased_L-2_H-128_A-2 is accessible\n</code></pre></p>"},{"location":"validation/commands/#error-handling","title":"Error Handling","text":"<p>The validation command provides detailed error messages when issues are found:</p> <pre><code>ideaweaver validate --config config/training_config.yaml\n</code></pre> <p>Example output: <pre><code>\u274c Invalid configuration: Missing required field: project_name\n</code></pre></p>"},{"location":"validation/commands/#best-practices","title":"Best Practices","text":"<ol> <li>Validate Early: Run validation before starting any operation</li> <li>Use Verbose Mode: When debugging configuration issues</li> <li>Version Control: Keep validated configurations in version control</li> <li>Documentation: Document any special configuration requirements </li> </ol>"},{"location":"validation/overview/","title":"Configuration Validation","text":"<p>The validation feature in IdeaWeaver helps ensure that your configuration files are properly structured and contain all necessary components for successful model training, RAG system setup, and other operations.</p>"},{"location":"validation/overview/#features","title":"Features","text":"<ul> <li>YAML Validation: Validate the structure and content of your configuration files</li> <li>Schema Checking: Ensure configuration files match the expected schemas for different components</li> <li>Dependency Verification: Check for required dependencies and settings</li> <li>Comprehensive Reporting: Get detailed feedback about configuration issues</li> </ul>"},{"location":"validation/overview/#use-cases","title":"Use Cases","text":"<ol> <li>Pre-training Validation: Validate configuration files before starting model training</li> <li>Component Integration: Verify settings for integrated components (MLflow, Weights &amp; Biases, etc.)</li> <li>Dependency Management: Check for required dependencies and their versions</li> </ol>"},{"location":"validation/overview/#example-configuration","title":"Example Configuration","text":"<p>Here's an example of a valid configuration file:</p> <pre><code># config.yml\ntraining:\n  model: microsoft/DialoGPT-medium\n  dataset: ./data/training.json\n  output_dir: ./models/fine-tuned\n  epochs: 5\n  batch_size: 8\n  learning_rate: 5e-5\n</code></pre>"},{"location":"validation/overview/#validation-process","title":"Validation Process","text":"<ol> <li>File Structure Check: Validates the overall structure of the YAML file</li> <li>Schema Validation: Ensures all sections match their expected schemas</li> <li>Dependency Check: Verifies that required dependencies are specified</li> <li>Value Validation: Checks that values are within acceptable ranges</li> <li>Integration Validation: Validates settings for enabled integrations</li> </ol>"},{"location":"validation/overview/#best-practices","title":"Best Practices","text":"<ol> <li>Regular Validation: Validate configuration files before starting any operation</li> <li>Version Control: Keep validated configurations in version control</li> <li>Environment-Specific: Use different configurations for different environments</li> <li>Documentation: Document any special configuration requirements </li> </ol>"}]}